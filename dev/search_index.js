var documenterSearchIndex = {"docs":
[{"location":"volatility/#Volatility-Models","page":"Volatility Models","title":"Volatility Models","text":"This page covers univariate volatility modeling: ARCH, GARCH (including EGARCH and GJR-GARCH), and Stochastic Volatility (SV) models. These models capture time-varying conditional variance — a pervasive feature of financial and macroeconomic time series.","category":"section"},{"location":"volatility/#Quick-Start","page":"Volatility Models","title":"Quick Start","text":"using MacroEconometricModels\nusing Random\n\n# S&P 500 monthly returns (FRED-MD)\nfred = load_example(:fred_md)\nsp_idx = findfirst(v -> occursin(\"S&P\", v) && occursin(\"500\", v), varnames(fred))\ny = filter(isfinite, apply_tcode(fred[:, varnames(fred)[sp_idx]], 5))\n\n# ARCH(5) — Engle (1982)\narch = estimate_arch(y, 5)\n\n# GARCH(1,1) — Bollerslev (1986)\ngarch = estimate_garch(y, 1, 1)\n\n# EGARCH(1,1) — Nelson (1991)\negarch = estimate_egarch(y, 1, 1)\n\n# GJR-GARCH(1,1) — Glosten, Jagannathan & Runkle (1993)\ngjr = estimate_gjr_garch(y, 1, 1)\n\n# Stochastic Volatility — Taylor (1986), Kim-Shephard-Chib (1998)\nRandom.seed!(42)\nsv = estimate_sv(y; n_samples=2000, burnin=1000)\n\n# Diagnostics\narch_lm_test(y, 5)         # ARCH-LM test\nljung_box_squared(y, 10)   # Ljung-Box on squared residuals\nnic = news_impact_curve(garch)  # News impact curve\n\n# Forecast 20 steps ahead\nfc = forecast(garch, 20; conf_level=0.95)\n\n","category":"section"},{"location":"volatility/#ARCH-Models","page":"Volatility Models","title":"ARCH Models","text":"","category":"section"},{"location":"volatility/#Theory","page":"Volatility Models","title":"Theory","text":"The Autoregressive Conditional Heteroskedasticity (ARCH) model of Engle (1982) allows the conditional variance to depend on past squared innovations. The ARCH(q) specification is:\n\ny_t = mu + varepsilon_t qquad varepsilon_t = sigma_t z_t qquad z_t sim mathcalN(0 1)\n\nsigma^2_t = omega + sum_i=1^q alpha_i varepsilon^2_t-i\n\nwhere\n\ny_t is the observed time series\nmu is the conditional mean (intercept)\nvarepsilon_t is the mean-corrected innovation\nsigma^2_t is the conditional variance at time t\nomega  0 is the variance intercept\nalpha_i geq 0 are the ARCH coefficients\nz_t is a standardized innovation\n\nStationarity condition: The ARCH(q) process is covariance stationary if sum_i=1^q alpha_i  1.\n\nUnconditional variance: Under stationarity, textVar(varepsilon_t) = omega  (1 - sum_i=1^q alpha_i).","category":"section"},{"location":"volatility/#Estimation","page":"Volatility Models","title":"Estimation","text":"ARCH models are estimated by maximum likelihood (MLE) using two-stage optimization:\n\nStage 1 (NelderMead): Derivative-free search to find a good starting region.\nStage 2 (L-BFGS): Gradient-based refinement from the Stage 1 solution.\n\nParameters are log-transformed internally to enforce positivity constraints (omega  0, alpha_i geq 0) without constrained optimization.\n\n# Estimate ARCH(5) model\narch = estimate_arch(y, 5)\n\n# Access estimated parameters\narch.omega      # Variance intercept\narch.alpha      # ARCH coefficients [α₁, ..., α₅]\narch.mu         # Mean\narch.loglik     # Log-likelihood","category":"section"},{"location":"volatility/#Diagnostics","page":"Volatility Models","title":"Diagnostics","text":"Two diagnostic tests check whether ARCH effects have been adequately captured:\n\nARCH-LM Test (Engle 1982): Tests for remaining ARCH effects in model residuals or raw data.\n\n# Test raw data for ARCH effects (H₀: no ARCH effects)\nstat, pval, q = arch_lm_test(y, 5)\n\n# Test standardized residuals after fitting (should fail to reject)\nstat, pval, q = arch_lm_test(arch, 5)\n\nThe test regresses squared residuals on q of their own lags and computes TR^2 sim chi^2(q). Rejection of H_0 indicates ARCH effects are present (or remain after fitting).\n\nLjung-Box Test on Squared Residuals: Tests for serial correlation in z_t^2.\n\nstat, pval, K = ljung_box_squared(arch, 10)\n\nThe test statistic is Q = n(n+2) sum_k=1^K hatrho^2_k  (n - k) sim chi^2(K), where hatrho_k is the sample autocorrelation of squared standardized residuals at lag k. Failure to reject indicates the model has adequately captured the variance dynamics.","category":"section"},{"location":"volatility/#ARCHModel-Return-Values","page":"Volatility Models","title":"ARCHModel Return Values","text":"Field Type Description\ny Vector{T} Original data\nq Int ARCH order\nmu T Estimated mean (intercept)\nomega T Variance intercept omega\nalpha Vector{T} ARCH coefficients alpha_1 ldots alpha_q\nconditional_variance Vector{T} Estimated hatsigma^2_t at each t\nstandardized_residuals Vector{T} hatz_t = hatvarepsilon_t  hatsigma_t\nresiduals Vector{T} Raw residuals hatvarepsilon_t = y_t - hatmu\nfitted Vector{T} Fitted values (mean)\nloglik T Maximized log-likelihood\naic T Akaike Information Criterion\nbic T Bayesian Information Criterion\nmethod Symbol Estimation method (:mle)\nconverged Bool Whether optimization converged\niterations Int Number of optimizer iterations\n\n","category":"section"},{"location":"volatility/#GARCH-Models","page":"Volatility Models","title":"GARCH Models","text":"","category":"section"},{"location":"volatility/#GARCH(p,q)-—-Bollerslev-(1986)","page":"Volatility Models","title":"GARCH(p,q) — Bollerslev (1986)","text":"The Generalized ARCH model extends ARCH by including lagged conditional variances:\n\nsigma^2_t = omega + sum_i=1^q alpha_i varepsilon^2_t-i + sum_j=1^p beta_j sigma^2_t-j\n\nwhere\n\nomega  0 is the variance intercept\nalpha_i geq 0 are the ARCH coefficients (impact of past shocks)\nbeta_j geq 0 are the GARCH coefficients (variance persistence)\np is the GARCH order (lagged variances) and q is the ARCH order (lagged squared residuals)\n\nStationarity condition: sum_i=1^q alpha_i + sum_j=1^p beta_j  1.\n\nUnconditional variance: sigma^2 = omega  (1 - sum alpha_i - sum beta_j).\n\nThe GARCH(1,1) is the most widely used specification in practice, capturing the key empirical regularity of volatility clustering with just three parameters.\n\n# Estimate GARCH(1,1) — the workhorse specification\ngarch = estimate_garch(y, 1, 1)\n\n# Persistence: how quickly volatility reverts to its long-run level\npersistence(garch)              # α₁ + β₁ (close to 1 = slow reversion)\nhalflife(garch)                 # Half-life in periods\nunconditional_variance(garch)   # Long-run variance level","category":"section"},{"location":"volatility/#EGARCH(p,q)-—-Nelson-(1991)","page":"Volatility Models","title":"EGARCH(p,q) — Nelson (1991)","text":"The Exponential GARCH models the log of conditional variance, ensuring positivity without parameter constraints and allowing asymmetric responses to positive and negative shocks:\n\nlog(sigma^2_t) = omega + sum_i=1^q alpha_i (z_t-i - mathbbEz) + sum_i=1^q gamma_i z_t-i + sum_j=1^p beta_j log(sigma^2_t-j)\n\nwhere\n\nz_t = varepsilon_t  sigma_t are standardized residuals\nalpha_i captures the magnitude (symmetric) effect of shocks\ngamma_i captures the sign (asymmetric/leverage) effect — typically gamma_i  0 means negative shocks increase volatility more than positive shocks of equal magnitude\nbeta_j governs persistence of log-variance\nmathbbEz = sqrt2pi for standard normal innovations\n\nStationarity condition: sum_j=1^p beta_j  1 (in log-variance, unconditional parameters).\n\nUnconditional variance: sigma^2 = exp(omega  (1 - sum beta_j)).\n\n# Estimate EGARCH(1,1)\negarch = estimate_egarch(y, 1, 1)\n\n# Leverage parameters (γ < 0 → \"leverage effect\")\negarch.gamma    # Leverage coefficients","category":"section"},{"location":"volatility/#GJR-GARCH(p,q)-—-Glosten,-Jagannathan-and-Runkle-(1993)","page":"Volatility Models","title":"GJR-GARCH(p,q) — Glosten, Jagannathan & Runkle (1993)","text":"The GJR-GARCH (also called Threshold GARCH) adds an indicator function for negative shocks:\n\nsigma^2_t = omega + sum_i=1^q (alpha_i + gamma_i mathbb1(varepsilon_t-i  0)) varepsilon^2_t-i + sum_j=1^p beta_j sigma^2_t-j\n\nwhere\n\ngamma_i geq 0 are leverage parameters\nmathbb1(varepsilon_t-i  0) = 1 when past shocks are negative\n\nWhen gamma_i  0, negative shocks have a larger impact on future variance than positive shocks of equal magnitude. This captures the empirical \"leverage effect\" first documented by Black (1976): stock price declines increase financial leverage, which in turn increases equity volatility.\n\nStationarity condition: sum alpha_i + sum gamma_i  2 + sum beta_j  1.\n\nUnconditional variance: sigma^2 = omega  (1 - sum alpha_i - sum gamma_i  2 - sum beta_j).\n\n# Estimate GJR-GARCH(1,1)\ngjr = estimate_gjr_garch(y, 1, 1)\n\n# Leverage effect: γ > 0 means negative shocks increase variance more\ngjr.gamma    # Leverage coefficients","category":"section"},{"location":"volatility/#News-Impact-Curve","page":"Volatility Models","title":"News Impact Curve","text":"The news impact curve (NIC) shows how a shock varepsilon_t-1 maps to the next-period conditional variance sigma^2_t, holding all other information constant at the unconditional level. For symmetric models (ARCH, GARCH), the NIC is a parabola centered at zero. For asymmetric models (EGARCH, GJR-GARCH), the NIC is steeper for negative shocks.\n\n# Compute news impact curves\nnic_garch  = news_impact_curve(garch)\nnic_egarch = news_impact_curve(egarch; range=(-3.0, 3.0), n_points=200)\nnic_gjr    = news_impact_curve(gjr)\n\n# Returns named tuple: (shocks=Vector, variance=Vector)\nnic_garch.shocks     # Grid of εₜ₋₁ values\nnic_garch.variance   # Corresponding σ²ₜ values\n\nComparing news impact curves across models reveals whether asymmetric specifications (EGARCH, GJR-GARCH) capture economically important leverage effects that symmetric GARCH misses. If the NIC from GARCH and GJR-GARCH are nearly identical, the leverage effect is negligible and the simpler symmetric model suffices.","category":"section"},{"location":"volatility/#GARCH-Family-Return-Values","page":"Volatility Models","title":"GARCH-Family Return Values","text":"GARCHModel Fields\n\nField Type Description\ny Vector{T} Original data\np Int GARCH order (lagged variances)\nq Int ARCH order (lagged squared residuals)\nmu T Estimated mean\nomega T Variance intercept omega\nalpha Vector{T} ARCH coefficients alpha_1 ldots alpha_q\nbeta Vector{T} GARCH coefficients beta_1 ldots beta_p\nconditional_variance Vector{T} Estimated hatsigma^2_t\nstandardized_residuals Vector{T} hatz_t\nresiduals Vector{T} hatvarepsilon_t\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence status\niterations Int Optimizer iterations\n\nEGARCHModel Fields\n\nField Type Description\ny Vector{T} Original data\np Int Log-variance persistence order\nq Int Shock order\nmu T Estimated mean\nomega T Log-variance intercept\nalpha Vector{T} Magnitude (symmetric) parameters\ngamma Vector{T} Leverage (asymmetric) parameters\nbeta Vector{T} Log-variance persistence parameters\nconditional_variance Vector{T} hatsigma^2_t\nstandardized_residuals Vector{T} hatz_t\nresiduals Vector{T} hatvarepsilon_t\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence status\niterations Int Optimizer iterations\n\nGJRGARCHModel Fields\n\nField Type Description\ny Vector{T} Original data\np Int GARCH order\nq Int ARCH order\nmu T Estimated mean\nomega T Variance intercept omega\nalpha Vector{T} Symmetric ARCH coefficients\ngamma Vector{T} Leverage parameters gamma_1 ldots gamma_q\nbeta Vector{T} GARCH coefficients\nconditional_variance Vector{T} hatsigma^2_t\nstandardized_residuals Vector{T} hatz_t\nresiduals Vector{T} hatvarepsilon_t\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence status\niterations Int Optimizer iterations\n\n","category":"section"},{"location":"volatility/#Stochastic-Volatility","page":"Volatility Models","title":"Stochastic Volatility","text":"","category":"section"},{"location":"volatility/#Theory-2","page":"Volatility Models","title":"Theory","text":"The stochastic volatility (SV) model of Taylor (1986) treats the log-variance as a latent autoregressive process:\n\ny_t = exp(h_t  2)  varepsilon_t qquad varepsilon_t sim mathcalN(0 1)\n\nh_t = mu + varphi (h_t-1 - mu) + sigma_eta eta_t qquad eta_t sim mathcalN(0 1)\n\nwhere\n\nh_t is the log-variance at time t\nmu is the log-variance level (unconditional mean of h_t)\nvarphi in (-1 1) is the persistence parameter\nsigma_eta  0 is the volatility of volatility\nvarepsilon_t and eta_t are independent standard normal innovations\n\nThe SV model differs fundamentally from GARCH in that volatility has its own independent source of randomness (eta_t), making it a state-space model with a non-Gaussian observation equation. This provides greater flexibility in capturing empirical volatility dynamics, but precludes closed-form likelihood evaluation.","category":"section"},{"location":"volatility/#Variants","page":"Volatility Models","title":"Variants","text":"Three SV variants are available:\n\nBasic SV (leverage=false, dist=:normal): The standard specification above.\n\nSV with Leverage (leverage=true): Allows correlation between return and volatility innovations:\n\nbeginpmatrix varepsilon_t  eta_t endpmatrix sim mathcalNleft(mathbf0 beginpmatrix 1  rho  rho  1 endpmatrixright)\n\nWhen rho  0 (the typical case for equities), negative returns are associated with increases in volatility, analogous to the leverage effect in EGARCH and GJR-GARCH models.\n\nSV with Student-t Errors (dist=:studentt): Replaces the Gaussian observation equation with Student-t innovations to accommodate heavier tails:\n\ny_t = exp(h_t  2)  varepsilon_t qquad varepsilon_t sim t_nu\n\nwhere nu  2 is the degrees of freedom parameter.","category":"section"},{"location":"volatility/#Priors","page":"Volatility Models","title":"Priors","text":"The SV model is estimated via the Kim-Shephard-Chib (1998) Gibbs sampler with the Omori et al. (2007) 10-component mixture approximation. The default priors are:\n\nParameter Prior Rationale\nmu mathcalN(0 10) Weakly informative for log-variance level\nvarphi textBeta(20 15) to (-1 1) Concentrates mass near 1 (high persistence), ensures stationarity\nsigma_eta textHalfNormal(1) Positive, moderately informative for vol-of-vol\nrho (leverage) textUniform(-1 1) Uninformative over correlation range\nnu (Student-t) textExponential(01) + 2 Ensures nu  2 (finite variance)","category":"section"},{"location":"volatility/#Estimation-2","page":"Volatility Models","title":"Estimation","text":"# Basic SV model\nsv = estimate_sv(y; n_samples=2000, burnin=1000)\n\n# SV with leverage effect\nsv_lev = estimate_sv(y; leverage=true, n_samples=2000, burnin=1000)\n\n# SV with Student-t errors\nsv_t = estimate_sv(y; dist=:studentt, n_samples=2000, burnin=1000)\n\n# Access posterior summaries\nmean(sv.mu_post)          # Posterior mean of μ\nmean(sv.phi_post)         # Posterior mean of φ\nmean(sv.sigma_eta_post)   # Posterior mean of σ_η\n\n# Posterior volatility (time series)\nsv.volatility_mean        # Posterior mean of exp(hₜ) at each t\nsv.volatility_quantiles   # Quantiles (T × n_quantiles matrix)\nsv.quantile_levels        # Default: [0.025, 0.5, 0.975]\n\n# Latent log-volatility draws\nsv.h_draws                # n_samples × T matrix of posterior hₜ draws\n\nnote: Technical Note\nThe Kim-Shephard-Chib (1998) Gibbs sampler approximates the non-Gaussian observation equation log y_t^2 = h_t + log varepsilon_t^2 using a 10-component Gaussian mixture (Omori et al., 2007). Each Gibbs iteration: (1) samples the mixture indicators conditional on h, (2) samples h_1T via the simulation smoother conditional on parameters and indicators, and (3) samples (mu varphi sigma_eta) from their conditional posteriors. Typical run times are under 30 seconds for T = 500 with 2000 posterior draws.","category":"section"},{"location":"volatility/#SVModel-Return-Values","page":"Volatility Models","title":"SVModel Return Values","text":"Field Type Description\ny Vector{T} Original data\nh_draws Matrix{T} Latent log-volatility draws (n_samples × T)\nmu_post Vector{T} Posterior draws of mu\nphi_post Vector{T} Posterior draws of varphi\nsigma_eta_post Vector{T} Posterior draws of sigma_eta\nvolatility_mean Vector{T} Posterior mean of exp(h_t) at each t\nvolatility_quantiles Matrix{T} T times n_q quantiles of exp(h_t)\nquantile_levels Vector{T} Quantile levels (e.g., 0025 05 0975)\ndist Symbol Error distribution (:normal or :studentt)\nleverage Bool Whether leverage effect was estimated\nn_samples Int Number of posterior samples\n\n","category":"section"},{"location":"volatility/#Volatility-Forecasting","page":"Volatility Models","title":"Volatility Forecasting","text":"All volatility models support multi-step ahead forecasting via forecast(). ARCH and GARCH-family models use simulation-based confidence intervals; SV models use posterior predictive simulation from MCMC draws.","category":"section"},{"location":"volatility/#ARCH/GARCH-Forecasts","page":"Volatility Models","title":"ARCH/GARCH Forecasts","text":"# Forecast 20 steps ahead\nfc = forecast(garch, 20; conf_level=0.95, n_sim=10000)\n\n# Point forecasts converge to unconditional variance\nfc.forecast     # Vector of length 20\nfc.ci_lower     # Lower CI bound\nfc.ci_upper     # Upper CI bound\nfc.se           # Standard errors\n\n# Compare unconditional variance with long-horizon forecast\nunconditional_variance(garch)\nfc.forecast[end]  # Should be close for large h\n\nFor stationary GARCH processes, multi-step forecasts converge geometrically to the unconditional variance at rate equal to the persistence parameter. The speed of convergence is measured by the half-life: texthalflife = log(05)  log(textpersistence).\n\nConfidence intervals are constructed by simulating n paths forward from the last observed state, generating the distribution of future conditional variances. For ARCH models, forecasts beyond horizon q equal the unconditional variance exactly (no lagged variance terms to propagate).","category":"section"},{"location":"volatility/#SV-Forecasts","page":"Volatility Models","title":"SV Forecasts","text":"# Posterior predictive forecast from SV model\nfc_sv = forecast(sv, 20; conf_level=0.95)\n\nfc_sv.forecast     # Posterior mean forecast\nfc_sv.ci_lower     # 2.5th percentile\nfc_sv.ci_upper     # 97.5th percentile\n\nFor SV models, each posterior draw provides a full parameter vector (mu varphi sigma_eta) and the terminal log-volatility h_T. The forecast simulates the log-volatility process forward from the last state for each draw, yielding a posterior predictive distribution of future volatility. The reported intervals are posterior predictive quantiles, not frequentist confidence intervals.","category":"section"},{"location":"volatility/#VolatilityForecast-Return-Values","page":"Volatility Models","title":"VolatilityForecast Return Values","text":"Field Type Description\nforecast Vector{T} Point forecasts of conditional variance hatsigma^2_T+h\nci_lower Vector{T} Lower confidence/credible interval bound\nci_upper Vector{T} Upper confidence/credible interval bound\nse Vector{T} Standard errors of forecasts\nhorizon Int Forecast horizon\nconf_level T Confidence level (e.g., 0.95)\nmodel_type Symbol Source model (:arch, :garch, :egarch, :gjr_garch, :sv)\n\n","category":"section"},{"location":"volatility/#Type-Accessors","page":"Volatility Models","title":"Type Accessors","text":"The following accessor functions provide model-specific summary statistics. The formulas differ across model types:\n\nFunction ARCH GARCH EGARCH GJR-GARCH SV\npersistence(m) sum alpha_i sum alpha_i + sum beta_j sum beta_j sum alpha_i + sum gamma_i2 + sum beta_j mathbbEvarphi\nhalflife(m) log(05)log(p) log(05)log(p) log(05)log(p) log(05)log(p) log(05)log(p)\nunconditional_variance(m) fracomega1 - sum alpha_i fracomega1 - sum alpha_i - sum beta_j expleft(fracomega1 - sum beta_jright) fracomega1 - sum alpha_i - sum gamma_i2 - sum beta_j exp(mathbbEmu)\narch_order(m) q q q q —\ngarch_order(m) — p p p —\n\nIn the table, p denotes persistence(m). The half-life returns Inf if the process is non-stationary (persistence geq 1).\n\npersistence(garch)              # 0.95 → high persistence\nhalflife(garch)                 # ≈ 13.5 periods\nunconditional_variance(garch)   # Long-run variance\narch_order(garch)               # q\ngarch_order(garch)              # p\n\n","category":"section"},{"location":"volatility/#StatsAPI-Interface","page":"Volatility Models","title":"StatsAPI Interface","text":"All volatility models implement the standard StatsAPI interface:\n\nFunction Description\nnobs(m) Number of observations\ncoef(m) Coefficient vector\nresiduals(m) Raw residuals hatvarepsilon_t\npredict(m) Conditional variance series hatsigma^2_t (or posterior mean for SV)\nloglikelihood(m) Maximized log-likelihood (ARCH/GARCH)\naic(m) Akaike Information Criterion\nbic(m) Bayesian Information Criterion\ndof(m) Number of estimated parameters\nislinear(m) false (all volatility models are nonlinear)\n\nnobs(garch)          # Number of observations\nloglikelihood(garch) # Maximized log-likelihood\naic(garch)           # AIC for model comparison\nbic(garch)           # BIC for model comparison\ncoef(garch)          # [μ, ω, α₁, ..., αq, β₁, ..., βp]\n\n","category":"section"},{"location":"volatility/#Complete-Example","page":"Volatility Models","title":"Complete Example","text":"This example estimates all four GARCH-family models on the same data, compares their news impact curves and forecasts, runs diagnostics, and estimates an SV model for comparison.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\n# S&P 500 monthly returns (FRED-MD)\nfred = load_example(:fred_md)\nsp_idx = findfirst(v -> occursin(\"S&P\", v) && occursin(\"500\", v), varnames(fred))\ny = filter(isfinite, apply_tcode(fred[:, varnames(fred)[sp_idx]], 5))\n\nprintln(\"S&P 500 returns: T=$(length(y))\")\n\n# === Step 1: Test for ARCH effects ===\nstat, pval, q = arch_lm_test(y, 5)\nprintln(\"\\nARCH-LM test (q=5):\")\nprintln(\"  Statistic: \", round(stat, digits=2))\nprintln(\"  P-value: \", round(pval, digits=6))\n\nstat2, pval2, K = ljung_box_squared(y, 10)\nprintln(\"\\nLjung-Box squared (K=10):\")\nprintln(\"  Statistic: \", round(stat2, digits=2))\nprintln(\"  P-value: \", round(pval2, digits=6))\n\n# === Step 2: Estimate competing models ===\ngarch   = estimate_garch(y, 1, 1)\negarch  = estimate_egarch(y, 1, 1)\ngjr     = estimate_gjr_garch(y, 1, 1)\n\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Model Comparison\")\nprintln(\"=\"^60)\nprintln(\"              AIC         BIC     Persistence\")\nprintln(\"  GARCH:   \", round(aic(garch), digits=1),\n        \"    \", round(bic(garch), digits=1),\n        \"    \", round(persistence(garch), digits=4))\nprintln(\"  EGARCH:  \", round(aic(egarch), digits=1),\n        \"    \", round(bic(egarch), digits=1),\n        \"    \", round(persistence(egarch), digits=4))\nprintln(\"  GJR:     \", round(aic(gjr), digits=1),\n        \"    \", round(bic(gjr), digits=1),\n        \"    \", round(persistence(gjr), digits=4))\n\n# === Step 3: Compare news impact curves ===\nnic_g  = news_impact_curve(garch)\nnic_e  = news_impact_curve(egarch)\nnic_j  = news_impact_curve(gjr)\n\nprintln(\"\\nNews Impact at ε = -2 vs ε = +2:\")\nidx_neg = findfirst(x -> x >= -2.0, nic_g.shocks)\nidx_pos = findfirst(x -> x >= 2.0, nic_g.shocks)\n\nprintln(\"  GARCH:  σ²(-2) = \", round(nic_g.variance[idx_neg], digits=4),\n        \"   σ²(+2) = \", round(nic_g.variance[idx_pos], digits=4))\nprintln(\"  EGARCH: σ²(-2) = \", round(nic_e.variance[idx_neg], digits=4),\n        \"   σ²(+2) = \", round(nic_e.variance[idx_pos], digits=4))\nprintln(\"  GJR:    σ²(-2) = \", round(nic_j.variance[idx_neg], digits=4),\n        \"   σ²(+2) = \", round(nic_j.variance[idx_pos], digits=4))\n\n# === Step 4: Check residual diagnostics ===\nprintln(\"\\nResidual ARCH-LM test (q=5):\")\nfor (name, m) in [(\"GARCH\", garch), (\"EGARCH\", egarch), (\"GJR\", gjr)]\n    _, p, _ = arch_lm_test(m, 5)\n    status = p > 0.05 ? \"Pass\" : \"FAIL\"\n    println(\"  $name: p=$(round(p, digits=4))  $status\")\nend\n\n# === Step 5: Forecast volatility ===\nH = 20\nfc_g = forecast(garch, H)\nfc_e = forecast(egarch, H)\nfc_j = forecast(gjr, H)\n\nprintln(\"\\nVolatility forecasts (conditional variance):\")\nprintln(\"  h    GARCH    EGARCH   GJR      Uncond\")\nfor h_idx in [1, 5, 10, 20]\n    println(\"  $h_idx    \",\n            round(fc_g.forecast[h_idx], digits=4), \"  \",\n            round(fc_e.forecast[h_idx], digits=4), \"  \",\n            round(fc_j.forecast[h_idx], digits=4), \"  \",\n            round(unconditional_variance(garch), digits=4))\nend\n\n# === Step 6: Stochastic volatility for comparison ===\nprintln(\"\\nEstimating SV model via KSC Gibbs sampler...\")\nRandom.seed!(42)\nsv = estimate_sv(y; n_samples=2000, burnin=1000)\n\nprintln(\"SV posterior summary:\")\nprintln(\"  μ (log-vol level):   \", round(mean(sv.mu_post), digits=3),\n        \" [\", round(quantile(sv.mu_post, 0.025), digits=3),\n        \", \", round(quantile(sv.mu_post, 0.975), digits=3), \"]\")\nprintln(\"  φ (persistence):    \", round(mean(sv.phi_post), digits=3),\n        \" [\", round(quantile(sv.phi_post, 0.025), digits=3),\n        \", \", round(quantile(sv.phi_post, 0.975), digits=3), \"]\")\nprintln(\"  σ_η (vol of vol):   \", round(mean(sv.sigma_eta_post), digits=3),\n        \" [\", round(quantile(sv.sigma_eta_post, 0.025), digits=3),\n        \", \", round(quantile(sv.sigma_eta_post, 0.975), digits=3), \"]\")\n\n# SV forecast\nfc_sv = forecast(sv, H)\nprintln(\"\\nSV forecast at h=1: \", round(fc_sv.forecast[1], digits=4))\nprintln(\"SV forecast at h=20: \", round(fc_sv.forecast[end], digits=4))\n\nThe S&P 500 returns exhibit strong ARCH effects, confirming the need for volatility modeling. The GARCH(1,1) persistence is typically close to 0.95 for equity returns, meaning volatility shocks are highly persistent. The news impact curves reveal asymmetry: for EGARCH and GJR-GARCH, a negative leverage parameter confirms that bad news increases volatility more than good news of equal magnitude — for symmetric GARCH, the NIC is symmetric by construction. All models' standardized residuals should pass the ARCH-LM test after fitting, confirming that the conditional variance dynamics are adequately captured. The SV model provides an independent, Bayesian assessment of the volatility dynamics via the Kim-Shephard-Chib (1998) Gibbs sampler.\n\n","category":"section"},{"location":"volatility/#See-Also","page":"Volatility Models","title":"See Also","text":"Hypothesis Tests – ARCH-LM test and diagnostic testing\nARIMA Models – Mean equation modeling before volatility estimation\nAPI Reference – Complete function signatures","category":"section"},{"location":"volatility/#References","page":"Volatility Models","title":"References","text":"Black, Fischer. 1976. \"Studies of Stock Price Volatility Changes.\" Proceedings of the 1976 Meetings of the American Statistical Association, 171–177.\nBollerslev, Tim. 1986. \"Generalized Autoregressive Conditional Heteroskedasticity.\" Journal of Econometrics 31 (3): 307–327. https://doi.org/10.1016/0304-4076(86)90063-1\nEngle, Robert F. 1982. \"Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation.\" Econometrica 50 (4): 987–1007. https://doi.org/10.2307/1912773\nGlosten, Lawrence R., Ravi Jagannathan, and David E. Runkle. 1993. \"On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks.\" Journal of Finance 48 (5): 1779–1801. https://doi.org/10.1111/j.1540-6261.1993.tb05128.x\nNelson, Daniel B. 1991. \"Conditional Heteroskedasticity in Asset Returns: A New Approach.\" Econometrica 59 (2): 347–370. https://doi.org/10.2307/2938260\nKim, Sangjoon, Neil Shephard, and Siddhartha Chib. 1998. \"Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models.\" Review of Economic Studies 65 (3): 361–393. https://doi.org/10.1111/1467-937X.00050\nOmori, Yasuhiro, Siddhartha Chib, Neil Shephard, and Jouchi Nakajima. 2007. \"Stochastic Volatility with Leverage: Fast and Efficient Likelihood Inference.\" Journal of Econometrics 140 (2): 425–449. https://doi.org/10.1016/j.jeconom.2006.07.008\nTaylor, Stephen J. 1986. Modelling Financial Time Series. Chichester: Wiley. ISBN 978-0-471-90975-7.","category":"section"},{"location":"bayesian/#Bayesian-VAR-(BVAR)","page":"Bayesian VAR","title":"Bayesian VAR (BVAR)","text":"This chapter covers Bayesian estimation methods for Vector Autoregression models, including the Minnesota prior, hyperparameter optimization, and conjugate posterior inference.","category":"section"},{"location":"bayesian/#Introduction","page":"Bayesian VAR","title":"Introduction","text":"Bayesian VAR (BVAR) estimation addresses the curse of dimensionality in VAR models by incorporating prior information to shrink coefficient estimates. This is particularly valuable when:\n\nThe number of parameters is large relative to sample size\nPrior economic knowledge should influence estimation\nUncertainty quantification via posterior distributions is desired\nForecasting performance is paramount\n\nKey References: Litterman (1986), Doan, Litterman & Sims (1984), Giannone, Lenza & Primiceri (2015)","category":"section"},{"location":"bayesian/#Quick-Start","page":"Bayesian VAR","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD: standard monetary VAR (slow-to-fast ordering)\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nhyper = MinnesotaHyperparameters(tau=0.5, decay=2.0, lambda=1.0, mu=1.0, omega=1.0)\nbest = optimize_hyperparameters(Y, 2; grid_size=20)                  # Optimize tau\npost = estimate_bvar(Y, 2; n_draws=1000, prior=:minnesota, hyper=best,\n                     varnames=[\"INDPRO\", \"CPI\", \"FFR\"])\nbirf = irf(post, 20; method=:cholesky)                              # Bayesian IRF\nbfevd = fevd(post, 20)                                              # Bayesian FEVD\n\n","category":"section"},{"location":"bayesian/#Bayesian-Framework","page":"Bayesian VAR","title":"Bayesian Framework","text":"","category":"section"},{"location":"bayesian/#The-Prior-Likelihood-Posterior-Paradigm","page":"Bayesian VAR","title":"The Prior-Likelihood-Posterior Paradigm","text":"In the Bayesian approach, we treat the VAR parameters as random variables and update our beliefs using Bayes' theorem:\n\np(B Sigma  Y) propto p(Y  B Sigma) cdot p(B Sigma)\n\nwhere:\n\np(Y  B Sigma) is the likelihood\np(B Sigma) is the prior\np(B Sigma  Y) is the posterior","category":"section"},{"location":"bayesian/#Natural-Conjugate-Prior","page":"Bayesian VAR","title":"Natural Conjugate Prior","text":"For computational convenience, we use the Normal-Inverse-Wishart conjugate prior:\n\nSigma sim textIW(nu_0 S_0)\n\ntextvec(B)  Sigma sim N(textvec(B_0) Sigma otimes Omega_0)\n\nThis yields a closed-form posterior of the same family.\n\n","category":"section"},{"location":"bayesian/#The-Minnesota-Prior","page":"Bayesian VAR","title":"The Minnesota Prior","text":"","category":"section"},{"location":"bayesian/#Motivation","page":"Bayesian VAR","title":"Motivation","text":"The Minnesota prior (Litterman, 1986; Doan, Litterman & Sims, 1984) shrinks VAR coefficients toward a random walk prior. This reflects the empirical observation that many macroeconomic variables are well-approximated by random walks, especially at short horizons.","category":"section"},{"location":"bayesian/#Prior-Specification","page":"Bayesian VAR","title":"Prior Specification","text":"Prior Mean: Each variable follows a random walk:\n\nEA_1ii = 1 quad EA_1ij = 0 text for  i neq j quad EA_l = 0 text for  l  1\n\nPrior Variance: The prior variance for coefficient (ij) at lag l is:\n\ntextVar(A_lij) = begincases\nfractau^2l^d  textif  i = j text (own lag) \nfractau^2 omega^2l^d cdot fracsigma_i^2sigma_j^2  textif  i neq j text (cross lag)\nendcases\n\nwhere:\n\ntau is the overall tightness (shrinkage intensity)\nd is the lag decay (typically d = 2)\nomega controls cross-variable shrinkage (typically omega  1)\nsigma_i^2 is the residual variance from a univariate AR(1) for variable i","category":"section"},{"location":"bayesian/#Interpretation-of-Hyperparameters","page":"Bayesian VAR","title":"Interpretation of Hyperparameters","text":"Parameter Effect Typical Values\ntau Overall shrinkage (lower = more shrinkage) 0.01 – 1.0\nd Lag decay (higher = faster decay) 1, 2, 3\nomega Cross-variable penalty (lower = more penalty) 0.5 – 1.0","category":"section"},{"location":"bayesian/#Julia-Implementation","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy variables\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Define hyperparameters\nhyper = MinnesotaHyperparameters(\n    tau = 0.5,      # Overall tightness\n    decay = 2.0,    # Lag decay\n    lambda = 1.0,   # Own-lag variance scaling\n    mu = 1.0,       # Cross-lag variance scaling\n    omega = 1.0     # Deterministic terms scaling\n)\n\n# Use in BVAR estimation\npost = estimate_bvar(Y, 2; n_draws=1000,\n                     prior=:minnesota, hyper=hyper,\n                     varnames=[\"INDPRO\", \"CPI\", \"FFR\"])\n\nThe tau=0.5 setting provides moderate shrinkage — coefficient estimates will be pulled halfway between the data-driven OLS estimates and the random walk prior. With decay=2.0, the prior variance for lag-l coefficients decays as 1l^2, so distant lags are strongly penalized. Setting mu=1.0 treats cross-variable lags the same as own lags; reducing mu (e.g., to 0.5) would impose stronger shrinkage on cross-variable coefficients, reflecting the common finding that own lags are more informative than other variables' lags.","category":"section"},{"location":"bayesian/#MinnesotaHyperparameters-Return-Values","page":"Bayesian VAR","title":"MinnesotaHyperparameters Return Values","text":"Field Type Description\ntau T Overall tightness (lower = more shrinkage toward prior)\ndecay T Lag decay exponent (higher = faster decay of lag importance)\nlambda T Own-lag variance scaling\nmu T Cross-lag variance scaling (lower = more penalty on cross-variable lags)\nomega T Deterministic terms scaling\n\n","category":"section"},{"location":"bayesian/#Dummy-Observations-Approach","page":"Bayesian VAR","title":"Dummy Observations Approach","text":"","category":"section"},{"location":"bayesian/#Implementation-via-Augmented-Regression","page":"Bayesian VAR","title":"Implementation via Augmented Regression","text":"We implement the Minnesota prior using dummy observations (Theil-Goldberger mixed estimation). The augmented data matrices are:\n\nPrior on coefficients (tightness dummies):\n\nY_d = beginbmatrix\ntextdiag(sigma_1 ldots sigma_n)  tau \n0_n(p-1) times n \ntextdiag(sigma_1 ldots sigma_n) \n0_1 times n\nendbmatrix quad\nX_d = beginbmatrix\n0_n times 1  J_p otimes textdiag(sigma_1 ldots sigma_n)  tau \n0_n(p-1) times 1  I_p-1 otimes textdiag(sigma_1 ldots sigma_n) \n0_n times 1  0_n times np \nc  0_1 times np\nendbmatrix\n\nwhere J_p = textdiag(1 2^d ldots p^d).\n\nThe posterior is then computed as OLS on the augmented data Y Y_d and X X_d.\n\nReference: Litterman (1986), Kadiyala & Karlsson (1997), Bańbura, Giannone & Reichlin (2010)","category":"section"},{"location":"bayesian/#Julia-Implementation-2","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Generate dummy observations for Minnesota prior\nY_dummy, X_dummy = gen_dummy_obs(Y, p, hyper)\n\n# Augment data\nY_aug = vcat(Y_actual, Y_dummy)\nX_aug = vcat(X_actual, X_dummy)\n\n# Posterior via OLS on augmented data\nB_post = (X_aug'X_aug) \\ (X_aug'Y_aug)\n\n# Output (for n=3, p=2):\n# size(Y_dummy) = (10, 3)    # 3n+n+1 = 10 dummy observations\n# size(X_dummy) = (10, 7)    # 1 + np = 7 regressors\n\nThe dummy observations encode the prior belief: the tightness dummies pull A_1 toward the identity (random walk), while the decay dummies shrink higher-lag coefficients toward zero. Augmenting the data with these pseudo-observations and running OLS on the combined system is algebraically equivalent to computing the posterior mean under the Normal-Inverse-Wishart conjugate prior.\n\n","category":"section"},{"location":"bayesian/#Hyperparameter-Optimization","page":"Bayesian VAR","title":"Hyperparameter Optimization","text":"","category":"section"},{"location":"bayesian/#Marginal-Likelihood","page":"Bayesian VAR","title":"Marginal Likelihood","text":"Rather than selecting tau subjectively, we can optimize it by maximizing the marginal likelihood (Giannone, Lenza & Primiceri, 2015):\n\np(Y  tau) = int p(Y  B Sigma) p(B Sigma  tau)  dB  dSigma\n\nFor the Normal-Inverse-Wishart prior with dummy observations, the log marginal likelihood has an analytical form:\n\nlog p(Y  tau) = c + fracT-k2 logtildeS^-1 - fracT_d2 logtildeS_d^-1 + log fracGamma_n(fracT+T_d - k2)Gamma_n(fracT_d - k2)\n\nwhere\n\nc is a normalization constant\nT is the sample size, k = 1 + np is the number of regressors per equation\nT_d is the number of dummy observations\ntildeS is the residual sum of squares from the augmented regression Y Y_d on X X_d\ntildeS_d is the residual sum of squares from the dummy-only regression\nGamma_n(cdot) is the multivariate gamma function\n\nReference: Giannone, Lenza & Primiceri (2015), Carriero, Clark & Marcellino (2015)","category":"section"},{"location":"bayesian/#Julia-Implementation-3","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy variables\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\np = 2\n\n# Find optimal shrinkage using marginal likelihood\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=20)\n\nprintln(\"Optimal hyperparameters:\")\nprintln(\"  τ (overall tightness): \", round(best_hyper.tau, digits=4))\nprintln(\"  d (lag decay): \", best_hyper.d)\n\n# Compute log marginal likelihood\nlml = log_marginal_likelihood(Y, p, hyper)\n\nThe optimal tau balances fit and complexity: values near 0.01 produce near-dogmatic shrinkage to the random walk prior (good for high-dimensional systems), while values near 1.0 produce minimal shrinkage (approaching OLS). The marginal likelihood automatically penalizes overfitting, so the optimal tau increases with sample size as data evidence accumulates.","category":"section"},{"location":"bayesian/#Grid-Search-Options","page":"Bayesian VAR","title":"Grid Search Options","text":"# Custom optimization grid\nbest_hyper = optimize_hyperparameters(Y, p;\n    grid_size = 30,           # Number of grid points\n    tau_range = (0.01, 2.0),  # Range for τ\n    d_values = [1, 2, 3]      # Values for d\n)\n\n","category":"section"},{"location":"bayesian/#Conjugate-Posterior-Sampling","page":"Bayesian VAR","title":"Conjugate Posterior Sampling","text":"","category":"section"},{"location":"bayesian/#The-Normal-Inverse-Wishart-Posterior","page":"Bayesian VAR","title":"The Normal-Inverse-Wishart Posterior","text":"Because we use the conjugate Normal-Inverse-Wishart (NIW) prior, the posterior has a closed-form expression of the same family. Two samplers are available:\n\n:direct (default): Draws i.i.d. from the analytical NIW posterior. No burn-in or thinning is needed because each draw is independent.\n\n:gibbs: Two-block Gibbs sampler that alternates between drawing B  Sigma Y and Sigma  B Y. This is useful for extensions, diagnostics, or comparing with the direct sampler. Supports burnin and thinning parameters.","category":"section"},{"location":"bayesian/#BVARPosterior-Type","page":"Bayesian VAR","title":"BVARPosterior Type","text":"The result of estimate_bvar is a BVARPosterior{T} struct containing:\n\nField Type Description\nB_draws Array{T,3} Coefficient draws (n_draws × k × n), where k = 1 + n×p\nSigma_draws Array{T,3} Covariance draws (n_draws × n × n)\nn_draws Int Number of posterior draws\np Int Number of VAR lags\nn Int Number of variables\ndata Matrix{T} Original Y matrix (for residual computation downstream)\nprior Symbol Prior used (:normal or :minnesota)\nsampler Symbol Sampler used (:direct or :gibbs)","category":"section"},{"location":"bayesian/#Julia-Implementation-4","page":"Bayesian VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate BVAR with conjugate NIW sampler (using Y from Quick Start)\npost = estimate_bvar(Y, 2;\n    n_draws = 1000,       # Posterior draws\n    prior = :minnesota,   # Prior type\n    hyper = best_hyper,   # Hyperparameters\n    sampler = :direct,    # i.i.d. draws (default)\n    varnames = [\"INDPRO\", \"CPI\", \"FFR\"]\n)\n\n# Access posterior draws\npost.B_draws       # n_draws × k × n coefficient draws\npost.Sigma_draws   # n_draws × n × n covariance draws\npost.n_draws       # Number of draws\npost.sampler       # :direct or :gibbs\n\nThe :direct sampler is typically 10–100× faster than Gibbs because it avoids iterative sampling. For a 3-variable VAR(2) with n_draws=1000, estimation takes under 1 second. The :gibbs sampler produces correlated draws but provides a useful cross-check: if the posterior summaries from :direct and :gibbs agree closely, the implementation is validated.\n\nnote: Technical Note\nFor the :gibbs sampler, increase n_draws and use the thinning parameter to reduce autocorrelation. The :direct sampler produces i.i.d. draws, so n_draws=1000 is sufficient for most applications. When prior=:minnesota and hyper=nothing, tau is automatically optimized via marginal likelihood maximization (Giannone, Lenza & Primiceri, 2015).\n\nReference: Kadiyala & Karlsson (1997), Giannone, Lenza & Primiceri (2015)\n\n","category":"section"},{"location":"bayesian/#Posterior-Point-Estimates","page":"Bayesian VAR","title":"Posterior Point Estimates","text":"","category":"section"},{"location":"bayesian/#Extracting-VARModel-from-Posterior","page":"Bayesian VAR","title":"Extracting VARModel from Posterior","text":"After estimation, it is often useful to obtain a single VARModel based on the posterior mean or median. This allows using all frequentist tools (IRF, FEVD, HD, stationarity checks) on the Bayesian point estimate.\n\nusing MacroEconometricModels\n\n# After estimating the BVAR on FRED-MD [INDPRO, CPI, FFR]:\n# post = estimate_bvar(Y, 2; n_draws=1000, prior=:minnesota, hyper=best_hyper,\n#                      varnames=[\"INDPRO\", \"CPI\", \"FFR\"])\n\n# Extract VARModel with posterior mean parameters\nmean_model = posterior_mean_model(post)\n\n# Extract VARModel with posterior median parameters\nmedian_model = posterior_median_model(post)\n\n# Now use standard VAR tools\nstab = is_stationary(mean_model)\nprintln(\"Posterior mean model stationary: \", stab.is_stationary)\nprintln(\"Max eigenvalue modulus: \", round(stab.max_modulus, digits=4))\n\n# Frequentist IRF from the posterior mean\nirfs_mean = irf(mean_model, 20; method=:cholesky)\n\nThe posterior_mean_model averages the coefficient matrix B and covariance Sigma across all posterior draws, providing a single point estimate that integrates over parameter uncertainty. The posterior_median_model uses the element-wise median instead, which is more robust to outlier draws but may produce a Sigma that is not positive definite in edge cases. The BVARPosterior stores the original data, so residuals are computed automatically for downstream analyses like historical_decomposition.\n\n","category":"section"},{"location":"bayesian/#Bayesian-Impulse-Response-Functions","page":"Bayesian VAR","title":"Bayesian Impulse Response Functions","text":"","category":"section"},{"location":"bayesian/#Posterior-IRF-Distribution","page":"Bayesian VAR","title":"Posterior IRF Distribution","text":"For each posterior draw, we compute impulse responses, yielding a posterior distribution over IRFs. We report:\n\nPosterior median: Point estimate\nCredible intervals: 68% (16th-84th percentile) or 90% (5th-95th percentile)","category":"section"},{"location":"bayesian/#Cholesky-Identification","page":"Bayesian VAR","title":"Cholesky Identification","text":"using MacroEconometricModels\n\n# Bayesian IRF with Cholesky identification\nH = 20  # Horizon\nbirf_chol = irf(post, H; method=:cholesky)\n\n# birf_chol.quantiles is (H+1) × n × n × 3 array\n# [:, :, :, 1] = 16th percentile\n# [:, :, :, 2] = median\n# [:, :, :, 3] = 84th percentile\n\n# Response of INDPRO to a monetary policy shock (shock 3 = FFR)\nprintln(\"Bayesian IRF of INDPRO to monetary policy shock:\")\nfor h in [0, 4, 8, 12, 20]\n    med = round(birf_chol.quantiles[h+1, 1, 3, 2], digits=3)\n    lo = round(birf_chol.quantiles[h+1, 1, 3, 1], digits=3)\n    hi = round(birf_chol.quantiles[h+1, 1, 3, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\nThe posterior median IRF at h = 0 is zero by construction (INDPRO is ordered first, so it does not respond to the monetary shock on impact). The credible interval narrows toward zero at long horizons, consistent with a stationary system. Unlike frequentist bootstrap CIs, Bayesian credible intervals integrate over parameter uncertainty in B and Sigma across all posterior draws, providing a complete characterization of the uncertainty around the response of industrial production to a monetary policy shock.","category":"section"},{"location":"bayesian/#BayesianImpulseResponse-Return-Values","page":"Bayesian VAR","title":"BayesianImpulseResponse Return Values","text":"Field Type Description\nquantiles Array{T,4} (H+1) times n times n times 3: dim 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} (H+1) times n times n posterior mean IRF\nhorizon Int Maximum IRF horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels","category":"section"},{"location":"bayesian/#Sign-Restrictions","page":"Bayesian VAR","title":"Sign Restrictions","text":"# Define sign restriction: contractionary monetary shock\n# raises FFR, lowers INDPRO and CPI on impact\nfunction check_monetary_shock(irf_array)\n    return irf_array[1, 3, 3] > 0 &&   # FFR rises\n           irf_array[1, 1, 3] < 0 &&   # INDPRO falls\n           irf_array[1, 2, 3] < 0       # CPI falls\nend\n\n# Bayesian IRF with sign restrictions\nbirf_sign = irf(post, H;\n    method = :sign,\n    check_func = check_monetary_shock\n)\n\nprintln(\"Bayesian sign-restricted monetary shock → INDPRO:\")\nfor h in [0, 4, 8, 12]\n    med = round(birf_sign.quantiles[h+1, 1, 3, 2], digits=3)\n    lo = round(birf_sign.quantiles[h+1, 1, 3, 1], digits=3)\n    hi = round(birf_sign.quantiles[h+1, 1, 3, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\nThe sign-restricted IRFs are set-identified: the credible intervals combine both parameter uncertainty (from the posterior draws) and identification uncertainty (from the rotation Q). The sign restrictions ensure that a contractionary monetary shock raises the federal funds rate and lowers output and prices on impact, consistent with conventional monetary transmission.\n\n","category":"section"},{"location":"bayesian/#Bayesian-FEVD","page":"Bayesian VAR","title":"Bayesian FEVD","text":"","category":"section"},{"location":"bayesian/#Posterior-FEVD-Distribution","page":"Bayesian VAR","title":"Posterior FEVD Distribution","text":"Similarly, forecast error variance decomposition can be computed for each posterior draw:\n\nusing MacroEconometricModels\n\n# Bayesian FEVD\nbfevd = fevd(post, H; method=:cholesky)\n\n# How much of INDPRO forecast error is due to the monetary shock (shock 3)?\nfor h in [1, 4, 12, 20]\n    println(\"FEVD at h=$h:\")\n    med = round(bfevd.quantiles[h, 1, 3, 2] * 100, digits=1)\n    lo = round(bfevd.quantiles[h, 1, 3, 1] * 100, digits=1)\n    hi = round(bfevd.quantiles[h, 1, 3, 3] * 100, digits=1)\n    println(\"  Monetary shock → INDPRO: $med% [$lo%, $hi%]\")\nend\n\nAt short horizons, monetary shocks explain a small fraction of INDPRO forecast error variance — consistent with the Cholesky ordering where INDPRO is first and does not respond to the monetary shock on impact. As the horizon increases, the monetary transmission mechanism operates through lagged effects, and the monetary shock's contribution grows. The wide credible intervals at long horizons reflect cumulating parameter uncertainty through the VMA representation. Bayesian FEVD credible intervals are typically wider than frequentist bootstrap CIs because they integrate over the full posterior distribution of (B Sigma).","category":"section"},{"location":"bayesian/#BayesianFEVD-Return-Values","page":"Bayesian VAR","title":"BayesianFEVD Return Values","text":"Field Type Description\nquantiles Array{T,4} H times n times n times 3: dim 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} H times n times n posterior mean FEVD proportions\nhorizon Int Maximum horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels\n\n","category":"section"},{"location":"bayesian/#Information-Criteria","page":"Bayesian VAR","title":"Information Criteria","text":"","category":"section"},{"location":"bayesian/#Log-Likelihood","page":"Bayesian VAR","title":"Log-Likelihood","text":"For a Gaussian VAR, the log-likelihood is:\n\nlog L = -fracT cdot n2 log(2pi) - fracT2 logSigma - frac12 sum_t=1^T u_t Sigma^-1 u_t","category":"section"},{"location":"bayesian/#Marginal-Likelihood-(Bayesian)","page":"Bayesian VAR","title":"Marginal Likelihood (Bayesian)","text":"For Bayesian model comparison, we use the marginal likelihood (also called evidence):\n\np(Y  mathcalM) = int p(Y  theta mathcalM) p(theta  mathcalM)  dtheta\n\nModels with higher marginal likelihood better balance fit and complexity.\n\n","category":"section"},{"location":"bayesian/#Complete-Example","page":"Bayesian VAR","title":"Complete Example","text":"using MacroEconometricModels\nusing Random\n\n# Load FRED-MD: industrial production, CPI, federal funds rate\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\np = 2\n\n# Step 1: Optimize hyperparameters\nprintln(\"Optimizing hyperparameters...\")\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=20)\nprintln(\"Optimal τ: \", round(best_hyper.tau, digits=4))\n\n# Step 2: Estimate BVAR\nprintln(\"\\nEstimating BVAR with conjugate NIW sampler...\")\nRandom.seed!(42)  # Reproducible posterior draws\npost = estimate_bvar(Y, p;\n    n_draws = 1000,\n    prior = :minnesota,\n    hyper = best_hyper,\n    varnames = [\"INDPRO\", \"CPI\", \"FFR\"]\n)\n\n# Step 3: Compute Bayesian IRF — response to monetary policy shock\nH = 20\nbirf = irf(post, H; method=:cholesky)\n\n# Step 4: Report results — INDPRO response to monetary shock (shock 3)\nprintln(\"\\nBayesian IRF (monetary shock → INDPRO):\")\nfor h in [0, 4, 8, 12, 20]\n    med = round(birf.quantiles[h+1, 1, 3, 2], digits=3)\n    lo = round(birf.quantiles[h+1, 1, 3, 1], digits=3)\n    hi = round(birf.quantiles[h+1, 1, 3, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend\n\nThis workflow demonstrates the complete Bayesian pipeline using FRED-MD data: hyperparameter optimization selects the optimal shrinkage tau via marginal likelihood, then the conjugate NIW sampler produces posterior draws from which we compute IRFs with credible intervals. The Cholesky ordering [INDPRO, CPI, FFR] identifies a monetary policy shock that raises the federal funds rate, and the credible intervals for the INDPRO response characterize the uncertainty around the output effects of monetary policy.\n\n","category":"section"},{"location":"bayesian/#Large-BVAR","page":"Bayesian VAR","title":"Large BVAR","text":"","category":"section"},{"location":"bayesian/#Handling-High-Dimensional-Systems","page":"Bayesian VAR","title":"Handling High-Dimensional Systems","text":"For large VAR systems (many variables), the Minnesota prior becomes essential:\n\nusing MacroEconometricModels\n\n# Load full FRED-MD dataset (100+ variables)\nfred = load_example(:fred_md)\n\n# Select variables with safe transformations (avoid log of non-positive values)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\np = 4\n\n# Stronger shrinkage for large systems\nhyper_large = MinnesotaHyperparameters(\n    tau = 0.1,      # Tighter prior\n    decay = 2.0,\n    lambda = 1.0,\n    mu = 0.5,       # Penalize cross-variable coefficients\n    omega = 1.0\n)\n\n# Or optimize automatically\nbest_hyper = optimize_hyperparameters(X, p)\n\nFor large systems (20+ variables), the number of VAR parameters (n^2 p + n) grows quadratically with the number of variables, quickly exceeding the sample size. The Minnesota prior prevents overfitting by shrinking cross-variable coefficients toward zero (mu=0.5) and applying strong overall tightness (tau=0.1). Bańbura, Giannone & Reichlin (2010) show that BVAR with optimized shrinkage outperforms both unrestricted VAR and small-scale models for macroeconomic forecasting.\n\nReference: Bańbura, Giannone & Reichlin (2010)\n\n","category":"section"},{"location":"bayesian/#References","page":"Bayesian VAR","title":"References","text":"","category":"section"},{"location":"bayesian/#Minnesota-Prior-and-BVAR","page":"Bayesian VAR","title":"Minnesota Prior and BVAR","text":"Bańbura, Marta, Domenico Giannone, and Lucrezia Reichlin. 2010. \"Large Bayesian Vector Auto Regressions.\" Journal of Applied Econometrics 25 (1): 71–92. https://doi.org/10.1002/jae.1137\nCarriero, Andrea, Todd E. Clark, and Massimiliano Marcellino. 2015. \"Bayesian VARs: Specification Choices and Forecast Accuracy.\" Journal of Applied Econometrics 30 (1): 46–73. https://doi.org/10.1002/jae.2315\nDoan, Thomas, Robert Litterman, and Christopher Sims. 1984. \"Forecasting and Conditional Projection Using Realistic Prior Distributions.\" Econometric Reviews 3 (1): 1–100. https://doi.org/10.1080/07474938408800053\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nKadiyala, K. Rao, and Sune Karlsson. 1997. \"Numerical Methods for Estimation and Inference in Bayesian VAR-Models.\" Journal of Applied Econometrics 12 (2): 99–132. https://doi.org/10.1002/(SICI)1099-1255(199703)12:2<99::AID-JAE429>3.0.CO;2-A\nLitterman, Robert B. 1986. \"Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.\" Journal of Business & Economic Statistics 4 (1): 25–38. https://doi.org/10.1080/07350015.1986.10509491","category":"section"},{"location":"bayesian/#Conjugate-Posterior-Sampling-2","page":"Bayesian VAR","title":"Conjugate Posterior Sampling","text":"Kim, Sangjoon, Neil Shephard, and Siddhartha Chib. 1998. \"Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models.\" Review of Economic Studies 65 (3): 361–393. https://doi.org/10.1111/1467-937X.00050","category":"section"},{"location":"api_functions/#api_functions","page":"Functions","title":"API Functions","text":"This page documents all functions in MacroEconometricModels.jl, organized by module.\n\n","category":"section"},{"location":"api_functions/#Data-Management","page":"Functions","title":"Data Management","text":"","category":"section"},{"location":"api_functions/#Validation-and-Cleaning","page":"Functions","title":"Validation and Cleaning","text":"","category":"section"},{"location":"api_functions/#FRED-Transformations","page":"Functions","title":"FRED Transformations","text":"","category":"section"},{"location":"api_functions/#Filtering","page":"Functions","title":"Filtering","text":"","category":"section"},{"location":"api_functions/#Summary-Statistics","page":"Functions","title":"Summary Statistics","text":"","category":"section"},{"location":"api_functions/#Panel-Data","page":"Functions","title":"Panel Data","text":"","category":"section"},{"location":"api_functions/#Data-Accessors-and-Conversion","page":"Functions","title":"Data Accessors and Conversion","text":"","category":"section"},{"location":"api_functions/#Time-Series-Filters","page":"Functions","title":"Time Series Filters","text":"","category":"section"},{"location":"api_functions/#ARIMA-Models","page":"Functions","title":"ARIMA Models","text":"","category":"section"},{"location":"api_functions/#Estimation","page":"Functions","title":"Estimation","text":"","category":"section"},{"location":"api_functions/#Forecasting","page":"Functions","title":"Forecasting","text":"","category":"section"},{"location":"api_functions/#Order-Selection","page":"Functions","title":"Order Selection","text":"","category":"section"},{"location":"api_functions/#ARIMA-Accessors","page":"Functions","title":"ARIMA Accessors","text":"","category":"section"},{"location":"api_functions/#VAR-Estimation","page":"Functions","title":"VAR Estimation","text":"","category":"section"},{"location":"api_functions/#Frequentist-Estimation","page":"Functions","title":"Frequentist Estimation","text":"","category":"section"},{"location":"api_functions/#Bayesian-Estimation","page":"Functions","title":"Bayesian Estimation","text":"","category":"section"},{"location":"api_functions/#Prior-Specification","page":"Functions","title":"Prior Specification","text":"","category":"section"},{"location":"api_functions/#VECM-Estimation","page":"Functions","title":"VECM Estimation","text":"","category":"section"},{"location":"api_functions/#VECM-Analysis-and-Forecasting","page":"Functions","title":"VECM Analysis and Forecasting","text":"","category":"section"},{"location":"api_functions/#Structural-Identification","page":"Functions","title":"Structural Identification","text":"","category":"section"},{"location":"api_functions/#Mountford-Uhlig-(2009)-Penalty-Function","page":"Functions","title":"Mountford-Uhlig (2009) Penalty Function","text":"","category":"section"},{"location":"api_functions/#Innovation-Accounting","page":"Functions","title":"Innovation Accounting","text":"","category":"section"},{"location":"api_functions/#Impulse-Response-Functions","page":"Functions","title":"Impulse Response Functions","text":"","category":"section"},{"location":"api_functions/#Forecast-Error-Variance-Decomposition","page":"Functions","title":"Forecast Error Variance Decomposition","text":"","category":"section"},{"location":"api_functions/#Historical-Decomposition","page":"Functions","title":"Historical Decomposition","text":"","category":"section"},{"location":"api_functions/#Summary-Tables","page":"Functions","title":"Summary Tables","text":"","category":"section"},{"location":"api_functions/#Local-Projections","page":"Functions","title":"Local Projections","text":"","category":"section"},{"location":"api_functions/#Core-LP-Estimation-and-Covariance","page":"Functions","title":"Core LP Estimation and Covariance","text":"","category":"section"},{"location":"api_functions/#LP-IV-(Stock-and-Watson-2018)","page":"Functions","title":"LP-IV (Stock & Watson 2018)","text":"","category":"section"},{"location":"api_functions/#Smooth-LP-(Barnichon-and-Brownlees-2019)","page":"Functions","title":"Smooth LP (Barnichon & Brownlees 2019)","text":"","category":"section"},{"location":"api_functions/#State-Dependent-LP-(Auerbach-and-Gorodnichenko-2013)","page":"Functions","title":"State-Dependent LP (Auerbach & Gorodnichenko 2013)","text":"","category":"section"},{"location":"api_functions/#Propensity-Score-LP-(Angrist-et-al.-2018)","page":"Functions","title":"Propensity Score LP (Angrist et al. 2018)","text":"","category":"section"},{"location":"api_functions/#LP-Forecasting","page":"Functions","title":"LP Forecasting","text":"","category":"section"},{"location":"api_functions/#LP-FEVD-(Gorodnichenko-and-Lee-2019)","page":"Functions","title":"LP-FEVD (Gorodnichenko & Lee 2019)","text":"","category":"section"},{"location":"api_functions/#Factor-Models","page":"Functions","title":"Factor Models","text":"","category":"section"},{"location":"api_functions/#Static-Factor-Model","page":"Functions","title":"Static Factor Model","text":"","category":"section"},{"location":"api_functions/#Dynamic-Factor-Model","page":"Functions","title":"Dynamic Factor Model","text":"","category":"section"},{"location":"api_functions/#Generalized-Dynamic-Factor-Model","page":"Functions","title":"Generalized Dynamic Factor Model","text":"","category":"section"},{"location":"api_functions/#Panel-VAR","page":"Functions","title":"Panel VAR","text":"","category":"section"},{"location":"api_functions/#Estimation-2","page":"Functions","title":"Estimation","text":"","category":"section"},{"location":"api_functions/#Structural-Analysis","page":"Functions","title":"Structural Analysis","text":"","category":"section"},{"location":"api_functions/#Bootstrap","page":"Functions","title":"Bootstrap","text":"","category":"section"},{"location":"api_functions/#Specification-Tests","page":"Functions","title":"Specification Tests","text":"","category":"section"},{"location":"api_functions/#GMM-Utilities","page":"Functions","title":"GMM Utilities","text":"","category":"section"},{"location":"api_functions/#GMM-Estimation","page":"Functions","title":"GMM Estimation","text":"","category":"section"},{"location":"api_functions/#Unit-Root-and-Cointegration-Tests","page":"Functions","title":"Unit Root and Cointegration Tests","text":"","category":"section"},{"location":"api_functions/#Model-Comparison-Tests","page":"Functions","title":"Model Comparison Tests","text":"","category":"section"},{"location":"api_functions/#Granger-Causality-Tests","page":"Functions","title":"Granger Causality Tests","text":"","category":"section"},{"location":"api_functions/#Volatility-Models","page":"Functions","title":"Volatility Models","text":"","category":"section"},{"location":"api_functions/#ARCH-Estimation-and-Diagnostics","page":"Functions","title":"ARCH Estimation and Diagnostics","text":"","category":"section"},{"location":"api_functions/#GARCH-Estimation-and-Diagnostics","page":"Functions","title":"GARCH Estimation and Diagnostics","text":"","category":"section"},{"location":"api_functions/#Stochastic-Volatility","page":"Functions","title":"Stochastic Volatility","text":"","category":"section"},{"location":"api_functions/#Volatility-Forecasting","page":"Functions","title":"Volatility Forecasting","text":"","category":"section"},{"location":"api_functions/#Volatility-Accessors","page":"Functions","title":"Volatility Accessors","text":"","category":"section"},{"location":"api_functions/#Volatility-StatsAPI-Interface","page":"Functions","title":"Volatility StatsAPI Interface","text":"","category":"section"},{"location":"api_functions/#Nowcasting","page":"Functions","title":"Nowcasting","text":"","category":"section"},{"location":"api_functions/#Estimation-3","page":"Functions","title":"Estimation","text":"","category":"section"},{"location":"api_functions/#Nowcast-and-Forecast","page":"Functions","title":"Nowcast and Forecast","text":"","category":"section"},{"location":"api_functions/#News-Decomposition","page":"Functions","title":"News Decomposition","text":"","category":"section"},{"location":"api_functions/#Panel-Balancing","page":"Functions","title":"Panel Balancing","text":"","category":"section"},{"location":"api_functions/#Display-and-References","page":"Functions","title":"Display and References","text":"","category":"section"},{"location":"api_functions/#Non-Gaussian-Identification","page":"Functions","title":"Non-Gaussian Identification","text":"","category":"section"},{"location":"api_functions/#Normality-Tests","page":"Functions","title":"Normality Tests","text":"","category":"section"},{"location":"api_functions/#ICA-based-Identification","page":"Functions","title":"ICA-based Identification","text":"","category":"section"},{"location":"api_functions/#Non-Gaussian-ML-Identification","page":"Functions","title":"Non-Gaussian ML Identification","text":"","category":"section"},{"location":"api_functions/#Heteroskedasticity-Identification","page":"Functions","title":"Heteroskedasticity Identification","text":"","category":"section"},{"location":"api_functions/#Identifiability-Tests","page":"Functions","title":"Identifiability Tests","text":"","category":"section"},{"location":"api_functions/#Covariance-Estimators","page":"Functions","title":"Covariance Estimators","text":"","category":"section"},{"location":"api_functions/#Utility-Functions","page":"Functions","title":"Utility Functions","text":"","category":"section"},{"location":"api_functions/#MacroEconometricModels.diagnose","page":"Functions","title":"MacroEconometricModels.diagnose","text":"diagnose(d::AbstractMacroData) -> DataDiagnostic\n\nScan data for NaN, Inf, constant columns, and very short series.\n\nExamples\n\nd = TimeSeriesData(randn(100, 3))\ndiag = diagnose(d)\ndiag.is_clean  # true if no issues\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.fix","page":"Functions","title":"MacroEconometricModels.fix","text":"fix(d::TimeSeriesData; method=:listwise) -> TimeSeriesData\n\nFix data issues and return a clean copy.\n\nMethods\n\n:listwise — drop rows with any NaN or Inf (default)\n:interpolate — linear interpolation for interior NaN, forward-fill edges\n:mean — replace NaN with column mean of finite values\n\nInf values are always replaced with NaN first (then handled by the chosen method). Constant columns are dropped with a warning.\n\nExamples\n\nd = TimeSeriesData([1.0 NaN; 2.0 3.0; 3.0 4.0])\nd_clean = fix(d; method=:listwise)  # drops row 1\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.validate_for_model","page":"Functions","title":"MacroEconometricModels.validate_for_model","text":"validate_for_model(d::AbstractMacroData, model_type::Symbol)\n\nCheck that data is compatible with the specified model type. Throws ArgumentError on mismatch.\n\nModel types requiring multivariate data (n_vars ≥ 2)\n\n:var, :vecm, :bvar, :factors, :dynamic_factors, :gdfm\n\nModel types requiring univariate data (n_vars == 1)\n\n:arima, :ar, :ma, :arma, :arch, :garch, :egarch, :gjr_garch, :sv, :hp_filter, :hamilton_filter, :beveridge_nelson, :baxter_king, :boosted_hp, :adf, :kpss, :pp, :za, :ngperron\n\nModel types accepting any dimensionality\n\n:lp, :lp_iv, :smooth_lp, :state_lp, :propensity_lp, :gmm\n\nExamples\n\nd = TimeSeriesData(randn(100, 3))\nvalidate_for_model(d, :var)    # OK\nvalidate_for_model(d, :arima)  # throws ArgumentError\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.apply_tcode","page":"Functions","title":"MacroEconometricModels.apply_tcode","text":"apply_tcode(y::AbstractVector{<:Real}, tcode::Int) -> Vector{Float64}\n\nApply FRED transformation code to a univariate series.\n\nCodes 4–7 require strictly positive data. The output vector is shorter than the input for difference-based codes:\n\ntcode 1: same length\ntcode 2, 4, 5: length T-1\ntcode 3, 6, 7: length T-2\n\nExamples\n\ny = [100.0, 102.0, 105.0, 103.0, 108.0]\napply_tcode(y, 5)  # log first differences (approx growth rates)\n\n\n\n\n\napply_tcode(d::TimeSeriesData, tcodes::Vector{Int}) -> TimeSeriesData\n\nApply per-variable FRED transformation codes. Rows are trimmed consistently (to the shortest transformed series).\n\nExamples\n\nd = TimeSeriesData(rand(200, 3) .+ 1; varnames=[\"GDP\",\"CPI\",\"FFR\"])\nd2 = apply_tcode(d, [5, 5, 1])  # log-diff GDP and CPI, leave FFR in levels\n\n\n\n\n\napply_tcode(d::TimeSeriesData, tcode::Int) -> TimeSeriesData\n\nApply the same FRED transformation code to all variables.\n\n\n\n\n\napply_tcode(d::TimeSeriesData) -> TimeSeriesData\n\nConvenience method: apply the transformation codes stored in d.tcode. Equivalent to apply_tcode(d, d.tcode).\n\nExamples\n\nd = load_example(:fred_md)  # tcode already set\nd_transformed = apply_tcode(d)\n\n\n\n\n\napply_tcode(d::PanelData, tcodes::Vector{Int}) -> PanelData\n\nApply per-variable FRED transformation codes to a PanelData container, processing each group independently. Each group is trimmed to its own common length after transformation, then reassembled.\n\nExamples\n\npd = xtset(df, :id, :t)\npd2 = apply_tcode(pd, [5, 5, 1])  # log-diff first two vars, leave third in levels\n\n\n\n\n\napply_tcode(d::PanelData, tcode::Int) -> PanelData\n\nApply the same FRED transformation code to all variables in a PanelData container.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.inverse_tcode","page":"Functions","title":"MacroEconometricModels.inverse_tcode","text":"inverse_tcode(y::AbstractVector{<:Real}, tcode::Int;\n              x_prev::Union{AbstractVector{<:Real},Nothing}=nothing) -> Vector{Float64}\n\nUndo a FRED transformation to recover the original series.\n\nFor difference-based codes (2, 3, 5, 6, 7), x_prev must supply the initial values needed for reconstruction:\n\ntcode 2: x_prev = [x₀] (1 value)\ntcode 3: x_prev = [x₀, x₁] (2 values, original levels)\ntcode 5: x_prev = [x₀] (1 value, original level)\ntcode 6: x_prev = [x₀, x₁] (2 values, original levels)\ntcode 7: x_prev = [x₀, x₁] (2 values, original levels)\n\nExamples\n\ny = [100.0, 102.0, 105.0]\nyd = apply_tcode(y, 2)                        # [2.0, 3.0]\ninverse_tcode(yd, 2; x_prev=[100.0])          # [102.0, 105.0]\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.apply_filter","page":"Functions","title":"MacroEconometricModels.apply_filter","text":"apply_filter(d::TimeSeriesData, specs::AbstractVector; component=:cycle, kwargs...)\n\nApply per-variable filter specifications to a TimeSeriesData container.\n\nEach element of specs can be:\n\nSymbol — filter name (:hp, :hamilton, :bn, :bk, :boosted_hp)\nAbstractFilterResult — a pre-computed filter result\nTuple{filter, Symbol} — filter with per-variable component override (e.g., (:hp, :trend))\nnothing — pass-through (no filtering for this variable)\n\nThe output is trimmed to the intersection of valid ranges across all variables.\n\nArguments\n\nd::TimeSeriesData — input data\nspecs::AbstractVector — per-variable filter specifications (length must equal n_vars)\n\nKeyword Arguments\n\ncomponent::Symbol=:cycle — default component to extract (:cycle or :trend)\nAdditional kwargs are forwarded to filter functions\n\nReturns\n\nA new TimeSeriesData with filtered data, trimmed to the common valid range.\n\nExamples\n\nd = TimeSeriesData(cumsum(randn(200, 3), dims=1); varnames=[\"GDP\",\"CPI\",\"FFR\"])\n\n# Per-variable: HP cycle for GDP, Hamilton cycle for CPI, pass-through FFR\nd2 = apply_filter(d, [:hp, :hamilton, nothing])\n\n# Per-variable component overrides via tuples\nd3 = apply_filter(d, [(:hp, :trend), (:hamilton, :cycle), nothing])\n\n\n\n\n\napply_filter(d::TimeSeriesData, spec::Union{Symbol, AbstractFilterResult};\n             component=:cycle, vars=nothing, kwargs...)\n\nApply a single filter to all variables (or a subset specified by vars).\n\nVariables not in vars are passed through unchanged.\n\nArguments\n\nd::TimeSeriesData — input data\nspec — filter symbol (:hp, :hamilton, :bn, :bk, :boosted_hp) or pre-computed result\n\nKeyword Arguments\n\ncomponent::Symbol=:cycle — component to extract (:cycle or :trend)\nvars::Union{Nothing, Vector{String}, Vector{Int}} — variables to filter (default: all)\nAdditional kwargs are forwarded to filter functions\n\nExamples\n\nd = TimeSeriesData(cumsum(randn(200, 3), dims=1); varnames=[\"GDP\",\"CPI\",\"FFR\"])\n\n# HP cycle for all variables\nd_hp = apply_filter(d, :hp)\n\n# HP trend for selected variables only\nd_sel = apply_filter(d, :hp; vars=[\"GDP\", \"CPI\"], component=:trend)\n\n\n\n\n\napply_filter(d::PanelData, spec; component=:cycle, vars=nothing, kwargs...)\n\nApply filters to a PanelData container group-by-group.\n\nEach group is extracted via group_data, filtered, and the results are reassembled into a new PanelData. Each group is trimmed independently to its own common valid range (groups may have different resulting lengths if unbalanced).\n\nArguments\n\nd::PanelData — input panel data\nspec — filter specification: a single Symbol, AbstractFilterResult, or AbstractVector of per-variable specs (same formats as TimeSeriesData)\n\nKeyword Arguments\n\ncomponent::Symbol=:cycle — component to extract\nvars — variables to filter (others pass-through)\nAdditional kwargs forwarded to filter functions\n\nExamples\n\npd = xtset(df, :id, :t)\npd_hp = apply_filter(pd, :hp; component=:cycle)\npd_sel = apply_filter(pd, :hp; vars=[\"GDP\"], component=:trend)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.describe_data","page":"Functions","title":"MacroEconometricModels.describe_data","text":"describe_data(d::AbstractMacroData) -> DataSummary\n\nCompute per-variable summary statistics (N, Mean, Std, Min, P25, Median, P75, Max, Skewness, Kurtosis). NaN values are excluded from all computations.\n\nFor PanelData, also prints panel dimensions via panel_summary.\n\nExamples\n\nd = TimeSeriesData(randn(200, 3); varnames=[\"GDP\",\"CPI\",\"FFR\"], frequency=Quarterly)\ndescribe_data(d)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.xtset","page":"Functions","title":"MacroEconometricModels.xtset","text":"xtset(df::DataFrame, group_col::Symbol, time_col::Symbol;\n      varnames=nothing, frequency=Other, tcode=nothing) -> PanelData{Float64}\n\nConstruct a PanelData container from a DataFrame, analogous to Stata's xtset.\n\nExtracts all numeric columns (excluding group_col and time_col), sorts by (group, time), validates no duplicate (group, time) pairs, and detects whether the panel is balanced.\n\nArguments\n\ndf::DataFrame — input data\ngroup_col::Symbol — column name identifying groups (entities)\ntime_col::Symbol — column name identifying time periods\n\nKeyword Arguments\n\nvarnames::Union{Vector{String},Nothing} — override variable names (default: column names)\nfrequency::Frequency — data frequency (default: Other)\ntcode::Union{Vector{Int},Nothing} — transformation codes per variable\n\nExamples\n\nusing DataFrames\ndf = DataFrame(id=repeat(1:3, inner=50), t=repeat(1:50, 3),\n               x=randn(150), y=randn(150))\npd = xtset(df, :id, :t)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.isbalanced","page":"Functions","title":"MacroEconometricModels.isbalanced","text":"isbalanced(d::PanelData) -> Bool\n\nReturn true if all groups have the same number of observations.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.groups","page":"Functions","title":"MacroEconometricModels.groups","text":"groups(d::PanelData) -> Vector{String}\n\nReturn the group names.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ngroups","page":"Functions","title":"MacroEconometricModels.ngroups","text":"ngroups(d::PanelData) -> Int\n\nReturn the number of groups.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.group_data","page":"Functions","title":"MacroEconometricModels.group_data","text":"group_data(d::PanelData, g) -> TimeSeriesData\n\nExtract data for a single group as a TimeSeriesData container.\n\ng can be an integer group index or a string group name.\n\nExamples\n\npd = xtset(df, :id, :t)\ng1 = group_data(pd, 1)       # by index\ng1 = group_data(pd, \"1\")     # by name\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.panel_summary","page":"Functions","title":"MacroEconometricModels.panel_summary","text":"panel_summary(d::PanelData)\n\nDisplay a summary table of the panel structure: number of groups, observations per group (min, mean, max), and balance status.\n\nExamples\n\npd = xtset(df, :id, :t)\npanel_summary(pd)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#StatsAPI.nobs-Tuple{TimeSeriesData}","page":"Functions","title":"StatsAPI.nobs","text":"StatsAPI.nobs(d::AbstractMacroData)\n\nReturn the number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.nvars","page":"Functions","title":"MacroEconometricModels.nvars","text":"nvars(d::AbstractMacroData)\n\nReturn the number of variables.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.varnames","page":"Functions","title":"MacroEconometricModels.varnames","text":"varnames(d::AbstractMacroData)\n\nReturn variable names.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.frequency","page":"Functions","title":"MacroEconometricModels.frequency","text":"frequency(d::TimeSeriesData)\nfrequency(d::PanelData)\n\nReturn the data frequency.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.time_index","page":"Functions","title":"MacroEconometricModels.time_index","text":"time_index(d::TimeSeriesData)\n\nReturn the integer time index vector.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.obs_id","page":"Functions","title":"MacroEconometricModels.obs_id","text":"obs_id(d::CrossSectionData)\n\nReturn the observation identifier vector.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.desc","page":"Functions","title":"MacroEconometricModels.desc","text":"desc(d::AbstractMacroData) -> String\n\nReturn the dataset description. Returns \"\" if no description has been set.\n\nExamples\n\nd = TimeSeriesData(randn(100, 3); desc=\"US macro quarterly data\")\ndesc(d)  # \"US macro quarterly data\"\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.vardesc","page":"Functions","title":"MacroEconometricModels.vardesc","text":"vardesc(d::AbstractMacroData) -> Dict{String,String}\n\nReturn the dictionary of per-variable descriptions.\n\nExamples\n\nd = TimeSeriesData(randn(100, 2); varnames=[\"GDP\",\"CPI\"],\n    vardesc=Dict(\"GDP\" => \"Real GDP growth\", \"CPI\" => \"Consumer prices\"))\nvardesc(d)  # Dict(\"GDP\" => \"Real GDP growth\", \"CPI\" => \"Consumer prices\")\n\n\n\n\n\nvardesc(d::AbstractMacroData, name::String) -> String\n\nReturn the description for variable name. Throws ArgumentError if no description exists for that variable.\n\nExamples\n\nvardesc(d, \"GDP\")  # \"Real GDP growth\"\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.rename_vars!","page":"Functions","title":"MacroEconometricModels.rename_vars!","text":"rename_vars!(d::AbstractMacroData, old => new)\nrename_vars!(d::AbstractMacroData, names::Vector{String})\n\nRename variables in a data container. With a Pair, renames a single variable. With a Vector{String}, replaces all variable names. Also updates vardesc keys.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.set_time_index!","page":"Functions","title":"MacroEconometricModels.set_time_index!","text":"set_time_index!(d::TimeSeriesData, idx::Vector{Int})\n\nSet the time index for a TimeSeriesData container.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.set_obs_id!","page":"Functions","title":"MacroEconometricModels.set_obs_id!","text":"set_obs_id!(d::CrossSectionData, ids::Vector{Int})\n\nSet observation identifiers for a CrossSectionData container.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.set_desc!","page":"Functions","title":"MacroEconometricModels.set_desc!","text":"set_desc!(d::AbstractMacroData, text::String)\n\nSet the dataset description.\n\nExamples\n\nd = TimeSeriesData(randn(100, 3))\nset_desc!(d, \"US macroeconomic quarterly data 1959-2024\")\ndesc(d)  # \"US macroeconomic quarterly data 1959-2024\"\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.set_vardesc!","page":"Functions","title":"MacroEconometricModels.set_vardesc!","text":"set_vardesc!(d::AbstractMacroData, name::String, text::String)\n\nSet the description for a single variable. The variable must exist in the data.\n\nExamples\n\nd = TimeSeriesData(randn(100, 2); varnames=[\"GDP\", \"CPI\"])\nset_vardesc!(d, \"GDP\", \"Real Gross Domestic Product, seasonally adjusted\")\nset_vardesc!(d, \"CPI\", \"Consumer Price Index for All Urban Consumers\")\nvardesc(d, \"GDP\")  # \"Real Gross Domestic Product, seasonally adjusted\"\n\n\n\n\n\nset_vardesc!(d::AbstractMacroData, descriptions::Dict{String,String})\n\nSet descriptions for multiple variables at once. All keys must be valid variable names.\n\nExamples\n\nset_vardesc!(d, Dict(\"GDP\" => \"Real GDP\", \"CPI\" => \"Consumer prices\"))\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.to_matrix","page":"Functions","title":"MacroEconometricModels.to_matrix","text":"to_matrix(d::TimeSeriesData) -> Matrix\n\nReturn the raw data matrix from a TimeSeriesData container.\n\n\n\n\n\nto_matrix(d::PanelData) -> Matrix\n\nReturn the raw stacked data matrix from a PanelData container.\n\n\n\n\n\nto_matrix(d::CrossSectionData) -> Matrix\n\nReturn the raw data matrix from a CrossSectionData container.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.to_vector","page":"Functions","title":"MacroEconometricModels.to_vector","text":"to_vector(d::TimeSeriesData) -> Vector\n\nReturn the data as a vector (requires exactly 1 variable).\n\n\n\n\n\nto_vector(d::TimeSeriesData, var::Int) -> Vector\n\nReturn a single column by index.\n\n\n\n\n\nto_vector(d::TimeSeriesData, var::String) -> Vector\n\nReturn a single column by name.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.load_example","page":"Functions","title":"MacroEconometricModels.load_example","text":"load_example(name::Symbol) -> AbstractMacroData\n\nLoad a built-in example dataset.\n\nAvailable Datasets\n\n:fred_md — FRED-MD Monthly Database, January 2026 vintage (126 variables × 804 months) → TimeSeriesData\n:fred_qd — FRED-QD Quarterly Database, January 2026 vintage (245 variables × 268 quarters) → TimeSeriesData\n:pwt — Penn World Table 10.01, 38 OECD countries (42 variables × 74 years, 1950–2023) → PanelData\n\nFor time series datasets, the returned TimeSeriesData includes variable names, transformation codes, frequency, per-variable descriptions (via vardesc), dataset description (via desc), and bibliographic references (via refs).\n\nFor panel datasets, the returned PanelData includes country identifiers as groups, year identifiers as time index, variable descriptions, and references.\n\nExamples\n\n# Load FRED-MD\nmd = load_example(:fred_md)\nnobs(md)       # 804\nnvars(md)      # 126\ndesc(md)       # \"FRED-MD Monthly Database, January 2026 Vintage (McCracken & Ng 2016)\"\nvardesc(md, \"INDPRO\")  # \"IP Index\"\nrefs(md)       # McCracken & Ng (2016)\n\n# Apply recommended transformations\nmd_transformed = apply_tcode(md, md.tcode)\n\n# Load FRED-QD\nqd = load_example(:fred_qd)\n\n# Load Penn World Table (panel data)\npwt = load_example(:pwt)\nnobs(pwt)         # 2812 (38 countries × 74 years)\nnvars(pwt)        # 42\nngroups(pwt)      # 38\ngroups(pwt)       # [\"AUS\", \"AUT\", ..., \"USA\"]\nisbalanced(pwt)   # true\ng = group_data(pwt, \"USA\")  # extract single country as TimeSeriesData\nrefs(pwt)         # Feenstra, Inklaar & Timmer (2015)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.hp_filter","page":"Functions","title":"MacroEconometricModels.hp_filter","text":"hp_filter(y::AbstractVector; lambda=1600.0) -> HPFilterResult\n\nApply the Hodrick-Prescott filter to decompose time series y into trend and cycle.\n\nSolves the optimization problem:\n\nmin_tau sum_t=1^T (y_t - tau_t)^2 + lambda sum_t=2^T-1 (tau_t+1 - 2tau_t + tau_t-1)^2\n\nImplementation uses a sparse pentadiagonal Cholesky factorization for O(T) cost.\n\nArguments\n\ny::AbstractVector: Time series data (length ≥ 3)\n\nKeywords\n\nlambda::Real=1600.0: Smoothing parameter. Common values: 6.25 (annual), 1600 (quarterly, default), 129600 (monthly)\n\nReturns\n\nHPFilterResult{T} with fields trend, cycle, lambda, T_obs\n\nExamples\n\ny = cumsum(randn(200))\nresult = hp_filter(y)\nresult = hp_filter(y; lambda=6.25)  # annual data\n\nReferences\n\nHodrick, R. J., & Prescott, E. C. (1997). JMCB 29(1): 1–16.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.hamilton_filter","page":"Functions","title":"MacroEconometricModels.hamilton_filter","text":"hamilton_filter(y::AbstractVector; h=8, p=4) -> HamiltonFilterResult\n\nApply the Hamilton (2018) regression filter for trend-cycle decomposition.\n\nRegresses y_t+h on 1 y_t y_t-1 ldots y_t-p+1 by OLS. The residuals form the cyclical component and the fitted values form the trend.\n\nThe filter loses h + p - 1 observations at the start of the sample.\n\nArguments\n\ny::AbstractVector: Time series data\n\nKeywords\n\nh::Int=8: Forecast horizon (default 8 for quarterly data = 2 years)\np::Int=4: Number of lags in the regression (default 4 for quarterly data)\n\nReturns\n\nHamiltonFilterResult{T} with fields trend, cycle, beta, h, p, T_obs, valid_range\n\nExamples\n\ny = cumsum(randn(200))\nresult = hamilton_filter(y)                 # quarterly defaults\nresult = hamilton_filter(y; h=24, p=12)     # monthly data (2-year horizon)\n\nReferences\n\nHamilton, J. D. (2018). REStat 100(5): 831–843.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.beveridge_nelson","page":"Functions","title":"MacroEconometricModels.beveridge_nelson","text":"beveridge_nelson(y::AbstractVector; method=:arima, p=:auto, q=:auto, max_terms=500, cycle_order=2) -> BeveridgeNelsonResult\n\nCompute the Beveridge-Nelson decomposition of time series y.\n\nAssumes y is I(1) and decomposes it into:\n\ny_t = tau_t + c_t\n\nwhere tau_t is a random walk with drift (permanent component) and c_t is a stationary transitory component.\n\nMethods\n\n:arima (default): Classic BN via ARIMA representation (Beveridge & Nelson, 1981)\n:statespace: Correlated UC model via MLE + Kalman smoother (Morley, Nelson & Zivot, 2003)\n\nArguments\n\ny::AbstractVector: Time series data (assumed I(1), length ≥ 10)\n\nKeywords\n\nmethod::Symbol=:arima: Decomposition method\np: AR order for ARMA model of Δy (method=:arima). :auto uses auto_arima\nq: MA order for ARMA model of Δy (method=:arima). :auto uses auto_arima\nmax_terms::Int=500: Maximum ψ-weights for MA(∞) truncation (method=:arima)\ncycle_order::Int=2: AR order for cyclical component (method=:statespace, 1 or 2)\n\nReturns\n\nBeveridgeNelsonResult{T} with fields permanent, transitory, drift, long_run_multiplier, arima_order, T_obs\n\nExamples\n\n# Classic ARIMA-based BN decomposition\ny = cumsum(randn(200)) + 0.3 * sin.(2π * (1:200) / 20)\nresult = beveridge_nelson(y)\nresult = beveridge_nelson(y; p=2, q=1)  # manual ARMA order\n\n# Morley (2002) correlated UC model\nresult = beveridge_nelson(y; method=:statespace)\nresult = beveridge_nelson(y; method=:statespace, cycle_order=1)\n\nReferences\n\nBeveridge, S., & Nelson, C. R. (1981). JME 7(2): 151–174.\nMorley, J. C., Nelson, C. R., & Zivot, E. (2003). REStat 85(2): 235–243.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.baxter_king","page":"Functions","title":"MacroEconometricModels.baxter_king","text":"baxter_king(y::AbstractVector; pl=6, pu=32, K=12) -> BaxterKingResult\n\nApply the Baxter-King band-pass filter to isolate business cycle frequencies.\n\nComputes symmetric moving average weights that approximate the ideal band-pass filter for periods between pl and pu. The filter is constrained to sum to zero (removing stochastic trends).\n\nLoses K observations at each end of the sample (2K total).\n\nArguments\n\ny::AbstractVector: Time series data (length > 2K)\n\nKeywords\n\npl::Int=6: Minimum period of oscillation to pass (quarterly: 6 = 1.5 years)\npu::Int=32: Maximum period of oscillation to pass (quarterly: 32 = 8 years)\nK::Int=12: Truncation length (number of leads/lags in the moving average)\n\nReturns\n\nBaxterKingResult{T} with fields cycle, trend, weights, pl, pu, K, T_obs, valid_range\n\nExamples\n\ny = cumsum(randn(200))\nresult = baxter_king(y)                    # quarterly defaults (6-32 quarters)\nresult = baxter_king(y; pl=2, pu=8, K=6)  # annual data (2-8 years)\n\nReferences\n\nBaxter, M., & King, R. G. (1999). REStat 81(4): 575–593.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.boosted_hp","page":"Functions","title":"MacroEconometricModels.boosted_hp","text":"boosted_hp(y::AbstractVector; lambda=1600.0, stopping=:BIC, max_iter=100, sig_p=0.05) -> BoostedHPResult\n\nApply the boosted HP filter (Phillips & Shi 2021) for improved trend estimation.\n\nIteratively re-filters the cyclical component of the standard HP filter. At each iteration, the cycle is decomposed again and the newly estimated \"trend of the cycle\" is added back to the overall trend.\n\nStopping criteria\n\n:ADF — Stop when the ADF test rejects the null of a unit root in the cycle at significance level sig_p (recommended for detecting stationarity)\n:BIC — Stop when the Phillips-Shi information criterion increases (balances variance reduction against effective degrees of freedom)\n:fixed — Run all max_iter iterations\n\nArguments\n\ny::AbstractVector: Time series data (length ≥ 3)\n\nKeywords\n\nlambda::Real=1600.0: HP smoothing parameter\nstopping::Symbol=:BIC: Stopping criterion (:ADF, :BIC, or :fixed)\nmax_iter::Int=100: Maximum number of boosting iterations\nsig_p::Real=0.05: Significance level for ADF stopping criterion\n\nReturns\n\nBoostedHPResult{T} with fields trend, cycle, lambda, iterations, stopping, bic_path, adf_pvalues, T_obs\n\nExamples\n\ny = cumsum(randn(200))\nresult = boosted_hp(y)                              # BIC stopping (default)\nresult = boosted_hp(y; stopping=:ADF, sig_p=0.05)   # ADF stopping\nresult = boosted_hp(y; stopping=:fixed, max_iter=5)  # fixed iterations\n\nReferences\n\nPhillips, P. C. B., & Shi, Z. (2021). IER 62(2): 521–570.\nMei, Z., Phillips, P. C. B., & Shi, Z. (2024). JAE 39(7): 1260–1281.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.trend","page":"Functions","title":"MacroEconometricModels.trend","text":"trend(result::AbstractFilterResult) -> Vector\n\nReturn the trend component from a filter result. For BeveridgeNelsonResult, returns the permanent component.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.cycle","page":"Functions","title":"MacroEconometricModels.cycle","text":"cycle(result::AbstractFilterResult) -> Vector\n\nReturn the cyclical component from a filter result. For BeveridgeNelsonResult, returns the transitory component.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_ar","page":"Functions","title":"MacroEconometricModels.estimate_ar","text":"estimate_ar(y, p; method=:ols, include_intercept=true) -> ARModel\n\nEstimate AR(p) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ\n\nArguments\n\ny: Time series vector\np: AR order (must be ≥ 1)\nmethod: Estimation method (:ols or :mle)\ninclude_intercept: Whether to include constant term\n\nReturns\n\nARModel with estimated coefficients and diagnostics.\n\nExample\n\ny = randn(200)\nmodel = estimate_ar(y, 2)\nprintln(model.phi)  # AR coefficients\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_ma","page":"Functions","title":"MacroEconometricModels.estimate_ma","text":"estimate_ma(y, q; method=:css_mle, include_intercept=true, max_iter=500) -> MAModel\n\nEstimate MA(q) model: yₜ = c + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nArguments\n\ny: Time series vector\nq: MA order (must be ≥ 1)\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term\nmax_iter: Maximum optimization iterations\n\nReturns\n\nMAModel with estimated coefficients and diagnostics.\n\nExample\n\ny = randn(200)\nmodel = estimate_ma(y, 1)\nprintln(model.theta)  # MA coefficient\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_arma","page":"Functions","title":"MacroEconometricModels.estimate_arma","text":"estimate_arma(y, p, q; method=:css_mle, include_intercept=true, max_iter=500) -> ARMAModel\n\nEstimate ARMA(p,q) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nArguments\n\ny: Time series vector\np: AR order\nq: MA order\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term\nmax_iter: Maximum optimization iterations\n\nReturns\n\nARMAModel with estimated coefficients and diagnostics.\n\nExample\n\ny = randn(200)\nmodel = estimate_arma(y, 1, 1)\nprintln(\"AR: \", model.phi, \" MA: \", model.theta)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_arima","page":"Functions","title":"MacroEconometricModels.estimate_arima","text":"estimate_arima(y, p, d, q; method=:css_mle, include_intercept=true, max_iter=500) -> ARIMAModel\n\nEstimate ARIMA(p,d,q) model by differencing d times and fitting ARMA(p,q).\n\nArguments\n\ny: Time series vector\np: AR order\nd: Integration order (number of differences)\nq: MA order\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term (on differenced series)\nmax_iter: Maximum optimization iterations\n\nReturns\n\nARIMAModel with estimated coefficients and diagnostics.\n\nExample\n\ny = cumsum(randn(200))  # Random walk\nmodel = estimate_arima(y, 1, 1, 0)  # ARIMA(1,1,0)\nprintln(model.phi)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._compute_psi_weights-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_psi_weights","text":"_compute_psi_weights(phi, theta, h) -> Vector{T}\n\nCompute ψ-weights for the MA(∞) representation of an ARMA process.\n\nThe ARMA(p,q) process can be written as: yₜ = μ + Σⱼ₌₀^∞ ψⱼ εₜ₋ⱼ\n\nwhere ψ₀ = 1 and ψⱼ follows the recursion: ψⱼ = φ₁ψⱼ₋₁ + ... + φₚψⱼ₋ₚ + θⱼ\n\nReturns [ψ₁, ψ₂, ..., ψₕ] (excludes ψ₀ = 1).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._confidence_band-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T}} where T","page":"Functions","title":"MacroEconometricModels._confidence_band","text":"_confidence_band(forecasts, se, conf_level)\n\nCompute symmetric confidence interval bounds from forecasts, standard errors, and a confidence level.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_arma-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T, Vector{T}, Vector{T}, T, Int64, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_arma","text":"_forecast_arma(y, resid, c, phi, theta, sigma2, h, conf_level) -> ARIMAForecast\n\nUnified point forecast + CI computation for any ARMA(p,q) model. AR models pass theta=T[], MA models pass phi=T[].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_variance-Union{Tuple{T}, Tuple{T, Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_variance","text":"_forecast_variance(sigma2, psi, h) -> Vector{T}\n\nCompute h-step ahead forecast variance.\n\nVar(eₜ₊ₕ) = σ² (1 + ψ₁² + ψ₂² + ... + ψₕ₋₁²)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._integrate_forecasts-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._integrate_forecasts","text":"_integrate_forecasts(y, fc_diff, d) -> Vector{T}\n\nIntegrate d-differenced forecasts back to original scale.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._integrate_se-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._integrate_se","text":"_integrate_se(se_diff, d) -> Vector{T}\n\nApproximate standard errors after integration.\n\nFor d-fold integration, the variance grows roughly as h^d. This is a conservative approximation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARIMAModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::ARIMAModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for ARIMA model. Forecasts are computed on the differenced series and then integrated back to the original scale.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARMAModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::ARMAModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for ARMA model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::ARModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for AR model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{MAModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::MAModel, h; conf_level=0.95) -> ARIMAForecast\n\nCompute h-step ahead forecasts with confidence intervals for MA model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{AbstractARIMAModel, Int64}","page":"Functions","title":"StatsAPI.predict","text":"predict(model::AbstractARIMAModel, h::Int) -> Vector{T}\n\nReturn h-step ahead point forecasts (without confidence intervals).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.select_arima_order","page":"Functions","title":"MacroEconometricModels.select_arima_order","text":"select_arima_order(y, max_p, max_q; criterion=:bic, d=0, method=:css_mle, include_intercept=true)\n\nAutomatically select ARMA/ARIMA order via grid search over information criteria.\n\nSearches over p ∈ 0:maxp and q ∈ 0:maxq, fits each model, and selects the order that minimizes the specified information criterion.\n\nArguments\n\ny: Time series vector\nmax_p: Maximum AR order to consider\nmax_q: Maximum MA order to consider\ncriterion: Selection criterion (:aic or :bic, default :bic)\nd: Integration order for ARIMA (default 0 = ARMA)\nmethod: Estimation method (:css, :mle, or :css_mle)\ninclude_intercept: Whether to include constant term\n\nReturns\n\nARIMAOrderSelection with best orders, IC matrices, and fitted models.\n\nExample\n\ny = randn(200)\nresult = select_arima_order(y, 3, 3; criterion=:bic)\nprintln(\"Best order: p=$(result.best_p_bic), q=$(result.best_q_bic)\")\nbest_model = result.best_model_bic\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.auto_arima","page":"Functions","title":"MacroEconometricModels.auto_arima","text":"auto_arima(y; max_p=5, max_q=5, max_d=2, criterion=:bic, method=:css_mle)\n\nAutomatically select and fit the best ARIMA model.\n\nPerforms order selection over p, d, and q using the specified criterion. For integration order d, uses unit root test heuristics.\n\nArguments\n\ny: Time series vector\nmax_p: Maximum AR order (default 5)\nmax_q: Maximum MA order (default 5)\nmax_d: Maximum integration order (default 2)\ncriterion: Selection criterion (:aic or :bic)\nmethod: Estimation method\n\nReturns\n\nBest fitted ARIMAModel or ARMAModel.\n\nExample\n\ny = cumsum(randn(200))\nmodel = auto_arima(y)\nprintln(model)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ic_table","page":"Functions","title":"MacroEconometricModels.ic_table","text":"ic_table(result::ARIMAOrderSelection; criterion=:bic)\n\nReturn a formatted table of IC values for printing.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ar_order","page":"Functions","title":"MacroEconometricModels.ar_order","text":"Return AR order p.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ma_order","page":"Functions","title":"MacroEconometricModels.ma_order","text":"Return MA order q.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.diff_order","page":"Functions","title":"MacroEconometricModels.diff_order","text":"Return integration order d.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_var","page":"Functions","title":"MacroEconometricModels.estimate_var","text":"estimate_var(Y::AbstractMatrix{T}, p::Int; check_stability::Bool=true) -> VARModel{T}\n\nEstimate VAR(p) via OLS: Yₜ = c + A₁Yₜ₋₁ + ... + AₚYₜ₋ₚ + uₜ.\n\nArguments\n\nY: Data matrix (T × n)\np: Number of lags\ncheck_stability: If true (default), warns if estimated VAR is non-stationary\n\nReturns\n\nVARModel with estimated coefficients, residuals, covariance matrix, and information criteria.\n\n\n\n\n\nEstimate VAR from DataFrame. Use vars to select columns.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.select_lag_order","page":"Functions","title":"MacroEconometricModels.select_lag_order","text":"Select optimal lag order via information criterion (:aic, :bic, :hqic).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#StatsAPI.vcov-Tuple{VARModel}","page":"Functions","title":"StatsAPI.vcov","text":"Covariance of vectorized coefficients: Σ ⊗ (X'X)⁻¹.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict","page":"Functions","title":"StatsAPI.predict","text":"In-sample fitted values.\n\n\n\n\n\nOut-of-sample forecasts for steps periods.\n\n\n\n\n\nPredicted values: F * Λ'.\n\n\n\n\n\nPredicted values: F * Λ'.\n\n\n\n\n\nPredicted values (common component).\n\n\n\n\n\npredict(model::AbstractARIMAModel, h::Int) -> Vector{T}\n\nReturn h-step ahead point forecasts (without confidence intervals).\n\n\n\n\n\nConditional variance series hatsigma^2_t.\n\n\n\n\n\nConditional variance series hatsigma^2_t.\n\n\n\n\n\nConditional variance series hatsigma^2_t.\n\n\n\n\n\nConditional variance series hatsigma^2_t.\n\n\n\n\n\nPosterior mean volatility series hatsigma^2_t.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#StatsAPI.r2-Tuple{VARModel}","page":"Functions","title":"StatsAPI.r2","text":"R² for each equation.\n\n\n\n\n\nR² for each variable.\n\n\n\n\n\nR² for each variable.\n\n\n\n\n\nR² for each variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.loglikelihood","page":"Functions","title":"StatsAPI.loglikelihood","text":"Gaussian log-likelihood.\n\n\n\n\n\nLog-likelihood of the fitted model.\n\n\n\n\n\nMaximized log-likelihood.\n\n\n\n\n\nMaximized log-likelihood.\n\n\n\n\n\nMaximized log-likelihood.\n\n\n\n\n\nMaximized log-likelihood.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#StatsAPI.confint-Tuple{VARModel}","page":"Functions","title":"StatsAPI.confint","text":"Confidence intervals at given level (default 95%).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_bvar","page":"Functions","title":"MacroEconometricModels.estimate_bvar","text":"estimate_bvar(Y, p; n_draws=1000, sampler=:direct, burnin=0, thin=1,\n              prior=:normal, hyper=nothing) -> BVARPosterior\n\nEstimate Bayesian VAR via conjugate Normal-Inverse-Wishart posterior.\n\nModel\n\nY = X B + E,    E ~ MN(0, Σ, I_T)\nPrior: Σ ~ IW(ν₀, S₀),  vec(B)|Σ ~ N(b₀, Σ ⊗ V₀)\n\nSamplers\n\n:direct (default) — i.i.d. draws from analytical posterior. burnin and thin are ignored.\n:gibbs — Standard two-block Gibbs sampler. burnin defaults to 200 if not specified.\n\nArguments\n\nY::AbstractMatrix: T × n data matrix\np::Int: Number of lags\n\nKeyword Arguments\n\nn_draws::Int=1000: Number of posterior draws to keep\nsampler::Symbol=:direct: Sampling algorithm (:direct or :gibbs)\nburnin::Int=0: Burnin period (only for :gibbs; defaults to 200 when sampler=:gibbs and burnin=0)\nthin::Int=1: Thinning interval (only for :gibbs)\nprior::Symbol=:normal: Prior type (:normal or :minnesota)\nhyper::Union{Nothing,MinnesotaHyperparameters}=nothing: Minnesota hyperparameters. When prior=:minnesota and hyper=nothing, tau is automatically optimized via marginal likelihood maximization (Giannone, Lenza & Primiceri 2015). Pass an explicit MinnesotaHyperparameters(...) to use fixed values instead.\n\nReturns\n\nBVARPosterior{T} containing coefficient and covariance draws.\n\nExample\n\nY = randn(200, 3)\npost = estimate_bvar(Y, 2; n_draws=1000)\npost_mn = estimate_bvar(Y, 2; prior=:minnesota, n_draws=500)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.posterior_mean_model","page":"Functions","title":"MacroEconometricModels.posterior_mean_model","text":"VARModel with posterior mean parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.posterior_median_model","page":"Functions","title":"MacroEconometricModels.posterior_median_model","text":"VARModel with posterior median parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.gen_dummy_obs-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, MinnesotaHyperparameters}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gen_dummy_obs","text":"gen_dummy_obs(Y, p, hyper) -> (Y_dummy, X_dummy)\n\nGenerate Minnesota prior dummy observations. Hyperparameters: tau (tightness), decay, lambda (sum-of-coef), mu (co-persistence), omega.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.log_marginal_likelihood-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, MinnesotaHyperparameters}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.log_marginal_likelihood","text":"log_marginal_likelihood(Y, p, hyper) -> T\n\nClosed-form log marginal likelihood for BVAR with Minnesota prior.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.optimize_hyperparameters-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.optimize_hyperparameters","text":"Optimize tau via grid search on marginal likelihood.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.optimize_hyperparameters_full-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.optimize_hyperparameters_full","text":"Full grid search over tau, lambda, mu.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_vecm","page":"Functions","title":"MacroEconometricModels.estimate_vecm","text":"estimate_vecm(Y, p; rank=:auto, deterministic=:constant, method=:johansen, significance=0.05)\n\nEstimate a Vector Error Correction Model.\n\nArguments\n\nY: Data matrix (T × n) in levels\np: Underlying VAR order (VECM has p-1 lagged differences)\nrank: Cointegrating rank; :auto (default) selects via Johansen trace test, or specify an integer\ndeterministic: :none, :constant (default), or :trend\nmethod: :johansen (default) or :engle_granger (bivariate, rank=1 only)\nsignificance: Significance level for automatic rank selection (default 0.05)\n\nReturns\n\nVECMModel with estimated α, β, Γ matrices, residuals, and diagnostics.\n\nExample\n\nY = cumsum(randn(200, 3), dims=1)\nY[:, 2] = Y[:, 1] + 0.1 * randn(200)\nm = estimate_vecm(Y, 2)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.to_var","page":"Functions","title":"MacroEconometricModels.to_var","text":"to_var(vecm::VECMModel) -> VARModel\n\nConvert VECM to VAR in levels representation.\n\nThe VECM: ΔYₜ = ΠYₜ₋₁ + Σᵢ ΓᵢΔYₜ₋ᵢ + μ + uₜ maps to VAR: Yₜ = c + A₁Yₜ₋₁ + ... + AₚYₜ₋ₚ + uₜ\n\nwith:\n\nA₁ = Π + Iₙ + Γ₁\nAᵢ = Γᵢ - Γᵢ₋₁  for i = 2, ..., p-1\nAₚ = -Γₚ₋₁\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.select_vecm_rank","page":"Functions","title":"MacroEconometricModels.select_vecm_rank","text":"select_vecm_rank(Y, p; criterion=:trace, significance=0.05) -> Int\n\nSelect cointegrating rank using the Johansen trace or max-eigenvalue test.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.cointegrating_rank","page":"Functions","title":"MacroEconometricModels.cointegrating_rank","text":"cointegrating_rank(m::VECMModel) -> Int\n\nReturn the cointegrating rank r of the VECM.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.granger_causality_vecm","page":"Functions","title":"MacroEconometricModels.granger_causality_vecm","text":"granger_causality_vecm(vecm, cause, effect) -> VECMGrangerResult\n\nTest Granger causality from variable cause to variable effect in a VECM.\n\nThree tests are computed:\n\nShort-run: Wald test on Γ coefficients of the cause variable in the effect equation\nLong-run: Wald test on α coefficients in the effect equation (error correction channel)\nStrong: Joint test of both short-run and long-run\n\nArguments\n\nvecm: Estimated VECMModel\ncause: Index of the causing variable\neffect: Index of the effect variable\n\nReturns\n\nVECMGrangerResult with test statistics, p-values, and degrees of freedom.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.fevd-Union{Tuple{T}, Tuple{VECMModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.fevd","text":"fevd(vecm::VECMModel, horizon; kwargs...) -> FEVD\n\nCompute FEVD for a VECM by converting to VAR representation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{VECMModel{T}}, Tuple{T}, Tuple{VECMModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(vecm::VECMModel, horizon; kwargs...) -> HistoricalDecomposition\n\nCompute historical decomposition for a VECM by converting to VAR representation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Union{Tuple{T}, Tuple{VECMModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(vecm::VECMModel, horizon; kwargs...) -> ImpulseResponse\n\nCompute IRFs for a VECM by converting to VAR representation. All identification methods (Cholesky, sign, narrative, etc.) are supported.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{VECMModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(vecm::VECMModel, h; ci_method=:none, reps=500, conf_level=0.95) -> VECMForecast\n\nForecast from a VECM by iterating the VECM equations in levels.\n\nUnlike VAR forecasting, this preserves the cointegrating relationships in the forecast path.\n\nArguments\n\nvecm: Estimated VECM\nh: Forecast horizon\nci_method: :none (default), :bootstrap, or :simulation\nreps: Number of bootstrap/simulation replications (default 500)\nconf_level: Confidence level (default 0.95)\n\nReturns\n\nVECMForecast with level and difference forecasts, plus CIs if requested.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_Q-Union{Tuple{T}, Tuple{VARModel{T}, Symbol, Int64, Any, Any}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_Q","text":"compute_Q(model, method, horizon, check_func, narrative_check;\n          max_draws=100, transition_var=nothing, regime_indicator=nothing)\n\nCompute identification matrix Q for structural VAR analysis.\n\nMethods\n\n:cholesky — Cholesky decomposition (recursive ordering)\n:sign — Sign restrictions (requires check_func)\n:narrative — Narrative restrictions (requires check_func and narrative_check)\n:long_run — Long-run restrictions (Blanchard-Quah)\n:fastica — FastICA (Hyvärinen 1999)\n:jade — JADE (Cardoso 1999)\n:sobi — SOBI (Belouchrani et al. 1997)\n:dcov — Distance covariance ICA (Matteson & Tsay 2017)\n:hsic — HSIC independence ICA (Gretton et al. 2005)\n:student_t — Student-t ML (Lanne et al. 2017)\n:mixture_normal — Mixture of normals ML (Lanne et al. 2017)\n:pml — Pseudo-ML (Gouriéroux et al. 2017)\n:skew_normal — Skew-normal ML (Lanne & Luoto 2020)\n:nongaussian_ml — Unified non-Gaussian ML dispatcher (default: Student-t)\n:markov_switching — Markov-switching heteroskedasticity (Lütkepohl & Netšunajev 2017)\n:garch — GARCH-based heteroskedasticity (Normandin & Phaneuf 2004)\n:smooth_transition — Smooth-transition heteroskedasticity (requires transition_var)\n:external_volatility — External volatility regimes (requires regime_indicator)\n\nKeyword Arguments\n\nmax_draws::Int=100: Maximum draws for sign/narrative identification\ntransition_var::Union{Nothing,AbstractVector}=nothing: Transition variable for :smooth_transition\nregime_indicator::Union{Nothing,AbstractVector{Int}}=nothing: Regime indicator for :external_volatility\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_irf-Union{Tuple{T}, Tuple{VARModel{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_irf","text":"compute_irf(model, Q, horizon) -> Array{T,3}\n\nCompute IRFs for rotation matrix Q. Returns (horizon × n × n) array. IRF[h, i, j] = response of variable i to shock j at horizon h-1.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_structural_shocks-Union{Tuple{T}, Tuple{VARModel{T}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_structural_shocks","text":"Compute structural shocks: εₜ = Q'L⁻¹uₜ.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.generate_Q-Union{Tuple{Int64}, Tuple{T}, Tuple{Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.generate_Q","text":"Generate random orthogonal matrix via QR decomposition (Haar measure).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_cholesky-Union{Tuple{VARModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_cholesky","text":"Identify via Cholesky decomposition (recursive ordering). Returns L where Σ = LL'.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_long_run-Union{Tuple{VARModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_long_run","text":"Identify via long-run restrictions: long-run cumulative impact matrix is lower triangular.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_narrative-Union{Tuple{T}, Tuple{VARModel{T}, Int64, Function, Function}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_narrative","text":"identify_narrative(model, horizon, sign_check, narrative_check; max_draws=1000)\n\nCombine sign and narrative restrictions. Returns (Q, irf, shocks).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identify_sign-Union{Tuple{T}, Tuple{VARModel{T}, Int64, Function}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identify_sign","text":"identify_sign(model, horizon, check_func; max_draws=1000, store_all=false)\n\nFind Q satisfying sign restrictions via random draws.\n\nWith store_all=false (default), returns (Q, irf) — the first valid rotation. With store_all=true, returns a SignIdentifiedSet containing ALL accepted rotations and their IRFs (Baumeister & Hamilton, 2015).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf_bounds-Union{Tuple{SignIdentifiedSet{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.irf_bounds","text":"irf_bounds(s::SignIdentifiedSet{T}; quantiles=[0.16, 0.84]) -> (lower, upper)\n\nCompute pointwise bounds (or quantile bands) over the identified set.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf_median-Union{Tuple{SignIdentifiedSet{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.irf_median","text":"irf_median(s::SignIdentifiedSet{T}) -> Array{T,3}\n\nCompute pointwise median IRF over the identified set.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.UhligSVARResult","page":"Functions","title":"MacroEconometricModels.UhligSVARResult","text":"UhligSVARResult{T<:AbstractFloat}\n\nResult from Mountford-Uhlig (2009) penalty function identification.\n\nFields\n\nQ::Matrix{T}: Optimal rotation matrix\nirf::Array{T,3}: Impulse responses (horizon × n × n)\npenalty::T: Total penalty at optimum (negative = better)\nshock_penalties::Vector{T}: Per-shock penalty values\nrestrictions::SVARRestrictions: The imposed restrictions\nconverged::Bool: Whether all sign restrictions are satisfied\n\n\n\n\n\n","category":"type"},{"location":"api_functions/#MacroEconometricModels.identify_uhlig","page":"Functions","title":"MacroEconometricModels.identify_uhlig","text":"identify_uhlig(model::VARModel{T}, restrictions::SVARRestrictions, horizon::Int;\n    n_starts=50, n_refine=10, max_iter_coarse=500, max_iter_fine=2000,\n    tol_coarse=1e-4, tol_fine=1e-8) -> UhligSVARResult{T}\n\nIdentify SVAR using Mountford & Uhlig (2009) penalty function approach.\n\nUses Nelder-Mead optimization over spherical coordinates to find the rotation matrix Q that best satisfies sign restrictions, with zero restrictions enforced as hard constraints via null-space projection.\n\nAlgorithm\n\nPrecompute MA coefficients and Cholesky factor L\nPhase 1 (coarse): n_starts Nelder-Mead runs from random theta_0 in 0 2pi\nPhase 2 (refinement): n_refine local re-optimizations from best solution\nBuild final Q, compute IRFs, check convergence\n\nKeywords\n\nn_starts::Int=50: Number of random starting points (Phase 1)\nn_refine::Int=10: Number of local refinements (Phase 2)\nmax_iter_coarse::Int=500: Max iterations per Phase 1 run\nmax_iter_fine::Int=2000: Max iterations per Phase 2 run\ntol_coarse::T=1e-4: Convergence tolerance for Phase 1\ntol_fine::T=1e-8: Convergence tolerance for Phase 2\n\nReturns\n\nUhligSVARResult{T} with optimal rotation matrix, IRFs, penalty values, and convergence indicator.\n\nExample\n\nmodel = estimate_var(Y, 2)\nrestrictions = SVARRestrictions(3;\n    zeros = [zero_restriction(3, 1)],\n    signs = [sign_restriction(1, 1, :positive),\n             sign_restriction(2, 1, :positive)]\n)\nresult = identify_uhlig(model, restrictions, 20)\n\nReference: Mountford & Uhlig (2009)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._simulate_irfs-Union{Tuple{T}, Tuple{VARModel{T}, Symbol, Int64, Any, Any, Symbol, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._simulate_irfs","text":"Simulate IRFs for confidence intervals (bootstrap or asymptotic).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._simulate_var-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._simulate_var","text":"Simulate VAR data from initial conditions and innovations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.cumulative_irf-Union{Tuple{BayesianImpulseResponse{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.cumulative_irf","text":"cumulative_irf(irf_result::BayesianImpulseResponse{T}) -> BayesianImpulseResponse{T}\n\nCompute cumulative Bayesian impulse response: Σₛ₌₀ʰ IRF_s.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.cumulative_irf-Union{Tuple{ImpulseResponse{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.cumulative_irf","text":"cumulative_irf(irf_result::ImpulseResponse{T}) -> ImpulseResponse{T}\n\nCompute cumulative impulse response for VAR models: Σₛ₌₀ʰ IRF_s.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.cumulative_irf-Union{Tuple{LPImpulseResponse{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.cumulative_irf","text":"cumulative_irf(irf::LPImpulseResponse{T}) -> LPImpulseResponse{T}\n\nCompute cumulative impulse response: Σₛ₌₀ʰ β_s.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Tuple{BVARPosterior, Int64}","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(post::BVARPosterior, horizon; method=:cholesky, quantiles=[0.16, 0.5, 0.84], ...)\n\nCompute Bayesian IRFs from posterior draws with posterior quantiles.\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nUses process_posterior_samples and compute_posterior_quantiles from bayesian_utils.jl.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Tuple{StructuralLP}","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(slp::StructuralLP) -> ImpulseResponse\n\nExtract the impulse response object from a structural LP result.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.irf-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.irf","text":"irf(model, horizon; method=:cholesky, ci_type=:none, reps=200, conf_level=0.95, ...)\n\nCompute IRFs with optional confidence intervals.\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nCI types\n\n:none, :bootstrap, :theoretical\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_irf-Tuple{AbstractMatrix, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.lp_irf","text":"lp_irf(Y::AbstractMatrix, shock_var::Int, horizon::Int; kwargs...) -> LPImpulseResponse\n\nConvenience function: estimate LP and extract IRF in one call.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_irf-Union{Tuple{LPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_irf","text":"lp_irf(model::LPModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract impulse response function with confidence intervals from LP model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_fevd-Union{Tuple{T}, Tuple{Array{T, 3}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_fevd","text":"Compute FEVD from IRF array: decomposition[i,j,h] = cumulative MSE contribution.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.fevd-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.fevd","text":"fevd(model, horizon; method=:cholesky, ...) -> FEVD\n\nCompute FEVD showing proportion of h-step forecast error variance attributable to each shock.\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_hd_contributions-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Array{Matrix{T}, 1}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_hd_contributions","text":"Compute historical decomposition contributions from structural shocks and MA coefficients. HD[t, i, j] = Σ{s=0}^{t-1} Θs[i, j] * ε_j(t-s)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_initial_conditions-Union{Tuple{T}, Tuple{Matrix{T}, Array{T, 3}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_initial_conditions","text":"Compute initial conditions as residual: actual - total shock contributions.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_structural_ma_coefficients-Union{Tuple{T}, Tuple{VARModel{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_structural_ma_coefficients","text":"Compute structural MA coefficients Θs = Φs * P for s = 0, ..., horizon-1. Returns Vector{Matrix{T}} of length horizon.\n\nUses _compute_ma_coefficients from identification.jl to avoid code duplication.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.contribution-Union{Tuple{T}, Tuple{BayesianHistoricalDecomposition{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.contribution","text":"contribution(hd::BayesianHistoricalDecomposition, var, shock; stat=:mean) -> Vector\n\nGet contribution time series for specific variable and shock (Bayesian).\n\nArguments\n\nhd: Bayesian historical decomposition result\nvar: Variable index (Int) or name (String)\nshock: Shock index (Int) or name (String)\nstat: :mean or quantile index (Int)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.contribution-Union{Tuple{T}, Tuple{HistoricalDecomposition{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.contribution","text":"contribution(hd::HistoricalDecomposition, var, shock) -> Vector\n\nGet contribution time series for specific variable and shock.\n\nArguments\n\nhd: Historical decomposition result\nvar: Variable index (Int) or name (String)\nshock: Shock index (Int) or name (String)\n\nExample\n\ncontrib_y1_s1 = contribution(hd, 1, 1)  # Contribution of shock 1 to variable 1\ncontrib_y1_s1 = contribution(hd, \"Var 1\", \"Shock 1\")\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(post::BVARPosterior, horizon; data=..., ...) -> BayesianHistoricalDecomposition\n\nCompute Bayesian historical decomposition from posterior draws with posterior quantiles.\n\nArguments\n\npost::BVARPosterior: Posterior draws from estimate_bvar\nhorizon::Int: Maximum horizon for MA coefficients\n\nKeyword Arguments\n\ndata::AbstractMatrix: Override data matrix (defaults to post.data)\nmethod::Symbol=:cholesky: Identification method\nquantiles::Vector{<:Real}=[0.16, 0.5, 0.84]: Posterior quantile levels\ncheck_func=nothing: Sign restriction check function\nnarrative_check=nothing: Narrative restriction check function\ntransition_var=nothing: Transition variable (for method=:smooth_transition)\nregime_indicator=nothing: Regime indicator (for method=:external_volatility)\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nReturns\n\nBayesianHistoricalDecomposition with posterior quantiles and means.\n\nExample\n\npost = estimate_bvar(Y, 2; n_draws=500)\nhd = historical_decomposition(post, 198)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{StructuralLP{T}}, Tuple{T}, Tuple{StructuralLP{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(slp::StructuralLP{T}, T_hd::Int) -> HistoricalDecomposition{T}\n\nCompute historical decomposition from structural LP.\n\nUses LP-estimated IRFs as the structural MA coefficients Θ_h and the structural shocks from the underlying VAR identification.\n\nArguments\n\nslp: Structural LP result\nT_hd: Number of time periods for decomposition (≤ T_eff of underlying VAR)\n\nReturns\n\nHistoricalDecomposition{T} with contributions, initial conditions, and actual data.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{T}, Tuple{VARModel{T}, SVARRestrictions}, Tuple{VARModel{T}, SVARRestrictions, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(model::VARModel, restrictions::SVARRestrictions, horizon; ...) -> BayesianHistoricalDecomposition\n\nCompute historical decomposition using Arias et al. (2018) identification with importance weights.\n\nArguments\n\nmodel::VARModel: Estimated VAR model\nrestrictions::SVARRestrictions: Zero and sign restrictions\nhorizon::Int: Maximum horizon for MA coefficients\n\nKeyword Arguments\n\nn_draws::Int=1000: Number of accepted draws\nn_rotations::Int=1000: Maximum rotation attempts per draw\nquantiles::Vector{<:Real}=[0.16, 0.5, 0.84]: Quantile levels for weighted quantiles\n\nReturns\n\nBayesianHistoricalDecomposition with weighted posterior quantiles and means.\n\nExample\n\nr = SVARRestrictions(3; signs=[sign_restriction(1, 1, :positive)])\nhd = historical_decomposition(model, r, 198; n_draws=500)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.historical_decomposition-Union{Tuple{VARModel{T}}, Tuple{T}, Tuple{VARModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.historical_decomposition","text":"historical_decomposition(model::VARModel, horizon; method=:cholesky, ...) -> HistoricalDecomposition\n\nCompute historical decomposition for a VAR model.\n\nDecomposes observed data into contributions from each structural shock plus initial conditions.\n\nArguments\n\nmodel::VARModel: Estimated VAR model\nhorizon::Int: Maximum horizon for MA coefficient computation (typically T_eff)\n\nKeyword Arguments\n\nmethod::Symbol=:cholesky: Identification method\ncheck_func=nothing: Sign restriction check function (for method=:sign or :narrative)\nnarrative_check=nothing: Narrative restriction check function (for method=:narrative)\nmax_draws::Int=1000: Maximum draws for sign/narrative identification\ntransition_var=nothing: Transition variable (for method=:smooth_transition)\nregime_indicator=nothing: Regime indicator (for method=:external_volatility)\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nReturns\n\nHistoricalDecomposition containing:\n\ncontributions: Shock contributions (Teff × nvars × n_shocks)\ninitial_conditions: Initial condition effects (Teff × nvars)\nactual: Actual data values\nshocks: Structural shocks\n\nExample\n\nmodel = estimate_var(Y, 2)\nhd = historical_decomposition(model, size(Y, 1) - 2)\nverify_decomposition(hd)  # Check decomposition identity\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.total_shock_contribution-Union{Tuple{T}, Tuple{HistoricalDecomposition{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.total_shock_contribution","text":"total_shock_contribution(hd::AbstractHistoricalDecomposition, var) -> Vector\n\nGet total contribution from all shocks to a variable over time.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.verify_decomposition-Union{Tuple{BayesianHistoricalDecomposition{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.verify_decomposition","text":"verify_decomposition(hd::BayesianHistoricalDecomposition; tol=1e-6) -> Bool\n\nVerify that mean contributions + mean initial_conditions ≈ actual (approximately, due to averaging).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.verify_decomposition-Union{Tuple{HistoricalDecomposition{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.verify_decomposition","text":"verify_decomposition(hd::HistoricalDecomposition; tol=1e-10) -> Bool\n\nVerify that contributions + initial_conditions ≈ actual.\n\nExample\n\nhd = historical_decomposition(model, horizon)\n@assert verify_decomposition(hd) \"Decomposition identity failed\"\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.has_uncertainty-Tuple{AbstractAnalysisResult}","page":"Functions","title":"MacroEconometricModels.has_uncertainty","text":"has_uncertainty(result::AbstractAnalysisResult) -> Bool\n\nCheck if the result includes uncertainty quantification (confidence intervals or posterior quantiles).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.point_estimate-Tuple{AbstractAnalysisResult}","page":"Functions","title":"MacroEconometricModels.point_estimate","text":"point_estimate(result::AbstractAnalysisResult)\n\nGet the point estimate from an analysis result.\n\nReturns the main values/estimates (IRF values, FEVD proportions, HD contributions).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{BVARPosterior}","page":"Functions","title":"MacroEconometricModels.report","text":"report(post::BVARPosterior)\n\nPrint comprehensive Bayesian VAR posterior summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{BaxterKingResult}","page":"Functions","title":"MacroEconometricModels.report","text":"report(r::BaxterKingResult)\n\nPrint Baxter-King band-pass filter summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{BeveridgeNelsonResult}","page":"Functions","title":"MacroEconometricModels.report","text":"report(r::BeveridgeNelsonResult)\n\nPrint Beveridge-Nelson decomposition summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{BoostedHPResult}","page":"Functions","title":"MacroEconometricModels.report","text":"report(r::BoostedHPResult)\n\nPrint boosted HP filter summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{FEVD}","page":"Functions","title":"MacroEconometricModels.report","text":"report(f::FEVD)\nreport(f::BayesianFEVD)\n\nPrint FEVD summary with decomposition at selected horizons.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{HPFilterResult}","page":"Functions","title":"MacroEconometricModels.report","text":"report(r::HPFilterResult)\n\nPrint HP filter summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{HamiltonFilterResult}","page":"Functions","title":"MacroEconometricModels.report","text":"report(r::HamiltonFilterResult)\n\nPrint Hamilton filter summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{HistoricalDecomposition}","page":"Functions","title":"MacroEconometricModels.report","text":"report(hd::HistoricalDecomposition)\nreport(hd::BayesianHistoricalDecomposition)\n\nPrint HD summary with contribution statistics.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{ImpulseResponse}","page":"Functions","title":"MacroEconometricModels.report","text":"report(irf::ImpulseResponse)\nreport(irf::BayesianImpulseResponse)\n\nPrint IRF summary with values at selected horizons.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{VECMForecast}","page":"Functions","title":"MacroEconometricModels.report","text":"report(f::VECMForecast)\n\nPrint VECM forecast summary.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Tuple{VECMGrangerResult}","page":"Functions","title":"MacroEconometricModels.report","text":"report(g::VECMGrangerResult)\n\nPrint VECM Granger causality test results.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.report","text":"report(model::VARModel)\n\nPrint comprehensive VAR model summary including specification, per-equation coefficient estimates with standard errors and significance, information criteria, residual covariance, and stationarity check.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.report-Union{Tuple{VECMModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.report","text":"report(vecm::VECMModel)\n\nPrint comprehensive VECM summary including cointegrating vectors, adjustment coefficients, short-run dynamics, and diagnostics.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.uncertainty_bounds-Tuple{AbstractAnalysisResult}","page":"Functions","title":"MacroEconometricModels.uncertainty_bounds","text":"uncertainty_bounds(result::AbstractAnalysisResult) -> Union{Nothing, Tuple}\n\nGet uncertainty bounds (lower, upper) if available, otherwise nothing.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._block_bootstrap-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._block_bootstrap","text":"Generate a block bootstrap sample from matrix Y.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._build_structural_lp_arrays-Union{Tuple{T}, Tuple{Array{LPModel{T}, 1}, Int64, Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._build_structural_lp_arrays","text":"Extract 3D IRF and SE arrays from per-shock LP models.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_predict_at_horizon-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}, Matrix{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_predict_at_horizon","text":"_lp_predict_at_horizon(Y, response_vars, residuals_h, T_eff_h, h)\n\nReconstruct fitted values at horizon h via Yh − residualsh identity.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_robust_vcov-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, NeweyWestEstimator{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_robust_vcov","text":"_lp_robust_vcov(X_h, U_h, cov_estimator, h)\n\nHorizon-aware robust variance-covariance for LP regressions.\n\nLP residuals at horizon h have MA(h-1) serial correlation (Jordà 2005), so the Newey-West bandwidth must be at least h+1. When automatic bandwidth selection (bandwidth == 0) yields a smaller value, this function enforces the floor max(m̂_NW, h+1).\n\nFor non-NW estimators (White, Driscoll-Kraay), falls through to the standard compute_block_robust_vcov.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._structural_lp_bootstrap-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Int64, Symbol, Int64, Symbol, Int64, T, Any, Any, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._structural_lp_bootstrap","text":"Block bootstrap for structural LP confidence intervals.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.build_control_columns!-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, Vararg{Int64, 4}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.build_control_columns!","text":"build_control_columns!(X_h::AbstractMatrix{T}, Y::AbstractMatrix{T},\n                       t_start::Int, t_end::Int, lags::Int, start_col::Int) where T\n\nFill control (lagged Y) columns into regressor matrix X_h. Returns the next available column index.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.build_response_matrix-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Int64, Vector{Int64}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.build_response_matrix","text":"build_response_matrix(Y::AbstractMatrix{T}, h::Int, t_start::Int, t_end::Int,\n                      response_vars::Vector{Int}) where T\n\nBuild response matrix Y_h at horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compare_var_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compare_var_lp","text":"compare_var_lp(Y::AbstractMatrix{T}, horizon::Int; lags::Int=4) where T\n\nCompare VAR-based and LP-based impulse responses.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_block_robust_vcov-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractCovarianceEstimator}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compute_block_robust_vcov","text":"compute_block_robust_vcov(X::AbstractMatrix{T}, U::AbstractMatrix{T},\n                          cov_estimator::AbstractCovarianceEstimator) where T\n\nCompute block-diagonal robust covariance for multi-equation system.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compute_horizon_bounds-Tuple{Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.compute_horizon_bounds","text":"compute_horizon_bounds(T_obs::Int, h::Int, lags::Int) -> (t_start, t_end)\n\nCompute valid observation bounds for horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.construct_lp_matrices-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.construct_lp_matrices","text":"construct_lp_matrices(Y::AbstractMatrix{T}, shock_var::Int, h::Int, lags::Int;\n                      response_vars::Vector{Int}=collect(1:size(Y,2))) where T\n\nConstruct regressor and response matrices for LP regression at horizon h.\n\nReturns: (Yh, Xh, valid_idx)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.create_cov_estimator-Union{Tuple{T}, Tuple{Symbol, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.create_cov_estimator","text":"create_cov_estimator(cov_type::Symbol, ::Type{T}; bandwidth::Int=0) where T\n\nCreate covariance estimator from symbol specification. Eliminates repeated if/else patterns across LP variants.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp","text":"estimate_lp(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n            lags::Int=4, response_vars::Vector{Int}=collect(1:size(Y,2)),\n            cov_type::Symbol=:newey_west, bandwidth::Int=0,\n            conf_level::Real=0.95) -> LPModel{T}\n\nEstimate Local Projection impulse response functions (Jordà 2005).\n\nThe LP regression for horizon h:     y{t+h} = αh + βh * shockt + Γh * controlst + ε_{t+h}\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_cholesky-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_cholesky","text":"estimate_lp_cholesky(Y::AbstractMatrix{T}, horizon::Int;\n                     lags::Int=4, cov_type::Symbol=:newey_west, kwargs...) -> Vector{LPModel{T}}\n\nEstimate LP with Cholesky-orthogonalized shocks.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_multi-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Vector{Int64}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_multi","text":"estimate_lp_multi(Y::AbstractMatrix{T}, shock_vars::Vector{Int}, horizon::Int;\n                  kwargs...) -> Vector{LPModel{T}}\n\nEstimate LP for multiple shock variables.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.extract_shock_irf-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Vector{Int64}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.extract_shock_irf","text":"extract_shock_irf(B::Vector{Matrix{T}}, vcov::Vector{Matrix{T}},\n                  response_vars::Vector{Int}, shock_coef_idx::Int;\n                  conf_level::Real=0.95) where T\n\nGeneric IRF extraction from coefficient and covariance vectors. Works for LPModel, LPIVModel, PropensityLPModel.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.structural_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.structural_lp","text":"structural_lp(Y::AbstractMatrix{T}, horizon::Int;\n              method=:cholesky, lags=4, var_lags=nothing,\n              cov_type=:newey_west, conf_level=0.95,\n              ci_type=:none, reps=200,\n              check_func=nothing, narrative_check=nothing,\n              max_draws=1000,\n              transition_var=nothing, regime_indicator=nothing) -> StructuralLP{T}\n\nEstimate structural LP impulse responses using VAR-based identification with LP estimation.\n\nAlgorithm:\n\nEstimate VAR(p) → obtain Σ and residuals\nCompute rotation matrix Q via chosen identification method\nCompute structural shocks εt = Q'L⁻¹ut\nFor each shock j, run LP with εj as regressor and Yeff as responses\nStack into 3D IRF array: irfs[h, i, j]\n\nArguments\n\nY: T × n data matrix\nhorizon: Maximum IRF horizon H\n\nKeyword Arguments\n\nmethod: Identification method\nlags: Number of LP control lags (default: 4)\nvar_lags: VAR lag order for identification (default: same as lags)\ncov_type: HAC estimator (:newey_west, :white)\nconf_level: Confidence level for CIs (default: 0.95)\nci_type: CI method (:none, :bootstrap)\nreps: Number of bootstrap replications\ncheck_func: Sign restriction check function (for :sign/:narrative)\nnarrative_check: Narrative check function (for :narrative)\nmax_draws: Maximum draws for sign/narrative identification\ntransition_var: Transition variable (for :smooth_transition)\nregime_indicator: Regime indicator (for :external_volatility)\n\nMethods\n\n:cholesky, :sign, :narrative, :long_run, :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :nongaussian_ml, :markov_switching, :garch, :smooth_transition, :external_volatility\n\nNote: :smooth_transition requires transition_var kwarg.       :external_volatility requires regime_indicator kwarg.\n\nReturns\n\nStructuralLP{T} with 3D IRFs, structural shocks, VAR model, and individual LP models.\n\nReferences\n\nPlagborg-Møller, M. & Wolf, C. K. (2021). \"Local Projections and VARs Estimate the Same Impulse Responses.\" Econometrica, 89(2), 955–980.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_iv-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_iv","text":"estimate_lp_iv(Y::AbstractMatrix{T}, shock_var::Int, instruments::AbstractMatrix{T},\n               horizon::Int; lags::Int=4, response_vars::Vector{Int}=collect(1:size(Y,2)),\n               cov_type::Symbol=:newey_west, bandwidth::Int=0) -> LPIVModel{T}\n\nEstimate LP with Instrumental Variables (Stock & Watson 2018) using 2SLS.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.first_stage_regression-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{T}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.first_stage_regression","text":"first_stage_regression(endog::AbstractVector{T}, instruments::AbstractMatrix{T},\n                       controls::AbstractMatrix{T}) -> NamedTuple\n\nFirst-stage regression for 2SLS with F-statistic for instrument relevance.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_iv_irf-Union{Tuple{LPIVModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_iv_irf","text":"lp_iv_irf(model::LPIVModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract IRF from LP-IV model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.sargan_test-Union{Tuple{T}, Tuple{LPIVModel{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.sargan_test","text":"sargan_test(model::LPIVModel{T}, h::Int) -> NamedTuple\n\nSargan-Hansen J-test for overidentification at horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.tsls_regression-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, AbstractVector{T}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.tsls_regression","text":"tsls_regression(Y::AbstractMatrix{T}, endog::AbstractVector{T},\n                endog_fitted::AbstractVector{T}, controls::AbstractMatrix{T};\n                cov_estimator::AbstractCovarianceEstimator=NeweyWestEstimator()) -> NamedTuple\n\nSecond-stage regression using fitted values from first stage.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.weak_instrument_test-Union{Tuple{LPIVModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.weak_instrument_test","text":"weak_instrument_test(model::LPIVModel{T}; threshold::T=T(10.0)) -> NamedTuple\n\nTest for weak instruments using Stock-Yogo rule of thumb (F > 10).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.bspline_basis-Tuple{AbstractVector{Int64}, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.bspline_basis","text":"bspline_basis(horizons::AbstractVector{Int}, degree::Int, n_interior_knots::Int;\n              T::Type{<:AbstractFloat}=Float64) -> BSplineBasis{T}\n\nConstruct B-spline basis matrix for given horizons.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.bspline_basis_value-Union{Tuple{T}, Tuple{T, Int64, Int64, Vector{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.bspline_basis_value","text":"bspline_basis_value(x::T, i::Int, degree::Int, knots::Vector{T}) where T\n\nEvaluate B-spline basis function using Cox-de Boor recursion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.compare_smooth_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.compare_smooth_lp","text":"compare_smooth_lp(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                  lambda::T=T(1.0), kwargs...) -> NamedTuple\n\nCompare standard LP and smooth LP.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.cross_validate_lambda-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.cross_validate_lambda","text":"cross_validate_lambda(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                      lambda_grid::Vector{T}=T.(10.0 .^ (-4:0.5:2)),\n                      k_folds::Int=5, kwargs...) -> T\n\nSelect optimal λ via k-fold cross-validation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_smooth_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_smooth_lp","text":"estimate_smooth_lp(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                   degree::Int=3, n_knots::Int=4, lambda::T=T(0.0),\n                   lags::Int=4, response_vars::Vector{Int}=collect(1:size(Y,2)),\n                   cov_type::Symbol=:newey_west, bandwidth::Int=0) -> SmoothLPModel{T}\n\nEstimate Smooth LP with B-spline parameterization (Barnichon & Brownlees 2019).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.roughness_penalty_matrix-Union{Tuple{BSplineBasis{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.roughness_penalty_matrix","text":"roughness_penalty_matrix(basis::BSplineBasis{T}) -> Matrix{T}\n\nCompute roughness penalty matrix R for B-splines (second derivative penalty).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.smooth_lp_irf-Union{Tuple{SmoothLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.smooth_lp_irf","text":"smooth_lp_irf(model::SmoothLPModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract smoothed IRF from SmoothLPModel.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_state_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, AbstractVector{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_state_lp","text":"estimate_state_lp(Y::AbstractMatrix{T}, shock_var::Int, state_var::AbstractVector{T},\n                  horizon::Int; gamma::Union{T,Symbol}=:estimate, ...) -> StateLPModel{T}\n\nEstimate state-dependent LP (Auerbach & Gorodnichenko 2013).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_transition_params-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_transition_params","text":"estimate_transition_params(state_var::AbstractVector{T}, Y::AbstractMatrix{T},\n                           shock_var::Int; method::Symbol=:nlls, ...) -> NamedTuple\n\nEstimate smooth transition parameters (γ, c).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.exponential_transition-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.exponential_transition","text":"exponential_transition(z::AbstractVector{T}, gamma::T, c::T) -> Vector{T}\n\nExponential (symmetric) transition: F(z) = 1 - exp(-γ(z - c)²)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.indicator_transition-Union{Tuple{T}, Tuple{AbstractVector{T}, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.indicator_transition","text":"indicator_transition(z::AbstractVector{T}, c::T) -> Vector{T}\n\nSharp indicator transition: F(z) = 1 if z ≥ c, else 0\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.logistic_transition-Union{Tuple{T}, Tuple{AbstractVector{T}, T, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.logistic_transition","text":"logistic_transition(z::AbstractVector{T}, gamma::T, c::T) -> Vector{T}\n\nLogistic transition function: F(z) = exp(-γ(z - c)) / (1 + exp(-γ(z - c)))\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.state_irf-Union{Tuple{StateLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.state_irf","text":"state_irf(model::StateLPModel{T}; regime::Symbol=:both, conf_level::Real=0.95) -> NamedTuple\n\nExtract state-dependent IRFs.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.test_regime_difference-Union{Tuple{StateLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.test_regime_difference","text":"test_regime_difference(model::StateLPModel{T}; h::Union{Int,Nothing}=nothing) -> NamedTuple\n\nTest whether IRFs differ across regimes.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.doubly_robust_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{Bool}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.doubly_robust_lp","text":"doubly_robust_lp(Y::AbstractMatrix{T}, treatment::AbstractVector{Bool},\n                 covariates::AbstractMatrix{T}, horizon::Int; ...) -> PropensityLPModel{T}\n\nDoubly robust LP estimator combining IPW and regression adjustment.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_propensity_lp-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{Bool}, AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_propensity_lp","text":"estimate_propensity_lp(Y::AbstractMatrix{T}, treatment::AbstractVector{Bool},\n                       covariates::AbstractMatrix{T}, horizon::Int; ...) -> PropensityLPModel{T}\n\nEstimate LP with Inverse Propensity Weighting (Angrist et al. 2018).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_propensity_score-Union{Tuple{T}, Tuple{AbstractVector{Bool}, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_propensity_score","text":"estimate_propensity_score(treatment::AbstractVector{Bool}, X::AbstractMatrix{T};\n                          method::Symbol=:logit) -> Vector{T}\n\nEstimate propensity scores P(D=1|X) via logit or probit.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.inverse_propensity_weights-Union{Tuple{T}, Tuple{AbstractVector{Bool}, AbstractVector{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.inverse_propensity_weights","text":"inverse_propensity_weights(treatment::AbstractVector{Bool}, propensity::AbstractVector{T};\n                           trimming::Tuple{T,T}=(T(0.01), T(0.99)), normalize::Bool=true) -> Vector{T}\n\nCompute IPW weights with optional trimming.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.propensity_diagnostics-Union{Tuple{PropensityLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.propensity_diagnostics","text":"propensity_diagnostics(model::PropensityLPModel{T}) -> NamedTuple\n\nPropensity score diagnostics (overlap, balance).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.propensity_irf-Union{Tuple{PropensityLPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.propensity_irf","text":"propensity_irf(model::PropensityLPModel{T}; conf_level::Real=0.95) -> LPImpulseResponse{T}\n\nExtract treatment effect (ATE) IRF from PropensityLPModel.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._build_forecast_controls-Union{Tuple{LPModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._build_forecast_controls","text":"Build the control vector from LP model's last observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_ci-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, T, Symbol}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_ci","text":"Compute forecast CIs from SEs using normal quantiles.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_forecast_bootstrap-Union{Tuple{T}, Tuple{LPModel{T}, Vector{T}, Vector{T}, Int64, T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._lp_forecast_bootstrap","text":"Bootstrap CIs for LP forecasts via residual resampling.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{LPModel{T}, AbstractVector{<:Real}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(lp::LPModel{T}, shock_path::AbstractVector{<:Real};\n         ci_method=:analytical, conf_level=0.95, n_boot=500) -> LPForecast{T}\n\nCompute direct multi-step LP forecasts given a shock trajectory.\n\nFor each horizon h=1,...,H, the forecast uses the LP regression coefficients:     ŷ{T+h} = αh + βh·shockh + Γh·controlsT\n\nwhere controls_T are the last lags observations of Y.\n\nArguments\n\nlp: Estimated LP model\nshock_path: Vector of length H with assumed future shock values\n\nKeyword Arguments\n\nci_method: :analytical (default), :bootstrap, or :none\nconf_level: Confidence level (default: 0.95)\nn_boot: Number of bootstrap replications (default: 500)\n\nReturns\n\nLPForecast{T} with point forecasts, CIs, and standard errors.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64, AbstractVector{<:Real}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(slp::StructuralLP{T}, shock_idx::Int, shock_path::AbstractVector{<:Real};\n         ci_method=:analytical, conf_level=0.95, n_boot=500) -> LPForecast{T}\n\nCompute direct multi-step forecast from a structural LP model using a specific orthogonalized shock.\n\nArguments\n\nslp: Structural LP model\nshock_idx: Index of the structural shock to use (1:n)\nshock_path: Vector of length H with assumed shock values\n\nKeyword Arguments\n\nci_method: :analytical (default), :bootstrap, or :none\nconf_level: Confidence level (default: 0.95)\nn_boot: Number of bootstrap replications (default: 500)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_lp_fevd_h-Union{Tuple{T}, Tuple{LPModel{T}, Vector{T}, Int64, Int64, Symbol}} where T","page":"Functions","title":"MacroEconometricModels._compute_lp_fevd_h","text":"Dispatch to R², LP-A, or LP-B estimator at horizon h.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_bootstrap-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64, Vector{T}, Int64, T, Union{Nothing, Int64}}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_bootstrap","text":"Bootstrap bias correction and CIs for LP-FEVD.\n\nFit bivariate VAR(L) on (z, y) with HQIC lag selection\nCompute 'true' FEVD from VAR (theoretical benchmark)\nSimulate B samples from VAR, compute LP-FEVD for each\nBias = mean(bootstrap FEVD) - true FEVD\nBias-corrected = raw - bias\nCIs from centered bootstrap distribution (Kilian 1998)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_lpa_h-Union{Tuple{T}, Tuple{LPModel{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_lpa_h","text":"_lp_fevd_lpa_h(lp_model, shock, resp_idx, h) -> T\n\nLP-A FEVD: ŝh = (Σ{i=0}^{h} β̂₀^{i,LP}² σ̂z²) / Var(f̂{t+h|t-1}). Uses IRF coefficients directly — no R² regression needed.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_lpb_h-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, LPModel{T}, Int64, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_lpb_h","text":"_lp_fevd_lpb_h(f_hat, shock, lp_model, resp_idx, t_start, h) -> T\n\nLP-B FEVD: ŝh = numerator / (numerator + Var(ṽ{t+h|t-1})), where ṽ is the residual from the R²-regression (Eq. 6).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._lp_fevd_r2_h-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._lp_fevd_r2_h","text":"_lp_fevd_r2_h(f_hat, shock, t_start, h) -> T\n\nR² FEVD at horizon h: regress forecast errors f̂ on shock leads [z{t+h},...,zt].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._r2_on_shock_leads-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._r2_on_shock_leads","text":"Regress fhat on [1, z{t+h}, z{t+h-1}, ..., zt] and return R².\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._r2_residual_variance-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._r2_residual_variance","text":"Regress f_hat on shock leads and return Var(residuals).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._scalar_lp_fevd_r2-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels._scalar_lp_fevd_r2","text":"Compute R²-based FEVD for scalar (z, y) pair at horizon h. Used in bootstrap bias correction.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._simulate_from_var-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._simulate_from_var","text":"Simulate T_sim observations from a VAR model with burn-in.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._var_theoretical_fevd-Union{Tuple{T}, Tuple{VARModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._var_theoretical_fevd","text":"Compute theoretical FEVD of variable 2 w.r.t. shock 1 from bivariate VAR.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.fevd-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.fevd","text":"fevd(slp::StructuralLP{T}, horizon::Int; kwargs...) -> LPFEVD{T}\n\nCompute LP-based FEVD for structural LP results. Dispatches to lp_fevd.\n\nSee lp_fevd for full documentation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_fevd-Union{Tuple{T}, Tuple{StructuralLP{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_fevd","text":"lp_fevd(slp::StructuralLP{T}, horizon::Int; kwargs...) -> LPFEVD{T}\n\nCompute LP-based FEVD using the R²-based estimator of Gorodnichenko & Lee (2019).\n\nAt each horizon h, the share of variable i's forecast error variance due to shock j is estimated by regressing LP forecast error residuals on structural shock leads z{t+h}, z{t+h-1}, ..., z_t and computing R².\n\nArguments\n\nslp: Structural LP result from structural_lp()\nhorizon: Maximum FEVD horizon (capped at IRF horizon)\n\nKeyword Arguments\n\nmethod: Estimator (:r2, :lpa, :lpb). Default: :r2\nbias_correct: Apply VAR-based bootstrap bias correction. Default: true\nn_boot: Number of bootstrap replications. Default: 500\nconf_level: Confidence level for CIs. Default: 0.95\nvar_lags: VAR lag order for bias correction (default: HQIC-selected)\n\nReturns\n\nLPFEVD{T} with raw proportions, bias-corrected values, SEs, and CIs.\n\nReference\n\nGorodnichenko, Y. & Lee, B. (2019). \"Forecast Error Variance Decompositions with Local Projections.\" JBES, 38(4), 921–933.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_factors-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_factors","text":"estimate_factors(X, r; standardize=true) -> FactorModel\n\nEstimate static factor model Xt = Λ Ft + e_t via Principal Component Analysis.\n\nArguments\n\nX: Data matrix (T × N), observations × variables\nr: Number of factors to extract\n\nKeyword Arguments\n\nstandardize::Bool=true: Standardize data before estimation\n\nReturns\n\nFactorModel containing factors, loadings, eigenvalues, and explained variance.\n\nExample\n\nX = randn(200, 50)  # 200 observations, 50 variables\nfm = estimate_factors(X, 3)  # Extract 3 factors\nr2(fm)  # R² for each variable\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{FactorModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::FactorModel, h; p=1, ci_method=:none, conf_level=0.95, n_boot=1000)\n\nForecast factors and observables h steps ahead from a static factor model.\n\nInternally fits a VAR(p) on the extracted factors, then uses the VAR dynamics to produce multi-step forecasts and (optionally) confidence intervals.\n\nArguments\n\nmodel: Estimated static factor model\nh: Forecast horizon\n\nKeyword Arguments\n\np::Int=1: VAR lag order for factor dynamics\nci_method::Symbol=:none: CI method — :none, :theoretical, or :bootstrap\nconf_level::Real=0.95: Confidence level for intervals\nn_boot::Int=1000: Number of bootstrap replications (if ci_method=:bootstrap)\n\nReturns\n\nFactorForecast with factor and observable forecasts (and CIs if requested).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ic_criteria-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ic_criteria","text":"ic_criteria(X, max_factors; standardize=true)\n\nCompute Bai-Ng (2002) information criteria IC1, IC2, IC3 for selecting the number of factors.\n\nArguments\n\nX: Data matrix (T × N)\nmax_factors: Maximum number of factors to consider\n\nReturns\n\nNamed tuple with IC values and optimal factor counts:\n\nIC1, IC2, IC3: Information criteria vectors\nr_IC1, r_IC2, r_IC3: Optimal factor counts\n\nExample\n\nresult = ic_criteria(X, 10)\nprintln(\"Optimal factors: IC1=\", result.r_IC1, \", IC2=\", result.r_IC2, \", IC3=\", result.r_IC3)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.scree_plot_data-Tuple{FactorModel}","page":"Functions","title":"MacroEconometricModels.scree_plot_data","text":"scree_plot_data(m::FactorModel)\n\nReturn data for scree plot: factor indices, explained variance, cumulative variance.\n\nExample\n\ndata = scree_plot_data(fm)\n# Plot: data.factors vs data.explained_variance\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{FactorModel}","page":"Functions","title":"StatsAPI.dof","text":"Degrees of freedom.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{FactorModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{FactorModel}","page":"Functions","title":"StatsAPI.predict","text":"Predicted values: F * Λ'.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.r2-Union{Tuple{FactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.r2","text":"R² for each variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Union{Tuple{FactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.residuals","text":"Residuals: X - predicted.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_dfm_loglikelihood-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, Bool}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._compute_dfm_loglikelihood","text":"Compute Gaussian log-likelihood given factors and parameters.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._em_mstep-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}, AbstractArray{T, 3}, AbstractArray{T, 3}, Int64, Int64, Bool}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._em_mstep","text":"M-step: update loadings, VAR coefficients, and covariances.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._estimate_dfm_em-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._estimate_dfm_em","text":"EM algorithm for maximum likelihood estimation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._estimate_dfm_twostep-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._estimate_dfm_twostep","text":"Two-step estimation: PCA for factors, then VAR on extracted factors.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.companion_matrix_factors-Union{Tuple{DynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.companion_matrix_factors","text":"companion_matrix_factors(model::DynamicFactorModel)\n\nConstruct companion matrix for factor VAR dynamics.\n\nThe companion form converts VAR(p) to VAR(1): [Ft; F{t-1}; ...] = C [F{t-1}; F{t-2}; ...] + [η_t; 0; ...]\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_dynamic_factors-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_dynamic_factors","text":"estimate_dynamic_factors(X, r, p; method=:twostep, standardize=true, max_iter=100, tol=1e-6)\n\nEstimate dynamic factor model with VAR(p) factor dynamics.\n\nArguments\n\nX: Data matrix (T × N)\nr: Number of factors\np: Number of lags in factor VAR\n\nKeyword Arguments\n\nmethod::Symbol=:twostep: Estimation method (:twostep or :em)\nstandardize::Bool=true: Standardize data\nmax_iter::Int=100: Maximum EM iterations (if method=:em)\ntol::Float64=1e-6: Convergence tolerance (if method=:em)\ndiagonal_idio::Bool=true: Assume diagonal idiosyncratic covariance\n\nReturns\n\nDynamicFactorModel with factors, loadings, VAR coefficients, and diagnostics.\n\nExample\n\ndfm = estimate_dynamic_factors(X, 3, 2)  # 3 factors, VAR(2)\nforecast(dfm, 12)  # 12-step ahead forecast\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{DynamicFactorModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::DynamicFactorModel, h; ci_method=:none, conf_level=0.95, n_boot=1000, ci=false, ci_level=0.95)\n\nForecast factors and observables h steps ahead.\n\nArguments\n\nmodel: Estimated dynamic factor model\nh: Forecast horizon\n\nKeyword Arguments\n\nci_method::Symbol=:none: CI method — :none, :theoretical, :bootstrap, or :simulation\nconf_level::Real=0.95: Confidence level for intervals\nn_boot::Int=1000: Bootstrap replications (for :bootstrap and :simulation)\nci::Bool=false: Legacy keyword — ci=true maps to ci_method=:simulation\nci_level::Real=0.95: Legacy keyword — maps to conf_level\n\nReturns\n\nFactorForecast with factor and observable forecasts (and CIs if requested).\n\nExample\n\nfc = forecast(dfm, 12; ci_method=:theoretical)\nfc.observables       # h×N matrix of forecasts\nfc.observables_lower # h×N lower CI bounds\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ic_criteria_dynamic-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ic_criteria_dynamic","text":"ic_criteria_dynamic(X, max_r, max_p; standardize=true, method=:twostep)\n\nSelect (r, p) via AIC/BIC grid search over factor and lag combinations.\n\nReturns\n\nNamed tuple with AIC/BIC matrices and optimal (r, p) combinations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.is_stationary-Tuple{DynamicFactorModel}","page":"Functions","title":"MacroEconometricModels.is_stationary","text":"is_stationary(model::DynamicFactorModel) -> Bool\n\nCheck if factor dynamics are stationary (max |eigenvalue| < 1).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.aic-Tuple{DynamicFactorModel}","page":"Functions","title":"StatsAPI.aic","text":"Akaike Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.bic-Tuple{DynamicFactorModel}","page":"Functions","title":"StatsAPI.bic","text":"Bayesian Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{DynamicFactorModel}","page":"Functions","title":"StatsAPI.dof","text":"Degrees of freedom: Nr + r²p + r(r+1)/2 + N.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.loglikelihood-Tuple{DynamicFactorModel}","page":"Functions","title":"StatsAPI.loglikelihood","text":"Log-likelihood of the fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{DynamicFactorModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{DynamicFactorModel}","page":"Functions","title":"StatsAPI.predict","text":"Predicted values: F * Λ'.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.r2-Union{Tuple{DynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.r2","text":"R² for each variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Union{Tuple{DynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.residuals","text":"Residuals: X - predicted.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_common_spectral_density-Union{Tuple{T}, Tuple{Array{Complex{T}, 3}, AbstractMatrix}} where T","page":"Functions","title":"MacroEconometricModels._compute_common_spectral_density","text":"Compute spectral density of common component from loadings and eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_kernel_weights-Tuple{Int64, Symbol}","page":"Functions","title":"MacroEconometricModels._compute_kernel_weights","text":"Compute kernel weights for spectral smoothing.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._compute_variance_explained-Union{Tuple{T}, Tuple{Matrix{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels._compute_variance_explained","text":"Compute variance explained by first q factors (averaged across frequencies).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._estimate_spectral_density-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Symbol}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._estimate_spectral_density","text":"Estimate spectral density matrix with kernel smoothing.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._extract_time_domain_factors-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Array{Complex{T}, 3}, Vector{T}}} where T","page":"Functions","title":"MacroEconometricModels._extract_time_domain_factors","text":"Extract time-domain factors via frequency-domain projection.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._forecast_factors_ar-Union{Tuple{T}, Tuple{Matrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._forecast_factors_ar","text":"AR(1) forecast for each factor series.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._reconstruct_time_domain-Union{Tuple{T}, Tuple{Array{Complex{T}, 3}, AbstractMatrix{T}}} where T","page":"Functions","title":"MacroEconometricModels._reconstruct_time_domain","text":"Reconstruct common component in time domain via inverse FFT.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._select_bandwidth-Tuple{Int64}","page":"Functions","title":"MacroEconometricModels._select_bandwidth","text":"Automatic bandwidth selection: T^(1/3).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._spectral_eigendecomposition-Union{Tuple{Array{Complex{T}, 3}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels._spectral_eigendecomposition","text":"Eigendecomposition of spectral density at each frequency.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.common_variance_share-Union{Tuple{GeneralizedDynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.common_variance_share","text":"common_variance_share(model::GeneralizedDynamicFactorModel) -> Vector\n\nFraction of each variable's variance explained by the common component.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_gdfm-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_gdfm","text":"estimate_gdfm(X, q; standardize=true, bandwidth=0, kernel=:bartlett, r=0) -> GeneralizedDynamicFactorModel\n\nEstimate Generalized Dynamic Factor Model using spectral methods.\n\nArguments\n\nX: Data matrix (T × N)\nq: Number of dynamic factors\n\nKeyword Arguments\n\nstandardize::Bool=true: Standardize data\nbandwidth::Int=0: Kernel bandwidth (0 = automatic selection)\nkernel::Symbol=:bartlett: Kernel for spectral smoothing (:bartlett, :parzen, :tukey)\nr::Int=0: Number of static factors (0 = same as q)\n\nReturns\n\nGeneralizedDynamicFactorModel with common/idiosyncratic components and spectral loadings.\n\nExample\n\ngdfm = estimate_gdfm(X, 3)\ncommon_variance_share(gdfm)  # Fraction of variance explained by common component\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{GeneralizedDynamicFactorModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::GeneralizedDynamicFactorModel, h; method=:ar, ci_method=:none, conf_level=0.95, n_boot=1000)\n\nForecast h steps ahead using AR extrapolation of factors.\n\nArguments\n\nmodel: Estimated GDFM\nh: Forecast horizon\n\nKeyword Arguments\n\nmethod::Symbol=:ar: Forecasting method (currently only :ar supported)\nci_method::Symbol=:none: CI method — :none, :theoretical, or :bootstrap\nconf_level::Real=0.95: Confidence level for intervals\nn_boot::Int=1000: Bootstrap replications (for :bootstrap)\n\nReturns\n\nFactorForecast with factor and observable forecasts (and CIs if requested).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ic_criteria_gdfm-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ic_criteria_gdfm","text":"ic_criteria_gdfm(X, max_q; standardize=true, bandwidth=0, kernel=:bartlett)\n\nInformation criteria for selecting number of dynamic factors.\n\nUses eigenvalue ratio test and cumulative variance threshold.\n\nReturns\n\nNamed tuple with:\n\neigenvalue_ratios: Ratios of consecutive eigenvalues\ncumulative_variance: Cumulative variance explained\nq_ratio: Optimal q from eigenvalue ratio\nq_variance: Optimal q from 90% variance threshold\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.spectral_eigenvalue_plot_data-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"MacroEconometricModels.spectral_eigenvalue_plot_data","text":"spectral_eigenvalue_plot_data(model::GeneralizedDynamicFactorModel)\n\nReturn data for plotting eigenvalues across frequencies.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.dof","text":"Degrees of freedom.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.predict-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.predict","text":"Predicted values (common component).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.r2-Union{Tuple{GeneralizedDynamicFactorModel{T}}, Tuple{T}} where T","page":"Functions","title":"StatsAPI.r2","text":"R² for each variable.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{GeneralizedDynamicFactorModel}","page":"Functions","title":"StatsAPI.residuals","text":"Residuals (idiosyncratic component).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_pvar","page":"Functions","title":"MacroEconometricModels.estimate_pvar","text":"estimate_pvar(d::PanelData{T}, p::Int; kwargs...) -> PVARModel{T}\n\nEstimate Panel VAR(p) via GMM.\n\nArguments\n\nd::PanelData{T} — balanced panel data (use xtset to construct)\np::Int — number of lags\n\nKeyword Arguments\n\ndependent_vars::Union{Vector{String},Nothing}=nothing — endogenous variable names (default: all)\npredet_vars::Vector{String}=String[] — predetermined variable names\nexog_vars::Vector{String}=String[] — strictly exogenous variable names\ntransformation::Symbol=:fd — :fd (first-difference) or :fod (forward orthogonal deviations)\nsteps::Symbol=:twostep — :onestep, :twostep, or :mstep (iterated)\nsystem_instruments::Bool=false — if true, use System GMM (Blundell-Bond)\nsystem_constant::Bool=true — include constant in level equation (System GMM)\nmin_lag_endo::Int=2 — minimum instrument lag for endogenous variables\nmax_lag_endo::Int=99 — maximum instrument lag (99 = all available)\ncollapse::Bool=false — collapse instruments to limit proliferation\npca_instruments::Bool=false — apply PCA reduction to instruments\npca_max_components::Int=0 — max PCA components (0 = auto)\nmax_iter::Int=100 — max iterations for iterated GMM\n\nReturns\n\nPVARModel{T} with coefficient estimates, robust standard errors, and GMM internals\n\nReferences\n\nArellano, M. & Bond, S. (1991). Review of Economic Studies 58(2), 277-297.\nBlundell, R. & Bond, S. (1998). Journal of Econometrics 87(1), 115-143.\nWindmeijer, F. (2005). Journal of Econometrics 126(1), 25-51.\n\nExamples\n\npd = xtset(df, :id, :time)\nmodel = estimate_pvar(pd, 2; steps=:twostep)\nmodel = estimate_pvar(pd, 1; system_instruments=true, steps=:twostep)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_pvar_feols","page":"Functions","title":"MacroEconometricModels.estimate_pvar_feols","text":"estimate_pvar_feols(d::PanelData{T}, p::Int; kwargs...) -> PVARModel{T}\n\nEstimate Panel VAR(p) via Fixed Effects OLS (within estimator).\n\nSimpler than GMM but subject to Nickell (1981) bias when T is small relative to N. Uses within-group demeaning to remove fixed effects, then pooled OLS with cluster-robust standard errors.\n\nArguments\n\nd::PanelData{T} — panel data\np::Int — number of lags\n\nKeyword Arguments\n\ndependent_vars::Union{Vector{String},Nothing}=nothing — endogenous variable names\npredet_vars::Vector{String}=String[] — predetermined variable names\nexog_vars::Vector{String}=String[] — strictly exogenous variable names\n\nExamples\n\npd = xtset(df, :id, :time)\nmodel = estimate_pvar_feols(pd, 2)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_oirf","page":"Functions","title":"MacroEconometricModels.pvar_oirf","text":"pvar_oirf(model::PVARModel{T}, H::Int) -> Array{T, 3}\n\nOrthogonalized impulse response functions via Cholesky decomposition.\n\nΨh = Φh P where P = chol(Σ) is the lower Cholesky factor.\n\nReturns H+1 × m × m array: oirf[h+1, response, shock].\n\nArguments\n\nmodel::PVARModel — estimated PVAR\nH::Int — maximum horizon\n\nExamples\n\noirf = pvar_oirf(model, 10)\noirf[1, :, :]  # impact response (h=0)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_girf","page":"Functions","title":"MacroEconometricModels.pvar_girf","text":"pvar_girf(model::PVARModel{T}, H::Int) -> Array{T, 3}\n\nGeneralized impulse response functions (Pesaran & Shin, 1998).\n\nGIRFh(j) = Φh Σ ej / √σjj\n\nwhere ej is the j-th unit vector and σjj = Σ[j,j].\n\nReturns H+1 × m × m array: girf[h+1, response, shock].\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_fevd","page":"Functions","title":"MacroEconometricModels.pvar_fevd","text":"pvar_fevd(model::PVARModel{T}, H::Int) -> Array{T, 3}\n\nForecast error variance decomposition based on OIRF.\n\nΩ[l, k, h] = Σ{j=0}^{h} (Ψj[l,k])² / MSE_h[l,l]\n\nwhere MSEh = Σ{j=0}^{h} Φj Σ Φj'.\n\nReturns H+1 × m × m array: fevd[h+1, variable, shock]. Each row sums to 1.\n\nExamples\n\nfv = pvar_fevd(model, 10)\nsum(fv[11, 1, :])  # ≈ 1.0 (all shocks for var 1 at h=10)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_stability","page":"Functions","title":"MacroEconometricModels.pvar_stability","text":"pvar_stability(model::PVARModel{T}) -> PVARStability{T}\n\nCheck stability of the PVAR by computing eigenvalues of the companion matrix. The system is stable if all eigenvalue moduli are strictly less than 1.\n\nExamples\n\nstab = pvar_stability(model)\nstab.is_stable  # true if all |λ| < 1\nstab.moduli     # sorted eigenvalue moduli\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_bootstrap_irf","page":"Functions","title":"MacroEconometricModels.pvar_bootstrap_irf","text":"pvar_bootstrap_irf(model::PVARModel{T}, H::Int;\n                    irf_type::Symbol=:oirf, n_draws::Int=500,\n                    ci::Real=0.95, rng::AbstractRNG=Random.default_rng()) -> NamedTuple\n\nGroup-level block bootstrap for PVAR impulse response confidence intervals.\n\nResamples N groups with replacement → re-estimates PVAR → computes IRF → collects pointwise quantiles.\n\nArguments\n\nmodel::PVARModel — estimated PVAR\nH::Int — maximum IRF horizon\n\nKeywords\n\nirf_type::Symbol=:oirf — :oirf or :girf\nn_draws::Int=500 — number of bootstrap replications\nci::Real=0.95 — confidence level\nrng::AbstractRNG — random number generator\n\nReturns\n\nNamedTuple with:\n\nirf::Array{T,3} — point estimate (H+1 × m × m)\nlower::Array{T,3} — lower CI bound\nupper::Array{T,3} — upper CI bound\ndraws::Array{T,4} — all bootstrap draws (n_draws × H+1 × m × m)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_hansen_j","page":"Functions","title":"MacroEconometricModels.pvar_hansen_j","text":"pvar_hansen_j(model::PVARModel{T}) -> PVARTestResult{T}\n\nHansen (1982) J-test for overidentifying restrictions.\n\nJ = (Σi Zi' ei)' W (Σi Zi' ei) ~ χ²(q - k)\n\nwhere q = number of instruments, k = number of parameters per equation.\n\nH0: All moment conditions are valid. H1: Some moment conditions are invalid.\n\nExamples\n\nj = pvar_hansen_j(model)\nj.pvalue > 0.05  # fail to reject → instruments valid\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_mmsc","page":"Functions","title":"MacroEconometricModels.pvar_mmsc","text":"pvar_mmsc(model::PVARModel{T}; hq_criterion::Real=2.1) -> NamedTuple\n\nAndrews-Lu (2001) Model and Moment Selection Criteria based on Hansen J-statistic.\n\nMMSCBIC  = J - (c - b) × log(n) MMSCAIC  = J - (c - b) × 2 MMSC_HQIC = J - Q(c - b) × log(log(n))\n\nLower values are preferred.\n\nReturns\n\nNamedTuple (bic, aic, hqic) of MMSC values.\n\nExamples\n\nmmsc = pvar_mmsc(model)\nmmsc.bic   # MMSC-BIC value\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.pvar_lag_selection","page":"Functions","title":"MacroEconometricModels.pvar_lag_selection","text":"pvar_lag_selection(d::PanelData{T}, max_p::Int; kwargs...) -> NamedTuple\n\nSelect optimal PVAR lag order by estimating models for p = 1, ..., max_p and comparing Andrews-Lu MMSC criteria.\n\nReturns\n\nNamedTuple with:\n\ntable::Matrix — comparison table (max_p × 4: p, BIC, AIC, HQIC)\nbest_bic::Int, best_aic::Int, best_hqic::Int — optimal lag orders\nmodels::Vector{PVARModel} — estimated models\n\nExamples\n\nsel = pvar_lag_selection(pd, 4)\nsel.best_bic  # optimal lag by BIC\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.linear_gmm_solve","page":"Functions","title":"MacroEconometricModels.linear_gmm_solve","text":"linear_gmm_solve(S_ZX::Matrix{T}, S_Zy::AbstractVecOrMat{T},\n                  W::Matrix{T}) -> Vector{T}\n\nSolve linear GMM analytically: E[Z'(y - Xβ)] = 0.\n\nGiven pre-aggregated cross-products SZX = Σi Zi'Xi and SZy = Σi Zi'yi:\n\nβ = (S_ZX' W S_ZX)⁻¹ S_ZX' W S_Zy\n\nArguments\n\nS_ZX — q × k aggregated instrument-regressor cross-product\nS_Zy — q × 1 (or q-vector) aggregated instrument-response cross-product\nW — q × q weighting matrix\n\nReturns\n\nParameter vector β (length k)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.gmm_sandwich_vcov","page":"Functions","title":"MacroEconometricModels.gmm_sandwich_vcov","text":"gmm_sandwich_vcov(S_ZX::Matrix{T}, W::Matrix{T},\n                   D_e::Matrix{T}) -> Matrix{T}\n\nRobust sandwich covariance for one-step GMM:\n\nV = (S_ZX' W S_ZX)⁻¹ S_ZX' W D_e W S_ZX (S_ZX' W S_ZX)⁻¹\n\nwhere De = Σi (Zi ei)(Zi ei)' is the robust moment covariance.\n\nArguments\n\nS_ZX — q × k aggregated instrument-regressor cross-product\nW — q × q weighting matrix\nD_e — q × q robust moment covariance\n\nReturns\n\nk × k covariance matrix\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.andrews_lu_mmsc","page":"Functions","title":"MacroEconometricModels.andrews_lu_mmsc","text":"andrews_lu_mmsc(j_stat::T, n_instruments::Int, n_params::Int,\n                 n_obs::Int; hq_criterion::Real=2.1) -> NamedTuple\n\nAndrews-Lu (2001) Model and Moment Selection Criteria based on Hansen J-statistic.\n\nMMSC_BIC  = J - (c - b) × log(n)\nMMSC_AIC  = J - (c - b) × 2\nMMSC_HQIC = J - Q(c - b) × log(log(n))\n\nwhere c = moment conditions, b = parameters, n = observations. Lower values indicate better model specification.\n\nArguments\n\nj_stat — Hansen J-test statistic\nn_instruments — number of moment conditions (c)\nn_params — number of parameters (b)\nn_obs — number of observations (n)\nhq_criterion — penalty parameter Q for HQIC (default 2.1)\n\nReturns\n\nNamedTuple (bic, aic, hqic).\n\nReferences\n\nAndrews, D. & Lu, B. (2001). Journal of Econometrics 101(1), 123-164.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.andrews_lu_mmsc-Union{Tuple{T}, Tuple{T, Int64, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.andrews_lu_mmsc","text":"andrews_lu_mmsc(j_stat::T, n_instruments::Int, n_params::Int,\n                 n_obs::Int; hq_criterion::Real=2.1) -> NamedTuple\n\nAndrews-Lu (2001) Model and Moment Selection Criteria based on Hansen J-statistic.\n\nMMSC_BIC  = J - (c - b) × log(n)\nMMSC_AIC  = J - (c - b) × 2\nMMSC_HQIC = J - Q(c - b) × log(log(n))\n\nwhere c = moment conditions, b = parameters, n = observations. Lower values indicate better model specification.\n\nArguments\n\nj_stat — Hansen J-test statistic\nn_instruments — number of moment conditions (c)\nn_params — number of parameters (b)\nn_obs — number of observations (n)\nhq_criterion — penalty parameter Q for HQIC (default 2.1)\n\nReturns\n\nNamedTuple (bic, aic, hqic).\n\nReferences\n\nAndrews, D. & Lu, B. (2001). Journal of Econometrics 101(1), 123-164.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_gmm-Union{Tuple{T}, Tuple{Function, AbstractVector{T}, Any}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_gmm","text":"estimate_gmm(moment_fn::Function, theta0::AbstractVector{T}, data;\n             weighting::Symbol=:two_step, max_iter::Int=100,\n             tol::T=T(1e-8), hac::Bool=true, bandwidth::Int=0) -> GMMModel{T}\n\nEstimate parameters via Generalized Method of Moments.\n\nMinimizes: Q(θ) = g(θ)'W g(θ) where g(θ) = (1/n) Σᵢ gᵢ(θ)\n\nArguments:\n\nmoment_fn: Function (theta, data) -> Matrix of moment conditions (n × q)\ntheta0: Initial parameter guess\ndata: Data passed to moment function\nweighting: Weighting method (:identity, :optimal, :two_step, :iterated)\nmax_iter: Maximum iterations for optimization and/or iterated GMM\ntol: Convergence tolerance\nhac: Use HAC correction for optimal weighting\nbandwidth: HAC bandwidth (0 = automatic)\n\nReturns:\n\nGMMModel with estimates, covariance, and J-test results\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.estimate_lp_gmm-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.estimate_lp_gmm","text":"estimate_lp_gmm(Y::AbstractMatrix{T}, shock_var::Int, horizon::Int;\n                lags::Int=4, weighting::Symbol=:two_step) -> Vector{GMMModel{T}}\n\nEstimate Local Projection via GMM.\n\nReturns a GMMModel for each horizon.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.gmm_objective-Union{Tuple{T}, Tuple{AbstractVector{T}, Function, Any, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gmm_objective","text":"gmm_objective(theta::AbstractVector{T}, moment_fn::Function, data,\n              W::AbstractMatrix{T}) -> T\n\nCompute GMM objective: Q(θ) = g(θ)'W g(θ)\n\nwhere g(θ) = (1/n) Σᵢ gᵢ(θ,data)\n\nArguments:\n\ntheta: Parameter vector\nmoment_fn: Function (theta, data) -> Matrix of moment conditions (n × q)\ndata: Data passed to moment function\nW: Weighting matrix (q × q)\n\nReturns:\n\nGMM objective value\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.gmm_sandwich_vcov-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, Matrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gmm_sandwich_vcov","text":"gmm_sandwich_vcov(S_ZX::Matrix{T}, W::Matrix{T},\n                   D_e::Matrix{T}) -> Matrix{T}\n\nRobust sandwich covariance for one-step GMM:\n\nV = (S_ZX' W S_ZX)⁻¹ S_ZX' W D_e W S_ZX (S_ZX' W S_ZX)⁻¹\n\nwhere De = Σi (Zi ei)(Zi ei)' is the robust moment covariance.\n\nArguments\n\nS_ZX — q × k aggregated instrument-regressor cross-product\nW — q × q weighting matrix\nD_e — q × q robust moment covariance\n\nReturns\n\nk × k covariance matrix\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.gmm_summary-Union{Tuple{GMMModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.gmm_summary","text":"gmm_summary(model::GMMModel{T}) -> NamedTuple\n\nSummary statistics for GMM estimation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.identity_weighting-Union{Tuple{Int64}, Tuple{T}, Tuple{Int64, Type{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.identity_weighting","text":"identity_weighting(n_moments::Int, ::Type{T}=Float64) -> Matrix{T}\n\nIdentity weighting matrix (one-step GMM).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.j_test-Union{Tuple{GMMModel{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.j_test","text":"j_test(model::GMMModel{T}) -> NamedTuple\n\nHansen's J-test for overidentifying restrictions.\n\nH0: All moment conditions are valid (E[g(θ₀)] = 0) H1: Some moment conditions are violated\n\nReturns:\n\nJ_stat: Test statistic\np_value: p-value from chi-squared distribution\ndf: Degrees of freedom (nmoments - nparams)\nreject_05: Whether to reject at 5% level\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.linear_gmm_solve-Union{Tuple{T}, Tuple{Matrix{T}, AbstractVecOrMat{T}, Matrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.linear_gmm_solve","text":"linear_gmm_solve(S_ZX::Matrix{T}, S_Zy::AbstractVecOrMat{T},\n                  W::Matrix{T}) -> Vector{T}\n\nSolve linear GMM analytically: E[Z'(y - Xβ)] = 0.\n\nGiven pre-aggregated cross-products SZX = Σi Zi'Xi and SZy = Σi Zi'yi:\n\nβ = (S_ZX' W S_ZX)⁻¹ S_ZX' W S_Zy\n\nArguments\n\nS_ZX — q × k aggregated instrument-regressor cross-product\nS_Zy — q × 1 (or q-vector) aggregated instrument-response cross-product\nW — q × q weighting matrix\n\nReturns\n\nParameter vector β (length k)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lp_gmm_moments-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64, Any, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.lp_gmm_moments","text":"lp_gmm_moments(Y::AbstractMatrix{T}, shock_var::Int, h::Int, theta,\n               lags::Int) -> Matrix{T}\n\nConstruct moment conditions for LP estimated via GMM.\n\nMoments: E[Zt * ε{t+h}] = 0 where ε{t+h} = y{t+h} - θ' * X_t and Z includes all exogenous variables.\n\nThis is useful when you need to impose cross-equation restrictions.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.minimize_gmm-Union{Tuple{T}, Tuple{Function, AbstractVector{T}, Any, AbstractMatrix{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.minimize_gmm","text":"minimize_gmm(moment_fn::Function, theta0::AbstractVector{T}, data,\n             W::AbstractMatrix{T}; max_iter::Int=100, tol::T=T(1e-8)) -> NamedTuple\n\nMinimize GMM objective using gradient descent with BFGS-like updates.\n\nReturns:\n\ntheta: Minimizer\nobjective: Final objective value\nconverged: Convergence flag\niterations: Number of iterations\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.numerical_gradient-Union{Tuple{T}, Tuple{Function, AbstractVector{T}}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.numerical_gradient","text":"numerical_gradient(f::Function, x::AbstractVector{T}; eps::T=T(1e-7)) -> Matrix{T}\n\nCompute numerical gradient (Jacobian) of function f at point x using central differences.\n\nArguments:\n\nf: Function that takes vector x and returns vector (moment conditions)\nx: Point at which to evaluate gradient\neps: Step size for finite differences\n\nReturns:\n\nJacobian matrix (nmoments × nparams)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.optimal_weighting_matrix-Union{Tuple{T}, Tuple{Function, AbstractVector{T}, Any}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.optimal_weighting_matrix","text":"optimal_weighting_matrix(moment_fn::Function, theta::AbstractVector{T}, data;\n                         hac::Bool=true, bandwidth::Int=0) -> Matrix{T}\n\nCompute optimal GMM weighting matrix: W = inv(Var(g)).\n\nFor i.i.d. data: W = inv((1/n) Σᵢ gᵢ gᵢ') For time series: Uses HAC estimation with Newey-West kernel.\n\nArguments:\n\nmoment_fn: Moment function\ntheta: Current parameter estimate\ndata: Data\nhac: Use HAC correction for serial correlation\nbandwidth: HAC bandwidth (0 = automatic)\n\nReturns:\n\nOptimal weighting matrix (q × q)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.adf_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.adf_test","text":"adf_test(y; lags=:aic, max_lags=nothing, regression=:constant) -> ADFResult\n\nAugmented Dickey-Fuller test for unit root.\n\nTests H₀: y has a unit root (non-stationary) against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nlags: Number of augmenting lags, or :aic/:bic/:hqic for automatic selection\nmax_lags: Maximum lags for automatic selection (default: floor(12*(T/100)^0.25))\nregression: Deterministic terms - :none, :constant (default), or :trend\n\nReturns\n\nADFResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk (has unit root)\nresult = adf_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nDickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. JASA, 74(366), 427-431.\nMacKinnon, J. G. (2010). Critical values for cointegration tests. Queen's Economics Department Working Paper No. 1227.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.kpss_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.kpss_test","text":"kpss_test(y; regression=:constant, bandwidth=:auto) -> KPSSResult\n\nKwiatkowski-Phillips-Schmidt-Shin test for stationarity.\n\nTests H₀: y is stationary against H₁: y has a unit root.\n\nArguments\n\ny: Time series vector\nregression: :constant (level stationarity) or :trend (trend stationarity)\nbandwidth: Bartlett kernel bandwidth, or :auto for Newey-West selection\n\nReturns\n\nKPSSResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = randn(200)  # Stationary series\nresult = kpss_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀ (stationarity)\n\nReferences\n\nKwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54(1-3), 159-178.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.pp_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.pp_test","text":"pp_test(y; regression=:constant, bandwidth=:auto) -> PPResult\n\nPhillips-Perron test for unit root with non-parametric correction.\n\nTests H₀: y has a unit root against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nregression: :none, :constant (default), or :trend\nbandwidth: Newey-West bandwidth, or :auto for automatic selection\n\nReturns\n\nPPResult containing test statistic (Zt), p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk\nresult = pp_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nPhillips, P. C., & Perron, P. (1988). Testing for a unit root in time series regression. Biometrika, 75(2), 335-346.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.za_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.za_test","text":"za_test(y; regression=:both, trim=0.15, lags=:aic, max_lags=nothing) -> ZAResult\n\nZivot-Andrews test for unit root with endogenous structural break.\n\nTests H₀: y has a unit root without break against H₁: y is stationary with break.\n\nArguments\n\ny: Time series vector\nregression: Type of break - :constant (intercept), :trend (slope), or :both\ntrim: Trimming fraction for break search (default 0.15)\nlags: Number of augmenting lags, or :aic/:bic for automatic selection\nmax_lags: Maximum lags for selection\n\nReturns\n\nZAResult containing minimum t-statistic, break point, p-value, etc.\n\nExample\n\n# Series with structural break\ny = vcat(randn(100), randn(100) .+ 2)\nresult = za_test(y; regression=:constant)\n\nReferences\n\nZivot, E., & Andrews, D. W. K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. JBES, 10(3), 251-270.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.ngperron_test-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.ngperron_test","text":"ngperron_test(y; regression=:constant) -> NgPerronResult\n\nNg-Perron unit root tests with GLS detrending (MZα, MZt, MSB, MPT).\n\nTests H₀: y has a unit root against H₁: y is stationary. These tests have better size properties than ADF/PP in small samples.\n\nArguments\n\ny: Time series vector\nregression: :constant (default) or :trend\n\nReturns\n\nNgPerronResult containing MZα, MZt, MSB, MPT statistics and critical values.\n\nExample\n\ny = cumsum(randn(100))\nresult = ngperron_test(y)\n# Check if MZt rejects at 5%\nresult.MZt < result.critical_values[:MZt][5]\n\nReferences\n\nNg, S., & Perron, P. (2001). Lag length selection and the construction of unit root tests with good size and power. Econometrica, 69(6), 1519-1554.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.johansen_test-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.johansen_test","text":"johansen_test(Y, p; deterministic=:constant) -> JohansenResult\n\nJohansen cointegration test for VAR system.\n\nTests for the number of cointegrating relationships among variables using trace and maximum eigenvalue tests.\n\nArguments\n\nY: Data matrix (T × n)\np: Number of lags in the VECM representation\ndeterministic: Specification for deterministic terms\n:none - No deterministic terms\n:constant - Constant in cointegrating relation (default)\n:trend - Linear trend in levels\n\nReturns\n\nJohansenResult containing trace and max-eigenvalue statistics, cointegrating vectors, adjustment coefficients, and estimated rank.\n\nExample\n\n# Generate cointegrated system\nn, T = 3, 200\nY = randn(T, n)\nY[:, 2] = Y[:, 1] + 0.1 * randn(T)  # Y2 cointegrated with Y1\n\nresult = johansen_test(Y, 2)\nresult.rank  # Should detect 1 or 2 cointegrating relations\n\nReferences\n\nJohansen, S. (1991). Estimation and hypothesis testing of cointegration vectors in Gaussian vector autoregressive models. Econometrica, 59(6), 1551-1580.\nOsterwald-Lenum, M. (1992). A note with quantiles of the asymptotic distribution of the ML cointegration rank test statistics. Oxford BEJM.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.is_stationary-Union{Tuple{VARModel{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.is_stationary","text":"is_stationary(model::VARModel) -> VARStationarityResult\n\nCheck if estimated VAR model is stationary.\n\nA VAR(p) is stationary if and only if all eigenvalues of the companion matrix have modulus strictly less than 1.\n\nReturns\n\nVARStationarityResult with:\n\nis_stationary: Boolean indicating stationarity\neigenvalues: Complex eigenvalues of companion matrix\nmax_modulus: Maximum eigenvalue modulus\ncompanion_matrix: The (np × np) companion form matrix\n\nExample\n\nmodel = estimate_var(Y, 2)\nresult = is_stationary(model)\nif !result.is_stationary\n    println(\"Warning: VAR is non-stationary, max modulus = \", result.max_modulus)\nend\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.test_all_variables-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.test_all_variables","text":"test_all_variables(Y; test=:adf, kwargs...) -> Vector\n\nApply unit root test to each column of Y.\n\nArguments\n\nY: Data matrix (T × n)\ntest: Test to apply (:adf, :kpss, :pp, :za, :ngperron)\nkwargs...: Additional arguments passed to the test\n\nReturns\n\nVector of test results, one per variable.\n\nExample\n\nY = randn(200, 3)\nY[:, 1] = cumsum(Y[:, 1])  # Make first column non-stationary\nresults = test_all_variables(Y; test=:adf)\n[r.pvalue for r in results]  # P-values for each variable\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.unit_root_summary-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.unit_root_summary","text":"unit_root_summary(y; tests=[:adf, :kpss, :pp], kwargs...) -> NamedTuple\n\nRun multiple unit root tests and return summary with PrettyTables output.\n\nArguments\n\ny: Time series vector\ntests: Vector of test symbols to run (default: [:adf, :kpss, :pp])\nkwargs...: Additional arguments passed to individual tests\n\nReturns\n\nNamedTuple with test results, conclusion, and summary table.\n\nExample\n\ny = cumsum(randn(200))\nsummary = unit_root_summary(y)\nsummary.conclusion  # Overall conclusion\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.lr_test","page":"Functions","title":"MacroEconometricModels.lr_test","text":"lr_test(m1, m2) -> LRTestResult\n\nLikelihood ratio test for nested models.\n\nComputes LR = -2(ℓR - ℓU) where the restricted model has fewer parameters. Under H₀ (restricted model is correct), LR ~ χ²(df) where df = dofU - dofR.\n\nWorks for any two models implementing loglikelihood, dof, and nobs from StatsAPI. Automatically determines which model is restricted by comparing degrees of freedom.\n\nArguments\n\nm1, m2: Two fitted models (order does not matter)\n\nReturns\n\nLRTestResult with test statistic, p-value, and model details.\n\nExample\n\nar2 = estimate_ar(y, 2; method=:mle)\nar4 = estimate_ar(y, 4; method=:mle)\nresult = lr_test(ar2, ar4)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.lm_test","page":"Functions","title":"MacroEconometricModels.lm_test","text":"lm_test(m1, m2) -> LMTestResult\n\nLagrange multiplier (score) test for nested models.\n\nEvaluates the score of the unrestricted log-likelihood at the restricted parameter estimates. Under H₀, LM = s'(-H)⁻¹s ~ χ²(df) where df = dofU - dofR.\n\nAutomatically determines restricted/unrestricted by comparing dof. Dispatches to model-family specific implementations for ARIMA, VAR, and volatility models.\n\nSupported model pairs\n\nAbstractARIMAModel × AbstractARIMAModel (same differencing order d)\nVARModel × VARModel (different lag orders, same data)\nARCHModel × ARCHModel, GARCHModel × GARCHModel\nARCHModel × GARCHModel (cross-type nesting)\nEGARCHModel × EGARCHModel, GJRGARCHModel × GJRGARCHModel\n\nArguments\n\nm1, m2: Two fitted models from the same family (order does not matter)\n\nReturns\n\nLMTestResult with test statistic, p-value, and score norm diagnostic.\n\nExample\n\nar2 = estimate_ar(y, 2; method=:mle)\nar4 = estimate_ar(y, 4; method=:mle)\nresult = lm_test(ar2, ar4)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.granger_test","page":"Functions","title":"MacroEconometricModels.granger_test","text":"granger_test(model::VARModel, cause::Int, effect::Int) -> GrangerCausalityResult\n\nTest whether variable cause Granger-causes variable effect in a VAR model.\n\nH₀: A₁[effect, cause] = A₂[effect, cause] = ... = Aₚ[effect, cause] = 0 H₁: At least one lag coefficient is nonzero\n\nUses a Wald χ² test with df = p (number of lags).\n\nArguments\n\nmodel: Estimated VARModel\ncause: Index of the causing variable (1-based)\neffect: Index of the effect variable (1-based)\n\nReturns\n\nGrangerCausalityResult with Wald statistic, p-value, and test details.\n\nExample\n\nY = randn(200, 3)\nm = estimate_var(Y, 2)\ng = granger_test(m, 1, 2)  # does variable 1 Granger-cause variable 2?\n\n\n\n\n\ngranger_test(model::VARModel, cause::Vector{Int}, effect::Int) -> GrangerCausalityResult\n\nTest whether a group of variables cause jointly Granger-cause variable effect.\n\nH₀: All lag coefficients from cause variables to the effect equation are zero H₁: At least one lag coefficient is nonzero\n\nUses a Wald χ² test with df = p × length(cause).\n\nArguments\n\nmodel: Estimated VARModel\ncause: Indices of the causing variables (1-based)\neffect: Index of the effect variable (1-based)\n\nReturns\n\nGrangerCausalityResult with Wald statistic, p-value, and test details.\n\nExample\n\nY = randn(200, 4)\nm = estimate_var(Y, 2)\ng = granger_test(m, [1, 2], 3)  # do variables 1 and 2 jointly Granger-cause variable 3?\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.granger_test_all","page":"Functions","title":"MacroEconometricModels.granger_test_all","text":"granger_test_all(model::VARModel) -> Matrix{Union{GrangerCausalityResult, Nothing}}\n\nCompute pairwise Granger causality tests for all variable pairs in a VAR model.\n\nReturns an n×n matrix where entry [i,j] tests whether variable j Granger-causes variable i. Diagonal entries are nothing.\n\nArguments\n\nmodel: Estimated VARModel\n\nReturns\n\nn×n matrix of GrangerCausalityResult (or nothing on diagonal).\n\nExample\n\nY = randn(200, 3)\nm = estimate_var(Y, 2)\nresults = granger_test_all(m)\nresults[2, 1]  # does variable 1 Granger-cause variable 2?\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_arch","page":"Functions","title":"MacroEconometricModels.estimate_arch","text":"estimate_arch(y, q; method=:mle) -> ARCHModel\n\nEstimate ARCH(q) model via Maximum Likelihood.\n\nσ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q\n\nUses two-stage optimization: NelderMead initialization → LBFGS refinement.\n\nArguments\n\ny: Time series vector\nq: ARCH order (≥ 1)\nmethod: Estimation method (currently only :mle)\n\nReturns\n\nARCHModel with estimated parameters and conditional variances.\n\nExample\n\ny = randn(500)\nmodel = estimate_arch(y, 1)\nprintln(\"ω = \", model.omega, \", α₁ = \", model.alpha[1])\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.arch_lm_test","page":"Functions","title":"MacroEconometricModels.arch_lm_test","text":"arch_lm_test(y_or_model, q=5)\n\nARCH-LM test for conditional heteroskedasticity (Engle 1982).\n\nH₀: No ARCH effects (α₁ = ... = αq = 0) H₁: ARCH(q) effects present\n\nTest statistic: T·R² from regression of ε²ₜ on ε²ₜ₋₁,...,ε²ₜ₋q, distributed χ²(q).\n\nArguments\n\ny_or_model: Raw data vector or AbstractVolatilityModel (uses standardized residuals)\nq: Number of lags (default 5)\n\nReturns\n\nNamed tuple (statistic, pvalue, q).\n\nExample\n\nresult = arch_lm_test(randn(500), 5)\nprintln(\"p-value: \", result.pvalue)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.ljung_box_squared","page":"Functions","title":"MacroEconometricModels.ljung_box_squared","text":"ljung_box_squared(z_or_model, K=10)\n\nLjung-Box test on squared (standardized) residuals.\n\nH₀: No serial correlation in z²ₜ H₁: Serial correlation present in z²ₜ\n\nTest statistic: Q = n(n+2) Σₖ ρ̂²ₖ/(n-k), distributed χ²(K).\n\nArguments\n\nz_or_model: Standardized residuals vector or AbstractVolatilityModel\nK: Number of lags (default 10)\n\nReturns\n\nNamed tuple (statistic, pvalue, K).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_garch","page":"Functions","title":"MacroEconometricModels.estimate_garch","text":"estimate_garch(y, p=1, q=1; method=:mle) -> GARCHModel\n\nEstimate GARCH(p,q) model via Maximum Likelihood (Bollerslev 1986).\n\nσ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q + β₁σ²ₜ₋₁ + ... + βpσ²ₜ₋p\n\nArguments\n\ny: Time series vector\np: GARCH order (default 1)\nq: ARCH order (default 1)\nmethod: Estimation method (currently only :mle)\n\nExample\n\nmodel = estimate_garch(y, 1, 1)\nprintln(\"Persistence: \", persistence(model))\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_egarch","page":"Functions","title":"MacroEconometricModels.estimate_egarch","text":"estimate_egarch(y, p=1, q=1; method=:mle) -> EGARCHModel\n\nEstimate EGARCH(p,q) model via Maximum Likelihood (Nelson 1991).\n\nlog(σ²ₜ) = ω + Σαᵢ(|zₜ₋ᵢ| - E|z|) + Σγᵢzₜ₋ᵢ + Σβⱼlog(σ²ₜ₋ⱼ)\n\nThe γ parameters capture leverage effects (typically γ < 0 for equities).\n\nArguments\n\ny: Time series vector\np: GARCH order (default 1)\nq: ARCH order (default 1)\nmethod: Estimation method (currently only :mle)\n\nExample\n\nmodel = estimate_egarch(y, 1, 1)\nprintln(\"Leverage: \", model.gamma[1])\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_gjr_garch","page":"Functions","title":"MacroEconometricModels.estimate_gjr_garch","text":"estimate_gjr_garch(y, p=1, q=1; method=:mle) -> GJRGARCHModel\n\nEstimate GJR-GARCH(p,q) model via Maximum Likelihood (Glosten, Jagannathan & Runkle 1993).\n\nσ²ₜ = ω + Σ(αᵢ + γᵢI(εₜ₋ᵢ<0))ε²ₜ₋ᵢ + Σβⱼσ²ₜ₋ⱼ\n\nγᵢ > 0 captures the asymmetric (leverage) effect.\n\nArguments\n\ny: Time series vector\np: GARCH order (default 1)\nq: ARCH order (default 1)\nmethod: Estimation method (currently only :mle)\n\nExample\n\nmodel = estimate_gjr_garch(y, 1, 1)\nprintln(\"Asymmetry: \", model.gamma[1])\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.news_impact_curve","page":"Functions","title":"MacroEconometricModels.news_impact_curve","text":"news_impact_curve(m; range=(-3,3), n_points=200)\n\nCompute the news impact curve: how a shock εₜ₋₁ maps to σ²ₜ, holding all else at unconditional values.\n\nReturns named tuple (shocks, variance) where both are vectors of length n_points.\n\nSupported models\n\nGARCHModel: Symmetric parabola\nGJRGARCHModel: Asymmetric parabola (steeper for negative shocks)\nEGARCHModel: Asymmetric exponential curve\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.estimate_sv","page":"Functions","title":"MacroEconometricModels.estimate_sv","text":"estimate_sv(y; n_samples=2000, burnin=1000,\n            dist=:normal, leverage=false,\n            quantile_levels=[0.025, 0.5, 0.975]) -> SVModel\n\nEstimate a Stochastic Volatility model via Kim-Shephard-Chib (1998) Gibbs sampler with Omori et al. (2007) 10-component mixture approximation.\n\nModel\n\nyₜ = exp(hₜ/2) εₜ,       εₜ ~ N(0,1)\nhₜ = μ + φ(hₜ₋₁ - μ) + σ_η ηₜ,  ηₜ ~ N(0,1)\n\nArguments\n\ny: Time series vector\nn_samples: Number of posterior samples to keep (default 2000)\nburnin: Number of burn-in iterations to discard (default 1000)\ndist: Error distribution (:normal or :studentt)\nleverage: Whether to include leverage effect (correlated innovations)\nquantile_levels: Quantile levels for volatility posterior\n\nExample\n\ny = randn(200) .* exp.(cumsum(0.1 .* randn(200)) ./ 2)\nmodel = estimate_sv(y; n_samples=1000)\nprintln(\"φ = \", mean(model.phi_post))\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{ARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::ARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from an ARCH(q) model.\n\nFor h > q, the forecast converges to the unconditional variance. Confidence intervals are computed via simulation.\n\nArguments\n\nm: Fitted ARCHModel\nh: Forecast horizon\nconf_level: Confidence level for intervals (default 0.95)\nn_sim: Number of simulation paths for CIs (default 10000)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{EGARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::EGARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from an EGARCH(p,q) model via simulation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{GARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::GARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from a GARCH(p,q) model.\n\nUses analytical iteration for point forecasts and simulation for CIs. Point forecasts converge to unconditional variance as h → ∞.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{GJRGARCHModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::GJRGARCHModel, h; conf_level=0.95, n_sim=10000) -> VolatilityForecast\n\nForecast conditional variance from a GJR-GARCH(p,q) model via simulation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{SVModel{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(m::SVModel, h; conf_level=0.95) -> VolatilityForecast\n\nPosterior predictive forecast of volatility from an SV model.\n\nFor each posterior draw (μ, φ, ση), simulates the log-volatility path forward h{T+1}, ..., h_{T+h} and returns quantiles of exp(hₜ).\n\nArguments\n\nm: Fitted SVModel\nh: Forecast horizon\nconf_level: Confidence level for intervals (default 0.95)\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.persistence","page":"Functions","title":"MacroEconometricModels.persistence","text":"Return persistence Σαᵢ for ARCH model.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.halflife","page":"Functions","title":"MacroEconometricModels.halflife","text":"Return half-life of volatility shocks: log(0.5) / log(persistence).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.unconditional_variance","page":"Functions","title":"MacroEconometricModels.unconditional_variance","text":"Return unconditional variance ω / (1 - Σαᵢ).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.arch_order","page":"Functions","title":"MacroEconometricModels.arch_order","text":"Return ARCH order q.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.garch_order","page":"Functions","title":"MacroEconometricModels.garch_order","text":"Return GARCH order p.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#StatsAPI.nobs-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.coef-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.coef","text":"Coefficient vector [μ, ω, α₁, …, αq].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.residuals","text":"Raw residuals hatvarepsilon_t.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.aic-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.aic","text":"Akaike Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.bic-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.bic","text":"Bayesian Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.dof","text":"Number of estimated parameters: 2 + q (μ + ω + q alphas).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.islinear-Tuple{ARCHModel}","page":"Functions","title":"StatsAPI.islinear","text":"false — ARCH models are nonlinear.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.coef-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.coef","text":"Coefficient vector [μ, ω, α₁, …, αq, β₁, …, βp].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.residuals","text":"Raw residuals hatvarepsilon_t.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.aic-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.aic","text":"Akaike Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.bic-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.bic","text":"Bayesian Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.dof","text":"Number of estimated parameters: 2 + q + p.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.islinear-Tuple{GARCHModel}","page":"Functions","title":"StatsAPI.islinear","text":"false — GARCH models are nonlinear.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.coef-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.coef","text":"Coefficient vector [μ, ω, α₁, …, αq, γ₁, …, γq, β₁, …, βp].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.residuals","text":"Raw residuals hatvarepsilon_t.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.aic-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.aic","text":"Akaike Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.bic-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.bic","text":"Bayesian Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.dof","text":"Number of estimated parameters: 2 + 2q + p.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.islinear-Tuple{EGARCHModel}","page":"Functions","title":"StatsAPI.islinear","text":"false — EGARCH models are nonlinear.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.coef-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.coef","text":"Coefficient vector [μ, ω, α₁, …, αq, γ₁, …, γq, β₁, …, βp].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.residuals","text":"Raw residuals hatvarepsilon_t.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.aic-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.aic","text":"Akaike Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.bic-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.bic","text":"Bayesian Information Criterion.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.dof-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.dof","text":"Number of estimated parameters: 2 + 2q + p.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.islinear-Tuple{GJRGARCHModel}","page":"Functions","title":"StatsAPI.islinear","text":"false — GJR-GARCH models are nonlinear.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.nobs-Tuple{SVModel}","page":"Functions","title":"StatsAPI.nobs","text":"Number of observations.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.coef-Tuple{SVModel}","page":"Functions","title":"StatsAPI.coef","text":"Posterior mean coefficients [μ, φ, σ_η].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.residuals-Tuple{SVModel}","page":"Functions","title":"StatsAPI.residuals","text":"Standardized residuals y_t  sqrthatsigma^2_t.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#StatsAPI.islinear-Tuple{SVModel}","page":"Functions","title":"StatsAPI.islinear","text":"false — SV models are nonlinear.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.nowcast_dfm","page":"Functions","title":"MacroEconometricModels.nowcast_dfm","text":"nowcast_dfm(Y, nM, nQ; r=2, p=1, idio=:ar1, blocks=nothing,\n            max_iter=100, thresh=1e-4) -> NowcastDFM{T}\n\nEstimate a dynamic factor model on mixed-frequency data with missing values.\n\nThe first nM columns of Y are monthly variables; the next nQ columns are quarterly variables (observed every 3rd month, NaN otherwise).\n\nArguments\n\nY::AbstractMatrix — T_obs × N data matrix (NaN for missing)\nnM::Int — number of monthly variables\nnQ::Int — number of quarterly variables\n\nKeyword Arguments\n\nr::Int=2 — number of factors\np::Int=1 — VAR lags in factor dynamics\nidio::Symbol=:ar1 — idiosyncratic component (:ar1 or :iid)\nblocks::Union{Matrix{Int},Nothing}=nothing — block structure (N × n_blocks)\nmax_iter::Int=100 — maximum EM iterations\nthresh::Real=1e-4 — convergence threshold (relative log-likelihood change)\n\nReturns\n\nNowcastDFM{T} with smoothed data, factors, and state-space parameters.\n\nReferences\n\nBańbura, M. & Modugno, M. (2014). Maximum Likelihood Estimation of Factor Models on Datasets with Arbitrary Pattern of Missing Data.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.nowcast_bvar","page":"Functions","title":"MacroEconometricModels.nowcast_bvar","text":"nowcast_bvar(Y, nM, nQ; lags=5, thresh=1e-6, max_iter=200,\n            lambda0=0.2, theta0=1.0, miu0=1.0, alpha0=2.0) -> NowcastBVAR{T}\n\nEstimate a large BVAR for mixed-frequency nowcasting.\n\nThe first nM columns are monthly variables; the next nQ columns are quarterly (observed every 3rd month). The BVAR is estimated on the complete (non-NaN) portion, then the Kalman smoother fills the ragged edge.\n\nArguments\n\nY::AbstractMatrix — T_obs × N data matrix (NaN for missing)\nnM::Int — number of monthly variables\nnQ::Int — number of quarterly variables\n\nKeyword Arguments\n\nlags::Int=5 — number of lags\nthresh::Real=1e-6 — optimization convergence threshold\nmax_iter::Int=200 — max optimization iterations\nlambda0::Real=0.2 — initial overall shrinkage\ntheta0::Real=1.0 — initial cross-variable shrinkage\nmiu0::Real=1.0 — initial sum-of-coefficients weight\nalpha0::Real=2.0 — initial co-persistence weight\n\nReturns\n\nNowcastBVAR{T} with smoothed data and posterior parameters.\n\nReferences\n\nCimadomo, J., Giannone, D., Lenza, M., Monti, F. & Sokol, A. (2022). Nowcasting with Large Bayesian Vector Autoregressions.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.nowcast_bridge","page":"Functions","title":"MacroEconometricModels.nowcast_bridge","text":"nowcast_bridge(Y, nM, nQ; lagM=1, lagQ=1, lagY=1) -> NowcastBridge{T}\n\nEstimate bridge equation combination nowcast.\n\nEach bridge equation uses a pair of monthly indicators (aggregated to quarterly) plus optional lagged quarterly variables and autoregressive terms to predict the target quarterly variable (last column).\n\nArguments\n\nY::AbstractMatrix — T_obs × N data matrix (NaN for missing)\nnM::Int — number of monthly variables\nnQ::Int — number of quarterly variables (last nQ columns)\n\nKeyword Arguments\n\nlagM::Int=1 — lags for monthly indicators (after quarterly aggregation)\nlagQ::Int=1 — lags for quarterly indicators\nlagY::Int=1 — autoregressive lags for target variable\n\nReturns\n\nNowcastBridge{T} with combined and individual nowcasts.\n\nReferences\n\nBańbura, M., Belousova, I., Bodnár, K. & Tóth, M. B. (2023). Nowcasting Employment in the Euro Area. ECB Working Paper No 2815.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.nowcast","page":"Functions","title":"MacroEconometricModels.nowcast","text":"nowcast(model::AbstractNowcastModel; target_var=nothing) -> NowcastResult{T}\n\nGenerate nowcast result from an estimated nowcasting model.\n\nReturns the current-quarter nowcast and next-quarter forecast for the target variable (default: last quarterly variable).\n\nArguments\n\nmodel::AbstractNowcastModel — estimated nowcast model\n\nKeyword Arguments\n\ntarget_var::Union{Int,Nothing}=nothing — target variable index (default: last column)\n\nReturns\n\nNowcastResult{T} with nowcast and forecast values.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.forecast-Union{Tuple{T}, Tuple{NowcastDFM{T}, Int64}} where T","page":"Functions","title":"MacroEconometricModels.forecast","text":"forecast(model::NowcastDFM, h; target_var=nothing) -> Vector{T}\n\nGenerate h-step ahead monthly forecast from DFM nowcasting model.\n\nArguments\n\nmodel::NowcastDFM — estimated model\nh::Int — forecast horizon (months)\n\nKeyword Arguments\n\ntarget_var::Union{Int,Nothing} — variable to forecast (default: all)\n\nReturns\n\nVector of h forecast values (if target_var specified) or Matrix h × N.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.nowcast-Union{Tuple{NowcastDFM{T}}, Tuple{T}} where T","page":"Functions","title":"MacroEconometricModels.nowcast","text":"nowcast(model::AbstractNowcastModel; target_var=nothing) -> NowcastResult{T}\n\nGenerate nowcast result from an estimated nowcasting model.\n\nReturns the current-quarter nowcast and next-quarter forecast for the target variable (default: last quarterly variable).\n\nArguments\n\nmodel::AbstractNowcastModel — estimated nowcast model\n\nKeyword Arguments\n\ntarget_var::Union{Int,Nothing}=nothing — target variable index (default: last column)\n\nReturns\n\nNowcastResult{T} with nowcast and forecast values.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.nowcast_news","page":"Functions","title":"MacroEconometricModels.nowcast_news","text":"nowcast_news(X_new, X_old, model::NowcastDFM, target_period;\n             target_var=size(X_new,2), groups=nothing) -> NowcastNews{T}\n\nCompute news decomposition between two data vintages.\n\nIdentifies new data releases (positions where X_old is NaN but X_new is not), computes their individual impacts on the nowcast via Kalman gain weights, and decomposes the total revision.\n\nArguments\n\nX_new::AbstractMatrix — new data vintage (T_obs × N)\nX_old::AbstractMatrix — old data vintage (same size, more NaN)\nmodel::NowcastDFM — estimated DFM model\ntarget_period::Int — time period for which to compute nowcast\n\nKeyword Arguments\n\ntarget_var::Int — target variable index (default: last column)\ngroups::Union{Vector{Int},Nothing} — group assignment per variable (for aggregation)\n\nReturns\n\nNowcastNews{T} with per-release impacts and total decomposition.\n\nReferences\n\nBańbura, M. & Modugno, M. (2014). Maximum Likelihood Estimation of Factor Models on Datasets with Arbitrary Pattern of Missing Data.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.balance_panel","page":"Functions","title":"MacroEconometricModels.balance_panel","text":"balance_panel(pd::PanelData; method=:dfm, r=3, p=2) -> PanelData\n\nBalance a panel dataset by filling missing values (NaN) using DFM-based nowcasting to estimate missing observations.\n\nFor each group with missing data, runs nowcast_dfm treating all variables as monthly (nM = n_vars, nQ = 0) to obtain Kalman-smoothed estimates.\n\nArguments\n\npd::PanelData — input panel data (may be unbalanced or have NaN)\n\nKeyword Arguments\n\nmethod::Symbol=:dfm — fill method (currently only :dfm)\nr::Int=3 — number of factors for DFM\np::Int=2 — VAR lags in DFM factor dynamics\n\nReturns\n\nNew PanelData with NaN filled and balanced=true if applicable.\n\nExamples\n\npd = xtset(df, :id, :t)\npd_bal = balance_panel(pd; r=2, p=1)\nisbalanced(pd_bal)\n\n\n\n\n\nbalance_panel(ts::TimeSeriesData; method=:dfm, r=3, p=2) -> TimeSeriesData\n\nFill missing values (NaN) in a TimeSeriesData container using DFM nowcasting.\n\nExamples\n\nts = TimeSeriesData(randn(100, 3))\nts.data[95:100, 2] .= NaN\nts_bal = balance_panel(ts; r=2)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._coef_table-Union{Tuple{T}, Tuple{IO, String, Vector{String}, Vector{T}, Vector{T}}} where T","page":"Functions","title":"MacroEconometricModels._coef_table","text":"_coef_table(io, title, names, coefs, se; dist=:z, dof_r=0, level=0.95)\n\nPublication-quality 7-column coefficient table (Stata/EViews style).\n\nColumns: Name | Coef. | Std.Err. | z/t | P>|z/t| | [95% CI lower | CI upper] | stars\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._matrix_table-Tuple{IO, AbstractMatrix, String}","page":"Functions","title":"MacroEconometricModels._matrix_table","text":"Print a labeled matrix as a PrettyTables table.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._pretty_table-Tuple{IO, Any}","page":"Functions","title":"MacroEconometricModels._pretty_table","text":"_pretty_table(io::IO, data; kwargs...)\n\nCentral PrettyTables wrapper that respects the global display backend.\n\nFor :text backend, applies _TEXT_TABLE_FORMAT automatically. For :latex and :html backends, omits text-only formatting options.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._select_horizons-Tuple{Int64}","page":"Functions","title":"MacroEconometricModels._select_horizons","text":"Select representative horizons for display.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._sig_legend-Tuple{IO}","page":"Functions","title":"MacroEconometricModels._sig_legend","text":"Print significance legend footer.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.get_display_backend-Tuple{}","page":"Functions","title":"MacroEconometricModels.get_display_backend","text":"get_display_backend() -> Symbol\n\nReturn the current PrettyTables display backend (:text, :latex, or :html).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.set_display_backend-Tuple{Symbol}","page":"Functions","title":"MacroEconometricModels.set_display_backend","text":"set_display_backend(backend::Symbol)\n\nSet the PrettyTables output backend. Options: :text (default), :latex, :html.\n\nExamples\n\nset_display_backend(:latex)   # all show() methods now emit LaTeX\nset_display_backend(:html)    # switch to HTML tables\nset_display_backend(:text)    # back to terminal-friendly text\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.refs","page":"Functions","title":"MacroEconometricModels.refs","text":"refs([io::IO], x; format=get_display_backend())\n\nPrint bibliographic references for a model, result, or method.\n\nSupports four output formats via the format keyword:\n\n:text — AEA plain text (default, follows get_display_backend())\n:latex — \\bibitem{} entries\n:bibtex — BibTeX @article{}/@book{} entries\n:html — HTML with clickable DOI links\n\nDispatch\n\nInstance dispatch: refs(model) prints references for the model type\nSymbol dispatch: refs(:fastica) prints references for a method name\n\nExamples\n\nmodel = estimate_var(Y, 2)\nrefs(model)                        # AEA text to stdout\nrefs(model; format=:bibtex)        # BibTeX entries\n\nrefs(:johansen)                    # Johansen (1991)\nrefs(:fastica; format=:latex)      # Hyvärinen (1999) as \\bibitem\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.jarque_bera_test","page":"Functions","title":"MacroEconometricModels.jarque_bera_test","text":"jarque_bera_test(model::VARModel; method=:multivariate) -> NormalityTestResult\njarque_bera_test(U::AbstractMatrix; method=:multivariate)  -> NormalityTestResult\n\nMultivariate Jarque-Bera test for normality of VAR residuals.\n\nMethods:\n\n:multivariate — joint test based on multivariate skewness and kurtosis (Lütkepohl 2005)\n:component — component-wise univariate JB tests on standardized residuals\n\nReference: Jarque & Bera (1980), Lütkepohl (2005, §4.5)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.mardia_test","page":"Functions","title":"MacroEconometricModels.mardia_test","text":"mardia_test(model::VARModel; type=:both) -> NormalityTestResult\nmardia_test(U::AbstractMatrix; type=:both) -> NormalityTestResult\n\nMardia's tests for multivariate normality based on multivariate skewness and kurtosis.\n\nTypes:\n\n:skewness — tests multivariate skewness b₁,ₖ\n:kurtosis — tests multivariate kurtosis b₂,ₖ\n:both — combined test (sum of both statistics)\n\nUnder H₀: T·b₁,ₖ/6 ~ χ²(k(k+1)(k+2)/6), (b₂,ₖ - k(k+2)) / √(8k(k+2)/T) ~ N(0,1).\n\nReference: Mardia (1970)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.doornik_hansen_test","page":"Functions","title":"MacroEconometricModels.doornik_hansen_test","text":"doornik_hansen_test(model::VARModel) -> NormalityTestResult\ndoornik_hansen_test(U::AbstractMatrix)  -> NormalityTestResult\n\nDoornik-Hansen omnibus test for multivariate normality.\n\nApplies the Bowman-Shenton transformation to each component's skewness and kurtosis, then sums z₁² + z₂² across components. Under H₀: DH ~ χ²(2k).\n\nReference: Doornik & Hansen (2008)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.henze_zirkler_test","page":"Functions","title":"MacroEconometricModels.henze_zirkler_test","text":"henze_zirkler_test(model::VARModel) -> NormalityTestResult\nhenze_zirkler_test(U::AbstractMatrix)  -> NormalityTestResult\n\nHenze-Zirkler test for multivariate normality based on the empirical characteristic function.\n\nThe test statistic is:\n\nT_beta = frac1n sum_ij e^-beta^2 D_ij2 - 2(1+beta^2)^-k2 sum_i e^-beta^2 d_i^2(2(1+beta^2)) + n(1+2beta^2)^-k2\n\nwhere D_ij = (z_i - z_j)(z_i - z_j) and d_i = z_i z_i.\n\nReference: Henze & Zirkler (1990)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.normality_test_suite","page":"Functions","title":"MacroEconometricModels.normality_test_suite","text":"normality_test_suite(model::VARModel) -> NormalityTestSuite\nnormality_test_suite(U::AbstractMatrix)  -> NormalityTestSuite\n\nRun all available multivariate normality tests and return a NormalityTestSuite.\n\nTests included:\n\nMultivariate Jarque-Bera\nComponent-wise Jarque-Bera\nMardia skewness\nMardia kurtosis\nMardia combined\nDoornik-Hansen\nHenze-Zirkler\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_fastica","page":"Functions","title":"MacroEconometricModels.identify_fastica","text":"identify_fastica(model::VARModel; contrast=:logcosh, approach=:deflation,\n                 max_iter=200, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR via FastICA (Hyvärinen 1999).\n\nRecovers independent non-Gaussian structural shocks by maximizing non-Gaussianity of the recovered sources.\n\nArguments:\n\ncontrast — non-Gaussianity measure: :logcosh (default, robust), :exp, :kurtosis\napproach — :deflation (one-by-one) or :symmetric (simultaneous)\nmax_iter — maximum iterations per component\ntol — convergence tolerance\n\nReference: Hyvärinen (1999)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_jade","page":"Functions","title":"MacroEconometricModels.identify_jade","text":"identify_jade(model::VARModel; max_iter=100, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR via JADE (Joint Approximate Diagonalization of Eigenmatrices).\n\nUses fourth-order cumulant matrices and joint diagonalization via Jacobi rotations.\n\nReference: Cardoso & Souloumiac (1993)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_sobi","page":"Functions","title":"MacroEconometricModels.identify_sobi","text":"identify_sobi(model::VARModel; lags=1:12, max_iter=100, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR via SOBI (Second-Order Blind Identification).\n\nUses autocovariance matrices at multiple lags and joint diagonalization. Exploits temporal structure rather than higher-order statistics.\n\nReference: Belouchrani et al. (1997)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_dcov","page":"Functions","title":"MacroEconometricModels.identify_dcov","text":"identify_dcov(model::VARModel; max_iter=200, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR by minimizing pairwise distance covariance between recovered shocks.\n\nDistance covariance (Székely et al. 2007) is zero iff the variables are independent, making it a natural criterion for ICA.\n\nReference: Matteson & Tsay (2017)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_hsic","page":"Functions","title":"MacroEconometricModels.identify_hsic","text":"identify_hsic(model::VARModel; kernel=:gaussian, sigma=1.0,\n              max_iter=200, tol=1e-6) -> ICASVARResult\n\nIdentify SVAR by minimizing pairwise HSIC between recovered shocks.\n\nHSIC with a characteristic kernel (Gaussian) is zero iff variables are independent.\n\nReference: Gretton et al. (2005)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_student_t","page":"Functions","title":"MacroEconometricModels.identify_student_t","text":"identify_student_t(model::VARModel; max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR assuming Student-t distributed structural shocks.\n\nEach shock εⱼ ~ t(νⱼ) (standardized to unit variance). Identification is achieved when at most one νⱼ = ∞ (Gaussian).\n\nReference: Lanne, Meitz & Saikkonen (2017)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_mixture_normal","page":"Functions","title":"MacroEconometricModels.identify_mixture_normal","text":"identify_mixture_normal(model::VARModel; n_components=2, max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR assuming mixture-of-normals distributed structural shocks.\n\nEach shock εⱼ ~ pj N(0,σ₁ⱼ²) + (1-pj) N(0,σ₂ⱼ²) with unit variance constraint.\n\nReference: Lanne & Lütkepohl (2010)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_pml","page":"Functions","title":"MacroEconometricModels.identify_pml","text":"identify_pml(model::VARModel; max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR via Pseudo Maximum Likelihood using Pearson Type IV distributions.\n\nAllows both skewness and excess kurtosis in the structural shocks.\n\nReference: Herwartz (2018)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_skew_normal","page":"Functions","title":"MacroEconometricModels.identify_skew_normal","text":"identify_skew_normal(model::VARModel; max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nIdentify SVAR assuming skew-normal distributed structural shocks.\n\nEach shock εⱼ has pdf f(x) = 2 φ(x) Φ(αⱼ x), where αⱼ controls skewness.\n\nReference: Azzalini (1985)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_nongaussian_ml","page":"Functions","title":"MacroEconometricModels.identify_nongaussian_ml","text":"identify_nongaussian_ml(model::VARModel; distribution=:student_t,\n                        max_iter=500, tol=1e-6) -> NonGaussianMLResult\n\nUnified non-Gaussian ML SVAR identification dispatcher.\n\nSupported distributions:\n\n:student_t — independent Student-t shocks (Lanne, Meitz & Saikkonen 2017)\n:mixture_normal — mixture of two normals (Lanne & Lütkepohl 2010)\n:pml — Pearson Type IV / Pseudo-ML (Herwartz 2018)\n:skew_normal — skew-normal (Azzalini 1985)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_markov_switching","page":"Functions","title":"MacroEconometricModels.identify_markov_switching","text":"identify_markov_switching(model::VARModel; n_regimes=2, max_iter=500, tol=1e-6) -> MarkovSwitchingSVARResult\n\nIdentify SVAR via Markov-switching heteroskedasticity (Lanne & Lütkepohl 2008).\n\nEstimates regime-specific covariance matrices Σ₁, Σ₂, ..., Σ_K via EM algorithm, then identifies B₀ from the eigendecomposition of Σ₁⁻¹ Σ₂.\n\nIdentification requires that the relative variance ratios (eigenvalues) are distinct.\n\nReference: Lanne & Lütkepohl (2008), Rigobon (2003)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_garch","page":"Functions","title":"MacroEconometricModels.identify_garch","text":"identify_garch(model::VARModel; max_iter=500, tol=1e-6) -> GARCHSVARResult\n\nIdentify SVAR via GARCH-based heteroskedasticity (Normandin & Phaneuf 2004).\n\nIterative procedure:\n\nStart with Cholesky B₀\nCompute structural shocks εt = B₀⁻¹ ut\nFit GARCH(1,1) to each ε_j,t\nUse conditional covariances to re-estimate B₀\nRepeat until convergence\n\nReference: Normandin & Phaneuf (2004)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_smooth_transition","page":"Functions","title":"MacroEconometricModels.identify_smooth_transition","text":"identify_smooth_transition(model::VARModel, transition_var::AbstractVector;\n                           max_iter=500, tol=1e-6) -> SmoothTransitionSVARResult\n\nIdentify SVAR via smooth-transition heteroskedasticity (Lütkepohl & Netšunajev 2017).\n\nThe covariance matrix varies smoothly between two regimes:\n\nSigma_t = B_0 I + G(s_t)(Lambda - I) B_0\n\nwhere G(st) = 1/(1 + exp(-γ(st - c))) is the logistic transition function.\n\nArguments:\n\ntransition_var — the transition variable s_t (e.g., a lagged endogenous variable)\n\nReference: Lütkepohl & Netšunajev (2017)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.identify_external_volatility","page":"Functions","title":"MacroEconometricModels.identify_external_volatility","text":"identify_external_volatility(model::VARModel, regime_indicator::AbstractVector{Int};\n                             regimes=2) -> ExternalVolatilitySVARResult\n\nIdentify SVAR via externally specified volatility regimes (Rigobon 2003).\n\nUses a known regime indicator (e.g., NBER recessions, financial crises) to split the sample and estimate regime-specific covariance matrices.\n\nArguments:\n\nregime_indicator — integer vector of regime labels (1, 2, ..., K)\nregimes — number of distinct regimes (default: 2)\n\nReference: Rigobon (2003)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_identification_strength","page":"Functions","title":"MacroEconometricModels.test_identification_strength","text":"test_identification_strength(model::VARModel; method=:fastica,\n                             n_bootstrap=999) -> IdentifiabilityTestResult\n\nTest the strength of non-Gaussian identification via bootstrap.\n\nResamples residuals with replacement, re-estimates B₀, and computes the Procrustes distance between bootstrap and original B₀. Small distances indicate strong identification.\n\nReturns: test statistic = median Procrustes distance, p-value from distribution.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_shock_gaussianity","page":"Functions","title":"MacroEconometricModels.test_shock_gaussianity","text":"test_shock_gaussianity(result::ICASVARResult) -> IdentifiabilityTestResult\ntest_shock_gaussianity(result::NonGaussianMLResult) -> IdentifiabilityTestResult\n\nTest whether recovered structural shocks are non-Gaussian using univariate JB tests.\n\nNon-Gaussian identification requires at most one shock to be Gaussian. This test checks each shock individually and reports the joint result.\n\nAt most one Gaussian shock → identification holds.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_gaussian_vs_nongaussian","page":"Functions","title":"MacroEconometricModels.test_gaussian_vs_nongaussian","text":"test_gaussian_vs_nongaussian(model::VARModel; distribution=:student_t) -> IdentifiabilityTestResult\n\nLikelihood ratio test: H₀ Gaussian vs H₁ non-Gaussian structural shocks.\n\nUnder H₀, the LR statistic LR = 2(ℓ₁ - ℓ₀) ~ χ²(nextraparams).\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_shock_independence","page":"Functions","title":"MacroEconometricModels.test_shock_independence","text":"test_shock_independence(result::ICASVARResult; max_lag=10) -> IdentifiabilityTestResult\ntest_shock_independence(result::NonGaussianMLResult; max_lag=10) -> IdentifiabilityTestResult\n\nTest independence of recovered structural shocks.\n\nUses both cross-correlation (portmanteau) and distance covariance tests. Independence is a necessary condition for valid identification.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.test_overidentification","page":"Functions","title":"MacroEconometricModels.test_overidentification","text":"test_overidentification(model::VARModel, result::AbstractNonGaussianSVAR;\n                        restrictions=nothing, n_bootstrap=499) -> IdentifiabilityTestResult\n\nTest overidentifying restrictions for non-Gaussian SVAR.\n\nWhen additional restrictions beyond non-Gaussianity are imposed (e.g., zero restrictions on B₀), this test checks whether those restrictions are consistent with the data.\n\nUses a bootstrap approach: compares the restricted log-likelihood to bootstrap distribution.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.newey_west","page":"Functions","title":"MacroEconometricModels.newey_west","text":"newey_west(X::AbstractMatrix{T}, residuals::AbstractVector{T};\n           bandwidth::Int=0, kernel::Symbol=:bartlett, prewhiten::Bool=false,\n           XtX_inv::Union{Nothing,AbstractMatrix{T}}=nothing) -> Matrix{T}\n\nCompute Newey-West HAC covariance matrix.\n\nV_NW = (X'X)^{-1} S (X'X)^{-1} where S = Γ₀ + Σⱼ₌₁ᵐ w(j) (Γⱼ + Γⱼ')\n\nArguments\n\nX: Design matrix (n × k)\nresiduals: Residuals vector (n × 1)\nbandwidth: Truncation lag (0 = automatic selection)\nkernel: Kernel function\nprewhiten: Use AR(1) prewhitening\nXtX_inv: Pre-computed (X'X)^{-1} for performance (optional)\n\nReturns\n\nRobust covariance matrix (k × k)\n\nPerformance\n\nPass XtX_inv when calling multiple times with the same X to avoid recomputation.\n\n\n\n\n\nnewey_west(X::AbstractMatrix{T}, residuals::AbstractMatrix{T}; ...) -> Matrix{T}\n\nMultivariate version for systems of equations.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.white_vcov","page":"Functions","title":"MacroEconometricModels.white_vcov","text":"white_vcov(X::AbstractMatrix{T}, residuals::AbstractVector{T}; variant::Symbol=:hc0,\n           XtX_inv::Union{Nothing,AbstractMatrix{T}}=nothing) -> Matrix{T}\n\nWhite heteroscedasticity-robust covariance estimator.\n\nVariants: :hc0, :hc1, :hc2, :hc3\n\nArguments\n\nX: Design matrix (n × k)\nresiduals: Residuals vector (n × 1)\nvariant: HC variant (:hc0 = standard, :hc1 = small sample, :hc2/:hc3 = leverage-adjusted)\nXtX_inv: Pre-computed (X'X)^{-1} for performance (optional)\n\nReturns\n\nRobust covariance matrix (k × k)\n\nPerformance\n\nPass XtX_inv when calling multiple times with the same X to avoid recomputation.\n\n\n\n\n\nwhite_vcov(X::AbstractMatrix{T}, residuals::AbstractMatrix{T}; ...) -> Matrix{T}\n\nMultivariate version.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.driscoll_kraay","page":"Functions","title":"MacroEconometricModels.driscoll_kraay","text":"driscoll_kraay(X::AbstractMatrix{T}, u::AbstractVector{T};\n               bandwidth::Int=0, kernel::Symbol=:bartlett,\n               XtX_inv::Union{Nothing,AbstractMatrix{T}}=nothing) -> Matrix{T}\n\nDriscoll-Kraay standard errors for time series regression.\n\nIn a pure time series context, this is equivalent to Newey-West HAC estimation applied to the moment conditions X'u. For panel data applications, it would average across cross-sectional units first, but here we treat the data as a single time series.\n\nArguments\n\nX: Design matrix (T × k)\nu: Residuals vector (T × 1)\nbandwidth: Bandwidth for kernel. If 0, uses optimal bandwidth selection.\nkernel: Kernel function (:bartlett, :parzen, :quadraticspectral, :tukeyhanning)\nXtX_inv: Pre-computed (X'X)^{-1} for performance (optional)\n\nReturns\n\nRobust covariance matrix (k × k)\n\nReferences\n\nDriscoll, J. C., & Kraay, A. C. (1998). Consistent covariance matrix estimation with spatially dependent panel data. Review of Economics and Statistics.\n\nPerformance\n\nPass XtX_inv when calling multiple times with the same X to avoid recomputation.\n\n\n\n\n\ndriscoll_kraay(X::AbstractMatrix{T}, U::AbstractMatrix{T};\n               bandwidth::Int=0, kernel::Symbol=:bartlett) -> Matrix{T}\n\nDriscoll-Kraay standard errors for multi-equation system.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.robust_vcov","page":"Functions","title":"MacroEconometricModels.robust_vcov","text":"robust_vcov(X::AbstractMatrix{T}, residuals::AbstractVecOrMat{T},\n            estimator::AbstractCovarianceEstimator) -> Matrix{T}\n\nDispatch to appropriate covariance estimator based on estimator type.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.long_run_variance","page":"Functions","title":"MacroEconometricModels.long_run_variance","text":"long_run_variance(x::AbstractVector{T}; bandwidth::Int=0, kernel::Symbol=:bartlett) -> T\n\nEstimate long-run variance: S = Σⱼ₌₋∞^∞ γⱼ\n\nUsed for unit root tests, cointegration tests, and other applications requiring consistent variance estimation under serial correlation.\n\nArguments\n\nx: Time series vector\nbandwidth: Truncation lag (0 = automatic)\nkernel: Kernel function\n\nReturns\n\nLong-run variance estimate (scalar)\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.long_run_covariance","page":"Functions","title":"MacroEconometricModels.long_run_covariance","text":"long_run_covariance(X::AbstractMatrix{T}; bandwidth::Int=0, kernel::Symbol=:bartlett) -> Matrix{T}\n\nEstimate long-run covariance matrix of multivariate time series.\n\nArguments\n\nX: Multivariate time series (T × k)\nbandwidth: Truncation lag (0 = automatic)\nkernel: Kernel function\n\nReturns\n\nLong-run covariance matrix (k × k)\n\nPerformance\n\nUses BLAS matrix operations for lag autocovariance computation.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels.optimal_bandwidth_nw","page":"Functions","title":"MacroEconometricModels.optimal_bandwidth_nw","text":"optimal_bandwidth_nw(residuals::AbstractVector{T}) -> Int\n\nCompute optimal bandwidth using Newey-West (1994) automatic selection.\n\n\n\n\n\noptimal_bandwidth_nw(residuals::AbstractMatrix{T}) -> Int\n\nMultivariate version: average optimal bandwidth across columns.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._default_names-Tuple{Int64, String}","page":"Functions","title":"MacroEconometricModels._default_names","text":"Generate default names: [\"prefix 1\", \"prefix 2\", ...]\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._suppress_warnings-Tuple{Any}","page":"Functions","title":"MacroEconometricModels._suppress_warnings","text":"Suppress all log messages (warnings, info) within f(). Used in bootstrap loops.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._validate_data","page":"Functions","title":"MacroEconometricModels._validate_data","text":"Validate data contains no NaN or Inf values.\n\n\n\n\n\n","category":"function"},{"location":"api_functions/#MacroEconometricModels._validate_narrative_data-Tuple{Symbol, AbstractMatrix}","page":"Functions","title":"MacroEconometricModels._validate_narrative_data","text":"Validate that narrative method has required data matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels._validate_var_shock_indices-Tuple{String, String, Vector{String}, Vector{String}}","page":"Functions","title":"MacroEconometricModels._validate_var_shock_indices","text":"Resolve variable/shock names to indices, throwing on invalid names.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.companion_matrix-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.companion_matrix","text":"Construct companion matrix F for VAR(p) → VAR(1) representation.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.construct_var_matrices-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.construct_var_matrices","text":"Construct VAR design matrices: Yeff = X * B + U. Returns (Yeff, X) where X = [1, Y{t-1}, ..., Y{t-p}].\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.extract_ar_coefficients-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64, Int64}} where T","page":"Functions","title":"MacroEconometricModels.extract_ar_coefficients","text":"Extract AR coefficient matrices [A₁, ..., Aₚ] from stacked B matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.logdet_safe-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.logdet_safe","text":"Log determinant with eigenvalue fallback for numerical issues.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.robust_inv-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.robust_inv","text":"Compute inverse with fallback to pseudo-inverse for singular matrices.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.safe_cholesky-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.safe_cholesky","text":"Cholesky decomposition with automatic jitter for numerical stability.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.univariate_ar_variance-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:AbstractFloat","page":"Functions","title":"MacroEconometricModels.univariate_ar_variance","text":"AR(1) residual standard deviation for Minnesota prior scaling.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_dynamic_factor_inputs-NTuple{4, Int64}","page":"Functions","title":"MacroEconometricModels.validate_dynamic_factor_inputs","text":"Validate dynamic factor model inputs.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_factor_inputs-Tuple{Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.validate_factor_inputs","text":"Validate factor model inputs: 1 ≤ r ≤ min(T, N).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_horizon-Tuple{Int64}","page":"Functions","title":"MacroEconometricModels.validate_horizon","text":"Validate horizon: h ≥ min_val (default 1).\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_in_range-Tuple{Real, String, Real, Real}","page":"Functions","title":"MacroEconometricModels.validate_in_range","text":"Validate lo ≤ value ≤ hi.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_lags-Tuple{Int64}","page":"Functions","title":"MacroEconometricModels.validate_lags","text":"Validate lag order: p ≥ 1.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_nonnegative-Tuple{Real, String}","page":"Functions","title":"MacroEconometricModels.validate_nonnegative","text":"Validate value ≥ 0.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_option-Tuple{Symbol, String, Tuple}","page":"Functions","title":"MacroEconometricModels.validate_option","text":"Validate symbol is in valid_options.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_positive-Tuple{Real, String}","page":"Functions","title":"MacroEconometricModels.validate_positive","text":"Validate value > 0.\n\n\n\n\n\n","category":"method"},{"location":"api_functions/#MacroEconometricModels.validate_var_inputs-Tuple{Int64, Int64, Int64}","page":"Functions","title":"MacroEconometricModels.validate_var_inputs","text":"Validate VAR inputs: p ≥ 1, T > p + minobsfactor, n ≥ 1.\n\n\n\n\n\n","category":"method"},{"location":"data/#Data-Management","page":"Data Management","title":"Data Management","text":"Applied macroeconometric research begins with data. This module provides typed data containers that track metadata (frequency, variable names, panel structure, transformation codes), validate inputs, compute summary statistics, and guarantee clean data for estimation.\n\nFeature Function Description\nContainers TimeSeriesData, PanelData, CrossSectionData Typed wrappers with metadata\nValidation diagnose, fix Detect and repair NaN, Inf, constant columns\nTransforms apply_tcode, inverse_tcode FRED transformation codes 1–7\nFiltering apply_filter Apply HP/Hamilton/BN/BK/Boosted HP per-variable\nPanel xtset, group_data Stata-style panel setup and slicing\nSummary describe_data Per-variable descriptive statistics\nDispatch estimate_var(d, p) All estimators accept TimeSeriesData directly","category":"section"},{"location":"data/#Quick-Start","page":"Data Management","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD and select key macro variables\nfred = load_example(:fred_md)\nsub = fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]\n\n# Apply FRED transformation codes to achieve stationarity\nd = apply_tcode(sub)\n\n# Diagnose — apply_tcode introduces NaN from differencing\ndiag = diagnose(d)\n\n# Fix by dropping NaN rows\nd_clean = fix(d)\n\n# Summary statistics\ndescribe_data(d_clean)\n\n# Estimate directly from data container\nmodel = estimate_var(d_clean, 2)\n\n# Panel data — Penn World Table\npwt = load_example(:pwt)\npanel_summary(pwt)\nusa = group_data(pwt, \"USA\")   # extract single country\n\n","category":"section"},{"location":"data/#Data-Containers","page":"Data Management","title":"Data Containers","text":"All containers inherit from AbstractMacroData and carry metadata alongside the numeric data matrix.","category":"section"},{"location":"data/#TimeSeriesData","page":"Data Management","title":"TimeSeriesData","text":"TimeSeriesData{T} is the primary container for single-entity time series:\n\n# Load built-in dataset (recommended)\nfred = load_example(:fred_md)   # 804 obs × 126 vars (Monthly)\nsub = fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]\n\n# From matrix with metadata\nd = TimeSeriesData(randn(200, 3);\n    varnames=[\"GDP\", \"CPI\", \"FFR\"],\n    frequency=Quarterly,\n    tcode=[5, 5, 1],\n    time_index=collect(1959:2158))\n\n# From vector (univariate)\nd = TimeSeriesData(randn(200); varname=\"GDP\", frequency=Monthly)\n\n# From DataFrame (auto-selects numeric columns)\nusing DataFrames\ndf = DataFrame(gdp=randn(100), cpi=randn(100), date=1:100)\nd = TimeSeriesData(df; frequency=Quarterly)\n\nNon-float inputs are automatically converted to Float64. Missing values in DataFrames become NaN.","category":"section"},{"location":"data/#Frequency-Enum","page":"Data Management","title":"Frequency Enum","text":"@enum Frequency Daily Monthly Quarterly Yearly Mixed Other\n\nThe frequency field is informational metadata used in summary displays. It does not affect estimation.","category":"section"},{"location":"data/#PanelData","page":"Data Management","title":"PanelData","text":"PanelData{T} stores stacked panel (longitudinal) data with group and time identifiers. Constructed via xtset():\n\n# Load Penn World Table (balanced panel, 38 OECD countries × 74 years)\npwt = load_example(:pwt)\n\n# Or construct from a DataFrame via xtset\nusing DataFrames\ndf = DataFrame(\n    country = repeat([\"US\", \"UK\", \"JP\"], inner=50),\n    quarter = repeat(1:50, 3),\n    gdp = randn(150),\n    cpi = randn(150)\n)\npd = xtset(df, :country, :quarter; frequency=Quarterly)","category":"section"},{"location":"data/#CrossSectionData","page":"Data Management","title":"CrossSectionData","text":"CrossSectionData{T} stores cross-sectional observations (single time point):\n\nd = CrossSectionData(randn(500, 4);\n    varnames=[\"income\", \"education\", \"age\", \"hours\"])\n\n","category":"section"},{"location":"data/#Descriptions","page":"Data Management","title":"Descriptions","text":"Data containers carry optional metadata descriptions — one for the dataset itself, and per-variable descriptions accessible by name. These are empty by default and only populated if the user provides them.","category":"section"},{"location":"data/#Setting-at-Construction","page":"Data Management","title":"Setting at Construction","text":"# Built-in datasets already carry descriptions\nfred = load_example(:fred_md)\ndesc(fred)              # \"FRED-MD Monthly Database, January 2026 Vintage ...\"\nvardesc(fred, \"INDPRO\")  # \"IP Index\"\n\n# Or set manually at construction\nd = TimeSeriesData(randn(200, 3);\n    varnames=[\"GDP\", \"CPI\", \"FFR\"],\n    frequency=Quarterly,\n    desc=\"US macroeconomic quarterly data 1959-2024\",\n    vardesc=Dict(\n        \"GDP\" => \"Real Gross Domestic Product, seasonally adjusted annual rate\",\n        \"CPI\" => \"Consumer Price Index for All Urban Consumers\",\n        \"FFR\" => \"Effective Federal Funds Rate\"))","category":"section"},{"location":"data/#Accessing-Descriptions","page":"Data Management","title":"Accessing Descriptions","text":"fred = load_example(:fred_md)\ndesc(fred)              # \"FRED-MD Monthly Database, January 2026 Vintage ...\"\nvardesc(fred, \"INDPRO\")  # \"IP Index\"\nvardesc(fred)            # Dict with all variable descriptions","category":"section"},{"location":"data/#Setting-After-Construction","page":"Data Management","title":"Setting After Construction","text":"d = TimeSeriesData(randn(100, 2); varnames=[\"GDP\", \"CPI\"])\nset_desc!(d, \"Updated dataset description\")\nset_vardesc!(d, \"GDP\", \"Real GDP growth rate\")\nset_vardesc!(d, Dict(\"GDP\" => \"Real GDP\", \"CPI\" => \"Consumer prices\"))\n\nDescriptions propagate through subsetting (d[:, [\"GDP\"]]), transformations (apply_tcode), cleaning (fix), and panel extraction (group_data). Renaming variables (rename_vars!) automatically updates vardesc keys.\n\n","category":"section"},{"location":"data/#Accessors-and-Indexing","page":"Data Management","title":"Accessors and Indexing","text":"All data types support a common interface:\n\nfred = load_example(:fred_md)\n\n# Dimensions\nnobs(fred)      # 804\nnvars(fred)     # 126\nsize(fred)      # (804, 126)\n\n# Metadata\nvarnames(fred)     # [\"RPI\", \"W875RX1\", ..., \"CONSPI\"]\nfrequency(fred)    # Monthly\ntime_index(fred)   # 1:804\n\n# Column extraction\nip = fred[:, \"INDPRO\"]                          # Vector{Float64}\nsub = fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]  # new TimeSeriesData with 3 variables\n\n# Conversion\nto_matrix(sub)    # raw T x n matrix\nto_vector(fred[:, [\"INDPRO\"]])   # raw vector (univariate only)","category":"section"},{"location":"data/#Renaming-Variables","page":"Data Management","title":"Renaming Variables","text":"d = TimeSeriesData(randn(50, 2); varnames=[\"a\", \"b\"])\nrename_vars!(d, \"a\" => \"GDP\")\nrename_vars!(d, [\"output\", \"prices\"])","category":"section"},{"location":"data/#Time-Index","page":"Data Management","title":"Time Index","text":"d = TimeSeriesData(randn(50, 1))\nset_time_index!(d, collect(1970:2019))\ntime_index(d)  # [1970, 1971, ..., 2019]\n\n","category":"section"},{"location":"data/#Validation","page":"Data Management","title":"Validation","text":"","category":"section"},{"location":"data/#Diagnosing-Issues","page":"Data Management","title":"Diagnosing Issues","text":"diagnose() scans for NaN, Inf, constant columns, and very short series:\n\n# Transforming FRED-MD introduces NaN from differencing and log of non-positive values\nfred = load_example(:fred_md)\nd = apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]])\n\ndiag = diagnose(d)\ndiag.is_clean     # false — NaN rows from differencing\ndiag.n_nan        # number of NaN per variable\ndiag.is_constant  # [false, false, false]\ndiag.is_short     # false","category":"section"},{"location":"data/#Fixing-Issues","page":"Data Management","title":"Fixing Issues","text":"fix() returns a clean copy using one of three methods:\n\n# Drop rows with any NaN/Inf (default)\nd_clean = fix(d; method=:listwise)\n\n# Linear interpolation for interior NaN, forward-fill edges\nd_clean = fix(d; method=:interpolate)\n\n# Replace NaN with column mean of finite values\nd_clean = fix(d; method=:mean)\n\nAll methods replace Inf with NaN first, then apply the chosen method. Constant columns are dropped automatically with a warning.\n\nnote: Technical Note\nfix() always returns a new TimeSeriesData object. The original is never modified. After fixing, diagnose(d_clean).is_clean is guaranteed to be true (unless all columns are constant).","category":"section"},{"location":"data/#Model-Compatibility","page":"Data Management","title":"Model Compatibility","text":"validate_for_model() checks dimensionality requirements:\n\nd_multi = TimeSeriesData(randn(100, 3))\nd_uni = TimeSeriesData(randn(100))\n\nvalidate_for_model(d_multi, :var)    # OK\nvalidate_for_model(d_uni, :arima)    # OK\nvalidate_for_model(d_uni, :var)      # throws ArgumentError\nvalidate_for_model(d_multi, :garch)  # throws ArgumentError\n\nModel Category Requirement Model Types\nMultivariate n geq 2 :var, :vecm, :bvar, :factors, :dynamic_factors, :gdfm\nUnivariate n = 1 :arima, :ar, :ma, :arma, :arch, :garch, :egarch, :gjr_garch, :sv, :hp_filter, :hamilton_filter, :beveridge_nelson, :baxter_king, :boosted_hp, :adf, :kpss, :pp, :za, :ngperron\nFlexible any :lp, :lp_iv, :smooth_lp, :state_lp, :propensity_lp, :gmm\n\n","category":"section"},{"location":"data/#FRED-Transformation-Codes","page":"Data Management","title":"FRED Transformation Codes","text":"The FRED-MD database uses integer codes to specify how each series should be transformed to achieve stationarity. apply_tcode() implements all seven codes:\n\nCode Transformation Formula Observations Lost\n1 Level x_t 0\n2 First difference Delta x_t 1\n3 Second difference Delta^2 x_t 2\n4 Log ln x_t 0\n5 Log first difference Delta ln x_t 1\n6 Log second difference Delta^2 ln x_t 2\n7 Delta percent change Delta(x_t  x_t-1 - 1) 2\n\nCodes 4–7 require strictly positive data.","category":"section"},{"location":"data/#Applying-Transformations","page":"Data Management","title":"Applying Transformations","text":"# Univariate\ny = [100.0, 105.0, 110.0, 108.0, 115.0]\ngrowth = apply_tcode(y, 5)   # log first differences\n\n# Apply recommended FRED codes to data container\nfred = load_example(:fred_md)\nsub = fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]\nd = apply_tcode(sub)   # uses per-variable tcode from metadata\n\n# Or specify codes explicitly\nd2 = apply_tcode(sub, [5, 5, 1])   # log-diff IP and CPI, level FFR\n\n# Same code for all variables\nd3 = apply_tcode(sub, 5)\n\nWhen applying per-variable codes to a TimeSeriesData, rows are trimmed consistently to the shortest transformed series, aligning to the end of the sample.","category":"section"},{"location":"data/#Inverse-Transformations","page":"Data Management","title":"Inverse Transformations","text":"inverse_tcode() undoes a transformation given initial values:\n\ny = [100.0, 105.0, 110.0, 108.0]\nyd = apply_tcode(y, 5)\n\n# Recover original levels\nrecovered = inverse_tcode(yd, 5; x_prev=[y[1]])\n# recovered ≈ [105.0, 110.0, 108.0]\n\nThe x_prev argument provides the initial values needed to anchor the reconstruction:\n\nCode Required x_prev\n1, 4 None\n2, 5 1 value (last pre-sample level)\n3, 6, 7 2 values (last two pre-sample levels)\n\nnote: Technical Note\nRound-trip accuracy (inverse_tcode(apply_tcode(y, c), c; x_prev=...)) is exact to machine precision for all codes.\n\n","category":"section"},{"location":"data/#Panel-Data","page":"Data Management","title":"Panel Data","text":"","category":"section"},{"location":"data/#Stata-style-xtset","page":"Data Management","title":"Stata-style xtset","text":"xtset() converts a DataFrame into a PanelData container, analogous to Stata's xtset command:\n\n# The preferred way to get panel data is load_example(:pwt)\n# For custom DataFrames, use xtset:\nusing DataFrames\n\ndf = DataFrame(\n    firm = repeat(1:50, inner=20),\n    year = repeat(2001:2020, 50),\n    investment = randn(1000),\n    output = randn(1000)\n)\n\npd = xtset(df, :firm, :year; frequency=Yearly)\n\nThe function:\n\nExtracts all numeric columns (excluding group and time columns)\nSorts by (group, time)\nValidates no duplicate (group, time) pairs\nDetects balanced vs unbalanced panels","category":"section"},{"location":"data/#Panel-Operations","page":"Data Management","title":"Panel Operations","text":"pwt = load_example(:pwt)\n\n# Structure summary\nisbalanced(pwt)       # true\nngroups(pwt)          # 38\ngroups(pwt)           # [\"AUS\", \"AUT\", ..., \"USA\"]\npanel_summary(pwt)    # printed summary table\n\n# Extract single entity as TimeSeriesData\nusa = group_data(pwt, \"USA\")       # by name\nusa = group_data(pwt, 38)          # by index\n\n","category":"section"},{"location":"data/#Summary-Statistics","page":"Data Management","title":"Summary Statistics","text":"describe_data() computes per-variable descriptive statistics displayed via PrettyTables:\n\nfred = load_example(:fred_md)\nd = fix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\ns = describe_data(d)\n\nThe returned DataSummary object contains fields: varnames, n, mean, std, min, p25, median, p75, max, skewness, kurtosis.\n\nFor PanelData, describe_data() additionally prints panel dimensions.\n\n","category":"section"},{"location":"data/#Estimation-Dispatch","page":"Data Management","title":"Estimation Dispatch","text":"All estimation functions accept TimeSeriesData directly via thin dispatch wrappers. This avoids manual conversion:\n\nfred = load_example(:fred_md)\nd = fix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\n\n# Multivariate — automatically calls to_matrix(d)\nmodel = estimate_var(d, 2)\npost = estimate_bvar(d, 2)\nfm = estimate_factors(d, 2)\nlp = estimate_lp(d, 1, 20)\n\n# Univariate — automatically calls to_vector(d) (requires n_vars == 1)\nd_uni = d[:, [\"INDPRO\"]]  # select single variable\nar = estimate_ar(d_uni, 2)\nadf = adf_test(d_uni)\n\nExplicit conversion is also available:\n\nto_matrix(d)             # Matrix{Float64}\nto_vector(d)             # Vector{Float64} (n_vars == 1 only)\nto_vector(d, \"INDPRO\")   # single column by name\nto_vector(d, 2)          # single column by index\n\n","category":"section"},{"location":"data/#Example-Datasets","page":"Data Management","title":"Example Datasets","text":"Three built-in datasets are included, stored as TOML files in the data/ directory:\n\nDataset Function Type Variables Observations Frequency\nFRED-MD load_example(:fred_md) TimeSeriesData 126 804 months (1959–2025) Monthly\nFRED-QD load_example(:fred_qd) TimeSeriesData 245 268 quarters (1959–2025) Quarterly\nPWT load_example(:pwt) PanelData 42 38 countries × 74 years (1950–2023) Yearly\n\nFRED-MD and FRED-QD are January 2026 vintage and include per-variable descriptions and recommended transformation codes from McCracken and Ng. PWT 10.01 covers 38 OECD countries with national accounts, productivity, and price level data from Feenstra, Inklaar, and Timmer.","category":"section"},{"location":"data/#FRED-Databases","page":"Data Management","title":"FRED Databases","text":"# Load FRED-MD\nmd = load_example(:fred_md)\nmd                              # 804 obs × 126 vars (Monthly)\ndesc(md)                        # \"FRED-MD Monthly Database, January 2026 Vintage ...\"\nvardesc(md, \"INDPRO\")           # \"IP Index\"\nrefs(md)                        # McCracken & Ng (2016)\n\n# Apply recommended FRED transformations to achieve stationarity\nmd_stationary = apply_tcode(md, md.tcode)\n\n# Estimate a VAR on a subset\nsub = md_stationary[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\", \"FEDFUNDS\"]]\nmodel = estimate_var(sub, 4)\n\n# Load FRED-QD\nqd = load_example(:fred_qd)\ndesc(qd)                        # \"FRED-QD Quarterly Database, January 2026 Vintage ...\"\nvardesc(qd, \"GDPC1\")            # \"Real Gross Domestic Product, 3 Decimal ...\"\nrefs(qd)                        # McCracken & Ng (2020)","category":"section"},{"location":"data/#Penn-World-Table","page":"Data Management","title":"Penn World Table","text":"The Penn World Table (PWT) 10.01 provides a balanced panel of 38 OECD countries over 1950–2023. It loads as PanelData, giving access to panel-specific functions like group_data, groups, and panel_summary.\n\n# Load PWT\npwt = load_example(:pwt)\nnobs(pwt)                       # 2812 (38 × 74)\nnvars(pwt)                      # 42\nngroups(pwt)                    # 38\ngroups(pwt)                     # [\"AUS\", \"AUT\", ..., \"USA\"]\nisbalanced(pwt)                 # true\nvardesc(pwt, \"rgdpna\")          # \"Real GDP at constant 2021 national prices ...\"\nrefs(pwt)                       # Feenstra, Inklaar & Timmer (2015)\n\n# Extract a single country as TimeSeriesData\nusa = group_data(pwt, \"USA\")\nnobs(usa)                       # 74 (years 1950–2023)\n\n# Run a VAR on US real GDP, consumption, and investment\nrgdpna = usa[:, \"rgdpna\"]\nrconna = usa[:, \"rconna\"]\n# Panel summary\npanel_summary(pwt)","category":"section"},{"location":"data/#References","page":"Data Management","title":"References","text":"Each loaded dataset carries bibliographic references accessible via refs(), supporting :text, :latex, :bibtex, and :html output formats:\n\nrefs(md; format=:bibtex)   # BibTeX entry for McCracken & Ng (2016)\nrefs(:fred_md)             # same via symbol dispatch\nrefs(:pwt)                 # Feenstra, Inklaar & Timmer (2015)\n\n","category":"section"},{"location":"data/#Filtering","page":"Data Management","title":"Filtering","text":"apply_filter() applies time series filters (HP, Hamilton, BN, BK, Boosted HP) to variables in a TimeSeriesData or PanelData, extracting trend or cycle components. When filters produce different-length outputs (e.g., Hamilton drops initial observations), the result is trimmed to the common valid range.","category":"section"},{"location":"data/#Basic-Usage","page":"Data Management","title":"Basic Usage","text":"# Log levels from FRED-MD (I(1) series, suitable for trend-cycle decomposition)\nfred = load_example(:fred_md)\nd = TimeSeriesData(\n    log.(to_matrix(fred[:, [\"INDPRO\", \"PAYEMS\", \"HOUST\"]]));\n    varnames=[\"INDPRO\", \"PAYEMS\", \"HOUST\"], frequency=Monthly)\n# Drop any NaN from log of non-positive values\nd = fix(d)\n\n# HP cycle for all variables (monthly lambda)\nd_hp = apply_filter(d, :hp; component=:cycle, lambda=129600.0)\n\n# HP trend for all variables\nd_trend = apply_filter(d, :hp; component=:trend, lambda=129600.0)\n\n# Hamilton filter (output is shorter — drops initial observations)\nd_ham = apply_filter(d, :hamilton; component=:cycle, h=24, p=12)\n\nAvailable filter symbols: :hp, :hamilton, :bn, :bk, :boosted_hp.","category":"section"},{"location":"data/#Per-Variable-Specifications","page":"Data Management","title":"Per-Variable Specifications","text":"# Different filters per variable (nothing = pass-through)\nd2 = apply_filter(d, [:hp, :hamilton, nothing]; component=:cycle)\n\n# Per-variable component overrides via tuples\nd3 = apply_filter(d, [(:hp, :trend), (:hamilton, :cycle), nothing])","category":"section"},{"location":"data/#Selective-Filtering","page":"Data Management","title":"Selective Filtering","text":"# Filter only selected variables (others pass through unchanged)\nd_sel = apply_filter(d, :hp; vars=[\"INDPRO\", \"PAYEMS\"], component=:cycle)\nd_sel = apply_filter(d, :hp; vars=[1, 2], component=:cycle)  # by index","category":"section"},{"location":"data/#Pre-Computed-Results","page":"Data Management","title":"Pre-Computed Results","text":"# Use a pre-computed filter result\nr = hp_filter(d[:, \"INDPRO\"]; lambda=129600.0)\nd2 = apply_filter(d, [r, :hp, nothing]; component=:cycle)","category":"section"},{"location":"data/#Forwarding-Filter-Parameters","page":"Data Management","title":"Forwarding Filter Parameters","text":"Additional keyword arguments are forwarded to the filter functions:\n\n# Custom HP lambda\nd_smooth = apply_filter(d, :hp; component=:cycle, lambda=100.0)\n\n# Custom Hamilton horizon and lags\nd_ham = apply_filter(d, :hamilton; component=:cycle, h=24, p=12)","category":"section"},{"location":"data/#Panel-Data-2","page":"Data Management","title":"Panel Data","text":"apply_filter applies filters group-by-group to PanelData, reassembling the results:\n\n# Penn World Table — real GDP and consumption for 38 OECD countries\npwt = load_example(:pwt)\n\n# HP cycle for all variables, applied per-group\npd_hp = apply_filter(pwt[:, [\"rgdpna\", \"rconna\"]], :hp; component=:cycle)\n\n# Filter only rgdpna, pass through rconna\npd_sel = apply_filter(pwt[:, [\"rgdpna\", \"rconna\"]], :hp; vars=[\"rgdpna\"], component=:cycle)\n\nnote: Technical Note\nFilters that produce shorter output (Hamilton, Baxter-King) trim each group independently. If groups have different lengths, the resulting panel may become unbalanced.\n\n","category":"section"},{"location":"data/#Complete-Example","page":"Data Management","title":"Complete Example","text":"using MacroEconometricModels\n\n# === Step 1: Load FRED-MD and select variables ===\nfred = load_example(:fred_md)\nsub = fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]\n\n# === Step 2: Apply FRED transformation codes ===\nd = apply_tcode(sub)\n\n# === Step 3: Diagnose — differencing introduces NaN ===\ndiag = diagnose(d)\nprintln(\"Clean: \", diag.is_clean)   # false\n\n# === Step 4: Fix by dropping NaN rows ===\nd_clean = fix(d)\nprintln(\"Clean: \", diagnose(d_clean).is_clean)   # true\n\n# === Step 5: Summary statistics ===\ndescribe_data(d_clean)\n\n# === Step 6: Validate for VAR ===\nvalidate_for_model(d_clean, :var)   # OK — multivariate\n\n# === Step 7: Estimate VAR directly from container ===\nmodel = estimate_var(d_clean, 2)\n\n# === Step 8: Structural analysis ===\nirfs = irf(model, 20; method=:cholesky)\n\n# === Step 9: Panel workflow with Penn World Table ===\npwt = load_example(:pwt)\npanel_summary(pwt)\n\n# Extract and estimate per country\nfor country in [\"USA\", \"GBR\", \"JPN\"]\n    gd = group_data(pwt, country)\n    y = filter(isfinite, log.(gd[:, \"rgdpna\"]))\n    hp = hp_filter(y)\n    println(\"$country: trend length = \", length(trend(hp)))\nend","category":"section"},{"location":"data/#See-Also","page":"Data Management","title":"See Also","text":"Time Series Filters – HP, Hamilton, BN, BK, and boosted HP filters used by apply_filter\nExamples – Complete worked examples including FRED-MD data pipeline\nAPI Reference – Complete function signatures","category":"section"},{"location":"data/#References-2","page":"Data Management","title":"References","text":"McCracken, Michael W., and Serena Ng. 2016. \"FRED-MD: A Monthly Database for Macroeconomic Research.\" Journal of Business & Economic Statistics 34 (4): 574–589. https://doi.org/10.1080/07350015.2015.1086655\nMcCracken, Michael W., and Serena Ng. 2020. \"FRED-QD: A Quarterly Database for Macroeconomic Research.\" Federal Reserve Bank of St. Louis Working Paper 2020-005. https://doi.org/10.20955/wp.2020.005","category":"section"},{"location":"nowcast/#Nowcasting","page":"Nowcasting","title":"Nowcasting","text":"This page documents the nowcasting module in MacroEconometricModels.jl, implementing three state-of-the-art approaches for real-time macroeconomic prediction with mixed-frequency data and ragged edges: Dynamic Factor Model (DFM), Large Bayesian VAR (BVAR), and Bridge Equations.","category":"section"},{"location":"nowcast/#Quick-Start","page":"Nowcasting","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD monthly indicators and prepare mixed-frequency panel\nfred = load_example(:fred_md)\nnc_md = fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\", \"M2SL\", \"FEDFUNDS\"]]\nY = to_matrix(apply_tcode(nc_md))\nY = Y[all.(isfinite, eachrow(Y)), :]\nY = Y[end-99:end, :]   # last 100 observations\n\n# Convert last column (FEDFUNDS) to \"quarterly\" by masking non-quarter months\nnM, nQ = 4, 1\nfor t in 1:size(Y, 1)\n    if mod(t, 3) != 0\n        Y[t, end] = NaN\n    end\nend\nY[end, end] = NaN       # simulate missing latest observation (ragged edge)\n\n# DFM nowcasting (Bańbura & Modugno 2014)\ndfm = nowcast_dfm(Y, nM, nQ; r=2, p=1, idio=:ar1)\n\n# Large BVAR nowcasting (Cimadomo et al. 2022)\nbvar = nowcast_bvar(Y, nM, nQ; lags=5)\n\n# Bridge equation nowcasting (Bańbura et al. 2023)\nbridge = nowcast_bridge(Y, nM, nQ; lagM=1, lagQ=1)\n\n# Extract nowcast and forecast\nresult = nowcast(dfm)\nresult.nowcast    # current-quarter estimate\nresult.forecast   # next-quarter forecast\n\n","category":"section"},{"location":"nowcast/#The-Nowcasting-Problem","page":"Nowcasting","title":"The Nowcasting Problem","text":"Central banks and forecasters face a fundamental timing problem: key macroeconomic aggregates like GDP are released with a significant delay and at low frequency (quarterly), while a rich set of monthly and weekly indicators is available in real time. Nowcasting produces current-quarter estimates by exploiting the information content of timely high-frequency releases.\n\nThe key challenges are:\n\nMixed frequencies –- monthly indicators and quarterly targets coexist in the same model\nRagged edges –- not all series are updated simultaneously; the most recent months have missing observations for slower-release variables\nLarge cross-sections –- dozens to hundreds of indicators provide complementary information\n\nunderbraceY_t_texttarget (quarterly) = fbig(underbraceX_1t ldots X_Nt_textmonthly indicatorsbig) + varepsilon_t\n\nwhere the challenge is that Y_t and some X_jt are unobserved at the forecast origin.\n\nnote: Data Layout Convention\nAll nowcasting functions expect a T times N matrix where the first nM columns are monthly variables and the last nQ columns are quarterly variables. Quarterly observations appear every 3rd row (months 3, 6, 9, 12, ...) with NaN for non-quarter-end months.\n\n","category":"section"},{"location":"nowcast/#Method-1:-Dynamic-Factor-Model-(DFM)","page":"Nowcasting","title":"Method 1: Dynamic Factor Model (DFM)","text":"The DFM approach extracts a small number of latent factors from a large cross-section and uses them to nowcast the target variable. This is the workhorse model used by the ECB, Federal Reserve Bank of New York, and many other institutions.","category":"section"},{"location":"nowcast/#Model-Specification","page":"Nowcasting","title":"Model Specification","text":"The observation equation links observed data to latent factors:\n\nx_it = lambda_i f_t + e_it\n\nThe factor dynamics follow a VAR(p):\n\nf_t = A_1 f_t-1 + cdots + A_p f_t-p + u_t quad u_t sim N(0 Q)\n\nwhere f_t in mathbbR^r are the latent factors, lambda_i are factor loadings, and e_it are idiosyncratic components.\n\nQuarterly temporal aggregation. Quarterly variables are linked to the monthly factors via Mariano-Murasawa weights 1 2 3 2 1, representing the flow nature of quarterly data as a weighted average of monthly latent values.\n\nEstimation. The EM algorithm alternates between:\n\nE-step: Kalman smoother with NaN-aware observation equations extracts factors\nM-step: Update state-space parameters (A, C, Q, R) from sufficient statistics","category":"section"},{"location":"nowcast/#Usage","page":"Nowcasting","title":"Usage","text":"dfm = nowcast_dfm(Y, nM, nQ;\n    r = 2,             # number of factors\n    p = 1,             # VAR lags in factor dynamics\n    idio = :ar1,       # idiosyncratic dynamics (:ar1 or :iid)\n    blocks = nothing,  # block structure (N × n_blocks matrix)\n    max_iter = 100,    # maximum EM iterations\n    thresh = 1e-4      # convergence threshold\n)","category":"section"},{"location":"nowcast/#Return-Values","page":"Nowcasting","title":"Return Values","text":"Field Type Description\nX_sm Matrix{T} Smoothed data (NaN filled)\nF Matrix{T} Smoothed factors (T × state_dim)\nC Matrix{T} Observation loadings\nA Matrix{T} State transition matrix\nQ Matrix{T} State innovation covariance\nR Matrix{T} Observation noise covariance (diagonal)\nloglik T Log-likelihood at convergence\nn_iter Int EM iterations used\nr Int Number of factors\np Int VAR lags\n\nnote: Technical Note: Block Structure\nThe blocks argument accepts an N times B binary matrix, where entry (ib) = 1 indicates variable i loads on block b. When blocks=nothing (default), all variables load on a single global factor. Block structures are useful when variables naturally group (e.g., real activity, prices, financial) and you want block-specific factors plus a global factor.\n\n","category":"section"},{"location":"nowcast/#Method-2:-Large-Bayesian-VAR","page":"Nowcasting","title":"Method 2: Large Bayesian VAR","text":"The BVAR approach estimates a large VAR directly on the mixed-frequency data, using informative priors to handle the curse of dimensionality. This follows Giannone, Lenza, and Primiceri (2015) with extensions for mixed-frequency data.","category":"section"},{"location":"nowcast/#Model-Specification-2","page":"Nowcasting","title":"Model Specification","text":"y_t = c + B_1 y_t-1 + cdots + B_p y_t-p + u_t quad u_t sim N(0 Sigma)\n\nPrior structure. The Normal-Inverse-Wishart prior implements four types of shrinkage via dummy observations:\n\nTightness (lambda): lag-decaying overall shrinkage\nCross-variable (theta): shrinks cross-variable coefficients relative to own-lag\nSum-of-coefficients (mu): unit root prior (random walk for each variable)\nCo-persistence (alpha): common stochastic trend prior\n\nHyperparameters are optimized via marginal log-likelihood maximization using Nelder-Mead.","category":"section"},{"location":"nowcast/#Usage-2","page":"Nowcasting","title":"Usage","text":"bvar = nowcast_bvar(Y, nM, nQ;\n    lags = 5,         # number of VAR lags\n    thresh = 1e-6,    # optimization convergence threshold\n    max_iter = 200,   # max optimization iterations\n    lambda0 = 0.2,    # initial overall shrinkage\n    theta0 = 1.0,     # initial cross-variable shrinkage\n    miu0 = 1.0,       # initial sum-of-coefficients weight\n    alpha0 = 2.0      # initial co-persistence weight\n)","category":"section"},{"location":"nowcast/#Return-Values-2","page":"Nowcasting","title":"Return Values","text":"Field Type Description\nX_sm Matrix{T} Smoothed data (NaN filled)\nbeta Matrix{T} Posterior mode VAR coefficients\nsigma Matrix{T} Posterior mode error covariance\nlambda T Optimized overall shrinkage\ntheta T Optimized cross-variable shrinkage\nmiu T Optimized sum-of-coefficients weight\nalpha T Optimized co-persistence weight\nloglik T Marginal log-likelihood\n\n","category":"section"},{"location":"nowcast/#Method-3:-Bridge-Equations","page":"Nowcasting","title":"Method 3: Bridge Equations","text":"Bridge equations provide a simple, transparent approach by regressing the quarterly target on aggregated monthly indicators using OLS. Multiple equations (one per pair of monthly indicators) are combined via median to produce a robust nowcast.","category":"section"},{"location":"nowcast/#Model-Specification-3","page":"Nowcasting","title":"Model Specification","text":"For each pair (m_1 m_2) of monthly indicators:\n\nY_t^Q = beta_0 + sum_l=1^L_M beta_m_1l X_m_1t-l^Q + sum_l=1^L_M beta_m_2l X_m_2t-l^Q + sum_l=1^L_Q gamma_l X_t^Q + sum_l=1^L_Y delta_l Y_t-l^Q + varepsilon_t\n\nwhere X^Q denotes monthly data aggregated to quarterly frequency (3-month moving average). The combination across binomn_M2 + n_M equations uses the median, which is robust to individual equation failures.","category":"section"},{"location":"nowcast/#Usage-3","page":"Nowcasting","title":"Usage","text":"bridge = nowcast_bridge(Y, nM, nQ;\n    lagM = 1,    # monthly indicator lags (after quarterly aggregation)\n    lagQ = 1,    # quarterly indicator lags\n    lagY = 1     # autoregressive lags for target\n)","category":"section"},{"location":"nowcast/#Return-Values-3","page":"Nowcasting","title":"Return Values","text":"Field Type Description\nX_sm Matrix{T} Smoothed data (NaN filled by interpolation)\nY_nowcast Vector{T} Combined nowcast (per quarter, median)\nY_individual Matrix{T} Individual equation nowcasts\nn_equations Int Number of bridge equations\ncoefficients Vector{Vector{T}} OLS coefficients per equation\n\n","category":"section"},{"location":"nowcast/#Nowcast-and-Forecast-Extraction","page":"Nowcasting","title":"Nowcast and Forecast Extraction","text":"","category":"section"},{"location":"nowcast/#Current-Quarter-Nowcast","page":"Nowcasting","title":"Current-Quarter Nowcast","text":"result = nowcast(model)        # works with any AbstractNowcastModel\nresult.nowcast                 # current-quarter value\nresult.forecast                # next-quarter forecast\nresult.method                  # :dfm, :bvar, or :bridge\n\nThe nowcast function extracts the current-quarter estimate from the smoothed data and produces a one-quarter-ahead forecast:\n\nDFM: 3-step state evolution (one quarter = 3 months)\nBVAR: one-step VAR forecast from last smoothed values\nBridge: median nowcast from individual equations","category":"section"},{"location":"nowcast/#Multi-Step-Forecast","page":"Nowcasting","title":"Multi-Step Forecast","text":"# DFM forecast (state-space projection)\nfc = forecast(dfm, 6)                # 6-step ahead\nfc = forecast(dfm, 6; target_var=10) # specific target variable\n\n# BVAR forecast (VAR iteration)\nfc = forecast(bvar, 6)\n\n","category":"section"},{"location":"nowcast/#News-Decomposition","page":"Nowcasting","title":"News Decomposition","text":"When new data releases arrive, the nowcast changes. The news decomposition (Banbura and Modugno 2014) attributes this revision to individual data releases, answering: Which data releases drove the revision?\n\nhaty^textnew - haty^textold = underbracesum_j in mathcalJ w_j cdot (x_j^textactual - x_j^textforecast)_textnews + underbraceDelta_textrevision_textdata revisions + underbraceDelta_textre-estimation_textparameter updates","category":"section"},{"location":"nowcast/#Usage-4","page":"Nowcasting","title":"Usage","text":"# Two data vintages (X_old has more NaN than X_new)\nX_old = copy(Y)\nX_new = copy(Y)\nX_old[end, 1:3] .= NaN  # these releases were not available before\n\n# Compute news decomposition\nnews = nowcast_news(X_new, X_old, dfm, size(Y, 1);\n    target_var = size(Y, 2),   # target variable (default: last column)\n    groups = nothing           # optional group assignment for aggregation\n)\n\nnews.old_nowcast              # previous nowcast\nnews.new_nowcast              # updated nowcast\nnews.impact_news              # per-release impact vector\nnews.impact_reestimation      # residual re-estimation effect\nnews.group_impacts            # aggregated by variable group\nnews.variable_names           # release identifiers\n\nnote: Interpretation\nA positive impact_news[j] means the actual value of release j was higher than expected (given the old information set), contributing to an upward revision of the nowcast. The sum of all news impacts plus the re-estimation residual equals the total revision.\n\n","category":"section"},{"location":"nowcast/#Balancing-Panels","page":"Nowcasting","title":"Balancing Panels","text":"The balance_panel utility fills missing values in TimeSeriesData or PanelData using DFM imputation:\n\n# Fill NaN in time series data\nts = TimeSeriesData(Y; varnames=[\"x1\",\"x2\",\"x3\"], frequency=Monthly)\nts_balanced = balance_panel(ts; r=2, p=1, method=:dfm)\n\n# Fill NaN in panel data\npd_balanced = balance_panel(pd; r=2)\n\nObserved values are preserved; only NaN entries are replaced with DFM-smoothed estimates.\n\n","category":"section"},{"location":"nowcast/#TimeSeriesData-Dispatch","page":"Nowcasting","title":"TimeSeriesData Dispatch","text":"All nowcasting functions accept TimeSeriesData directly:\n\nts = TimeSeriesData(Y; varnames=varnames, frequency=Monthly)\ndfm = nowcast_dfm(ts, nM, nQ; r=2)\nbvar = nowcast_bvar(ts, nM, nQ; lags=5)\nbridge = nowcast_bridge(ts, nM, nQ)\n\n","category":"section"},{"location":"nowcast/#StatsAPI-Interface","page":"Nowcasting","title":"StatsAPI Interface","text":"Function DFM BVAR Bridge\nloglikelihood(m) Log-likelihood at convergence Marginal log-likelihood –-\npredict(m) Smoothed data X_sm Smoothed data X_sm Smoothed data X_sm\nnobs(m) Number of time periods Number of time periods Number of time periods\n\n","category":"section"},{"location":"nowcast/#Choosing-a-Method","page":"Nowcasting","title":"Choosing a Method","text":"Criterion DFM BVAR Bridge\nCross-section size Large (50–200 variables) Medium-large (10–50) Small-medium (5–20)\nInterpretability Factors are latent Direct variable coefficients Simple OLS regressions\nNews decomposition Native support –- –-\nComputational cost Moderate (EM iterations) Moderate (hyperparameter optimization) Fast (closed-form OLS)\nRagged edge handling Kalman smoother Kalman smoother Interpolation\nMixed frequency Mariano-Murasawa temporal aggregation Kalman smoother Quarterly aggregation\nBest for Large mixed-frequency panels Medium panels with strong priors Quick baseline, transparent models\n\n","category":"section"},{"location":"nowcast/#Complete-Example","page":"Nowcasting","title":"Complete Example","text":"using MacroEconometricModels\n\n# === Step 1: Prepare FRED-MD mixed-frequency panel ===\nfred = load_example(:fred_md)\nnc_md = fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\", \"M2SL\", \"FEDFUNDS\"]]\nY = to_matrix(apply_tcode(nc_md))\nY = Y[all.(isfinite, eachrow(Y)), :]\nY = Y[end-99:end, :]   # last 100 observations\nT_obs = size(Y, 1)\n\n# Convert last column (FEDFUNDS) to \"quarterly\" target\nnM, nQ = 4, 1\nN = nM + nQ\nfor t in 1:T_obs\n    if mod(t, 3) != 0\n        Y[t, end] = NaN\n    end\nend\nY[end, end] = NaN       # simulate missing latest observation (ragged edge)\n\nprintln(\"Data: T=$T_obs, nM=$nM monthly, nQ=$nQ quarterly\")\nprintln(\"Missing: \", sum(isnan.(Y)), \" / \", length(Y), \" entries\")\n\n# === Step 2: Estimate all three models ===\ndfm = nowcast_dfm(Y, nM, nQ; r=2, p=1, idio=:ar1, max_iter=100)\nbvar = nowcast_bvar(Y, nM, nQ; lags=5)\nbridge = nowcast_bridge(Y, nM, nQ; lagM=1, lagQ=1)\n\n# === Step 3: Compare nowcasts ===\nr_dfm = nowcast(dfm)\nr_bvar = nowcast(bvar)\nr_bridge = nowcast(bridge)\n\nprintln(\"\\nNowcast comparison (target: FEDFUNDS quarterly):\")\nprintln(\"  DFM:    nowcast = \", round(r_dfm.nowcast, digits=3),\n        \", forecast = \", round(r_dfm.forecast, digits=3))\nprintln(\"  BVAR:   nowcast = \", round(r_bvar.nowcast, digits=3),\n        \", forecast = \", round(r_bvar.forecast, digits=3))\nprintln(\"  Bridge: nowcast = \", round(r_bridge.nowcast, digits=3),\n        \", forecast = \", round(r_bridge.forecast, digits=3))\n\n# === Step 4: DFM forecasting ===\nfc = forecast(dfm, 6; target_var=N)\nprintln(\"\\nDFM 6-step forecast for target variable:\")\nfor h in 1:6\n    println(\"  h=$h: \", round(fc[h], digits=3))\nend\n\n# === Step 5: News decomposition ===\n# Simulate that INDPRO, UNRATE, CPIAUCSL were just released for the latest month\nX_old = copy(Y)\nX_new = copy(Y)\nX_old[end, 1:3] .= NaN\n\nnews = nowcast_news(X_new, X_old, dfm, T_obs; target_var=N)\nprintln(\"\\nNews decomposition:\")\nprintln(\"  Old nowcast: \", round(news.old_nowcast, digits=3))\nprintln(\"  New nowcast: \", round(news.new_nowcast, digits=3))\nprintln(\"  Total revision: \", round(news.new_nowcast - news.old_nowcast, digits=3))\nprintln(\"  Top release impacts:\")\nsorted_idx = sortperm(abs.(news.impact_news), rev=true)\nfor k in 1:min(3, length(sorted_idx))\n    j = sorted_idx[k]\n    println(\"    \", news.variable_names[j], \": \", round(news.impact_news[j], digits=4))\nend\n\nInterpretation. The DFM extracts common factors from the 4 monthly FRED-MD indicators (INDPRO, UNRATE, CPIAUCSL, M2SL) and the quarterly FEDFUNDS target, filling the ragged edge via the Kalman smoother. The BVAR estimates all cross-variable dynamics directly with informative priors. Bridge equations provide a simple, transparent baseline. The news decomposition shows which data releases drove the most recent nowcast revision –- for example, a surprise in industrial production or unemployment may revise the quarterly target estimate.\n\n","category":"section"},{"location":"nowcast/#References","page":"Nowcasting","title":"References","text":"Banbura, Marta, and Michele Modugno. 2014. \"Maximum Likelihood Estimation of Factor Models on Datasets with Arbitrary Pattern of Missing Data.\" Journal of Applied Econometrics 29 (1): 133–160. https://doi.org/10.1002/jae.2306\nCimadomo, Jacopo, Domenico Giannone, Michele Lenza, Francesca Monti, and Andrej Sokol. 2022. \"Nowcasting with Large Bayesian Vector Autoregressions.\" ECB Working Paper No. 2696.\nBanbura, Marta, Irina Belousova, Katalin Bodnar, and Mate Barnabas Toth. 2023. \"Nowcasting Employment in the Euro Area.\" ECB Working Paper No. 2815.\nDelle Chiaie, Simona, Florian Heider, Soren Hoppe, Alexander Melemenidis, Mate Barnabas Toth, and Stefan Zeugner. 2022. \"Real-time Data and Advance Estimates in Nowcasting.\" ECB Research Bulletin No. 75.\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nMariano, Roberto S., and Yasutomo Murasawa. 2003. \"A New Coincident Index of Business Cycles Based on Monthly and Quarterly Series.\" Journal of Applied Econometrics 18 (4): 427–443. https://doi.org/10.1002/jae.695","category":"section"},{"location":"manual/#Manual","page":"VAR","title":"Manual","text":"This manual provides a comprehensive theoretical background for the macroeconometric methods implemented in MacroEconometricModels.jl, including precise mathematical formulations and references to the literature.","category":"section"},{"location":"manual/#Quick-Start","page":"VAR","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD: industrial production, CPI, federal funds rate\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nmodel = estimate_var(Y, 4)                                 # Estimate VAR(4) via OLS\nsel = select_lag_order(Y, 13)                              # AIC/BIC/HQIC lag selection\nirfs = irf(model, 20; method=:cholesky)                    # Cholesky-identified IRFs\ndecomp = fevd(model, 20)                                   # Forecast error variance decomposition\nid = identify_sign(model; check_func=f, n_draws=1000)      # Sign restriction identification\nhd = historical_decomposition(model, size(model.U, 1))     # Historical decomposition\n\n","category":"section"},{"location":"manual/#Vector-Autoregression-(VAR)","page":"VAR","title":"Vector Autoregression (VAR)","text":"","category":"section"},{"location":"manual/#The-Reduced-Form-VAR-Model","page":"VAR","title":"The Reduced-Form VAR Model","text":"A VAR(p) model for an n-dimensional vector of endogenous variables y_t is defined as:\n\ny_t = c + A_1 y_t-1 + A_2 y_t-2 + cdots + A_p y_t-p + u_t\n\nwhere:\n\ny_t is an n times 1 vector of endogenous variables at time t\nc is an n times 1 vector of intercepts\nA_i are n times n coefficient matrices for lag i = 1 ldots p\nu_t is an n times 1 vector of reduced-form innovations with Eu_t = 0 and Eu_t u_t = Sigma\n\nReference: Sims (1980), Lütkepohl (2005, Chapter 2)","category":"section"},{"location":"manual/#Compact-Matrix-Representation","page":"VAR","title":"Compact Matrix Representation","text":"For estimation, we stack observations into matrices. Let T denote the effective sample size after accounting for lags. Define:\n\nY = beginbmatrix y_p+1  y_p+2  vdots  y_T endbmatrix_(T-p) times n quad\nX = beginbmatrix 1  y_p  y_p-1  cdots  y_1 \n1  y_p+1  y_p  cdots  y_2 \nvdots  vdots  vdots  ddots  vdots \n1  y_T-1  y_T-2  cdots  y_T-p endbmatrix_(T-p) times (1+np)\n\nThe VAR can be written in matrix form as:\n\nY = X B + U\n\nwhere B = c A_1 A_2 ldots A_p is a (1+np) times n coefficient matrix.","category":"section"},{"location":"manual/#OLS-Estimation","page":"VAR","title":"OLS Estimation","text":"The OLS estimator is given by:\n\nhatB = (XX)^-1 XY\n\nThe residual covariance matrix is estimated as:\n\nhatSigma = frac1T-p-k hatUhatU\n\nwhere hatU = Y - XhatB and k = 1 + np is the number of regressors per equation.\n\nReference: Hamilton (1994, Chapter 11), Lütkepohl (2005, Section 3.2)","category":"section"},{"location":"manual/#VARModel-Return-Values","page":"VAR","title":"VARModel Return Values","text":"estimate_var returns a VARModel{T} with the following fields:\n\nField Type Description\nY Matrix{T} Original T times n data matrix\np Int Number of lags\nB Matrix{T} (1+np) times n coefficient matrix c A_1 ldots A_p\nU Matrix{T} (T-p) times n residual matrix\nSigma Matrix{T} n times n residual covariance matrix\naic T Akaike Information Criterion\nbic T Bayesian Information Criterion\nhqic T Hannan-Quinn Information Criterion\n\nnote: Technical Note\nThe coefficient matrix B stores the intercept in the first row, followed by A_1 A_2 ldots A_p stacked vertically. To extract lag-i coefficients: A_i = model.B[(i-1)*n+2 : i*n+1, :]. The intercept is model.B[1, :].","category":"section"},{"location":"manual/#Stability-Condition","page":"VAR","title":"Stability Condition","text":"A VAR(p) is stable (stationary) if all eigenvalues of the companion matrix F lie inside the unit circle:\n\nF = beginbmatrix\nA_1  A_2  cdots  A_p-1  A_p \nI_n  0  cdots  0  0 \n0  I_n  cdots  0  0 \nvdots  vdots  ddots  vdots  vdots \n0  0  cdots  I_n  0\nendbmatrix_np times np\n\nStability Check: lambda_i  1 for all eigenvalues lambda_i of F.","category":"section"},{"location":"manual/#Information-Criteria-for-Lag-Selection","page":"VAR","title":"Information Criteria for Lag Selection","text":"The optimal lag length can be selected using information criteria:\n\nAkaike Information Criterion (AIC):\n\ntextAIC(p) = loghatSigma + frac2T(n^2 p + n)\n\nBayesian Information Criterion (BIC):\n\ntextBIC(p) = loghatSigma + fraclog TT(n^2 p + n)\n\nHannan-Quinn Criterion (HQ):\n\ntextHQ(p) = loghatSigma + frac2 log(log T)T(n^2 p + n)\n\nSelect the lag order p that minimizes the criterion.\n\nReference: Lütkepohl (2005, Section 4.3)\n\n","category":"section"},{"location":"manual/#Structural-VAR-(SVAR)-and-Identification","page":"VAR","title":"Structural VAR (SVAR) and Identification","text":"","category":"section"},{"location":"manual/#From-Reduced-Form-to-Structural-Shocks","page":"VAR","title":"From Reduced-Form to Structural Shocks","text":"The reduced-form residuals u_t are linear combinations of structural shocks varepsilon_t:\n\nu_t = B_0 varepsilon_t\n\nwhere:\n\nB_0 is the n times n contemporaneous impact matrix\nvarepsilon_t are structural shocks with Evarepsilon_t varepsilon_t = I_n\n\nThe relationship between the reduced-form and structural covariance is:\n\nSigma = B_0 B_0\n\nThe identification problem is that infinitely many B_0 matrices satisfy this condition. To identify structural shocks, we need n(n-1)2 additional restrictions.\n\nReference: Kilian & Lütkepohl (2017, Chapter 8)","category":"section"},{"location":"manual/#Cholesky-Identification-(Recursive)","page":"VAR","title":"Cholesky Identification (Recursive)","text":"The Cholesky decomposition imposes a lower triangular structure on B_0:\n\nB_0 = textchol(Sigma)\n\nThis implies a recursive causal ordering where variable i responds contemporaneously only to variables 1 2 ldots i-1.\n\nEconomic Interpretation: The ordering reflects assumptions about the speed of adjustment. Variables ordered first respond only to their own shocks contemporaneously.\n\nReference: Sims (1980), Christiano, Eichenbaum & Evans (1999)","category":"section"},{"location":"manual/#Sign-Restrictions","page":"VAR","title":"Sign Restrictions","text":"Sign restrictions identify structural shocks by constraining the signs of impulse responses at selected horizons. Let Theta_h denote the impulse response at horizon h. The identification algorithm:\n\nCompute the Cholesky decomposition: P = textchol(Sigma)\nDraw a random orthogonal matrix Q from the Haar measure (using QR decomposition of a random matrix)\nCompute candidate impact matrix: B_0 = PQ\nCheck if impulse responses Theta_0 = B_0 Theta_1 ldots satisfy the sign restrictions\nIf restrictions are satisfied, keep the draw; otherwise, discard and repeat\n\nImplementation: We use the algorithm of Rubio-Ramírez, Waggoner & Zha (2010).\n\nReference: Faust (1998), Uhlig (2005), Rubio-Ramírez, Waggoner & Zha (2010)","category":"section"},{"location":"manual/#Narrative-Restrictions","page":"VAR","title":"Narrative Restrictions","text":"Narrative restrictions combine sign restrictions with historical information about specific shocks at particular dates. Following Antolín-Díaz & Rubio-Ramírez (2018):\n\nShock Sign Narrative: At date t^*, structural shock j was positive/negative\nShock Contribution Narrative: At date t^*, shock j was the main driver of variable i\n\nThe algorithm:\n\nDraw orthogonal matrix Q satisfying sign restrictions\nRecover structural shocks: varepsilon = B_0^-1 u\nCheck if narrative constraints are satisfied\nWeight the draw using importance sampling\n\nReference: Antolín-Díaz & Rubio-Ramírez (2018)","category":"section"},{"location":"manual/#Long-Run-(Blanchard-Quah)-Identification","page":"VAR","title":"Long-Run (Blanchard-Quah) Identification","text":"Long-run restrictions constrain the cumulative effect of structural shocks. For a stationary VAR, the long-run impact matrix is:\n\nC(1) = (I_n - A_1 - A_2 - cdots - A_p)^-1 B_0\n\nBlanchard & Quah (1989) impose that certain shocks have zero long-run effect on specific variables by requiring C(1) to be lower triangular:\n\nC(1) = textcholleft( (I - A(1))^-1 Sigma (I - A(1))^-1 right)\n\nThen B_0 = (I - A(1)) C(1).\n\nEconomic Application: Demand shocks have no long-run effect on output (supply-driven long-run fluctuations).\n\nReference: Blanchard & Quah (1989), King, Plosser, Stock & Watson (1991)","category":"section"},{"location":"manual/#Arias-et-al.-(2018)-Identification","page":"VAR","title":"Arias et al. (2018) Identification","text":"When sign restrictions alone are insufficient, one can impose zero restrictions on specific impulse responses in addition to sign constraints. Arias, Rubio-Ramírez & Waggoner (2018) develop an algorithm that draws orthogonal rotation matrices Q from a distribution that is uniform over the set satisfying the zero restrictions, then filters for sign satisfaction.\n\nRestriction Types:\n\nType Function Description\nZero zero_restriction(var, shock; horizon=0) Variable var does not respond to shock at horizon\nSign sign_restriction(var, shock, :positive; horizon=0) Response has required sign at horizon\n\nAlgorithm: For n variables with r_j zero restrictions on shock j:\n\nCompute MA coefficients Phi_0 ldots Phi_H and Cholesky factor L\nFor each draw, construct Q column-by-column via QR decomposition in the null space of the zero restriction matrix\nCheck sign restrictions on the candidate IRF Theta_h = Phi_h L Q\nCorrect non-uniform sampling via importance weights when zero restrictions reduce the dimension\n\nusing MacroEconometricModels\n\n# Load FRED-MD monetary policy variables\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 4)\n\n# Define restrictions: monetary policy shock (shock 3)\n# Zero: INDPRO does not respond to monetary shock on impact\n# Sign: FFR rises, CPI falls after a contractionary monetary shock\nrestrictions = SVARRestrictions(3;\n    zeros = [zero_restriction(1, 3; horizon=0)],        # No impact on INDPRO on impact\n    signs = [sign_restriction(3, 3, :positive),          # FFR rises on impact\n             sign_restriction(2, 3, :negative; horizon=1)] # CPI falls at h=1\n)\n\n# Identify\nresult = identify_arias(model, restrictions, 20; n_draws=1000)\nprintln(\"Acceptance rate: \", round(result.acceptance_rate * 100, digits=1), \"%\")\n\n# Weighted IRF percentiles\npct = irf_percentiles(result; probs=[0.16, 0.5, 0.84])\nprintln(\"Median IRF(FFR→INDPRO, h=0): \", round(pct[1, 1, 3, 2], digits=3))\n\n# Bayesian version\n# bresult = identify_arias_bayesian(post, restrictions, 20)\n\nThe acceptance rate indicates what fraction of random draws satisfy all restrictions simultaneously. Low rates (below 1%) suggest the restrictions may be nearly contradictory or overly stringent. The importance weights correct for non-uniform sampling induced by zero restrictions — the weighted percentiles provide correctly calibrated credible intervals.","category":"section"},{"location":"manual/#AriasSVARResult-Return-Values","page":"VAR","title":"AriasSVARResult Return Values","text":"Field Type Description\nQ_draws Vector{Matrix{T}} Accepted rotation matrices\nirf_draws Array{T,4} n_draws times H times n times n IRF draws\nweights Vector{T} Importance weights (normalized to sum to 1)\nacceptance_rate T Fraction of draws satisfying all restrictions\nrestrictions SVARRestrictions The imposed restrictions\n\nReference: Arias, Rubio-Ramírez & Waggoner (2018)","category":"section"},{"location":"manual/#Mountford-Uhlig-(2009)-Penalty-Function-Identification","page":"VAR","title":"Mountford-Uhlig (2009) Penalty Function Identification","text":"When you want a single best rotation rather than a distribution of draws, Mountford & Uhlig (2009) provide a penalty function approach. Zero restrictions are enforced exactly via null-space projection; sign restrictions are encouraged via a penalty function optimized with Nelder-Mead.\n\nWhen to use: Use identify_uhlig when you need one optimal Q satisfying sign (and optionally zero) restrictions. Use identify_arias when you need a distribution of valid Q matrices for credible intervals.\n\nAlgorithm:\n\nParameterize Q column-by-column using spherical coordinates in the null space of zero-restriction constraints\nDefine penalty: -sum_s w_s cdot textsign_s cdot textIRF_s  sigma_s where w_s = 100 if the sign is satisfied, w_s = 1 if violated\nMinimize penalty via two-phase Nelder-Mead: coarse global search, then local refinement\n\nusing MacroEconometricModels\n\n# Load FRED-MD monetary policy variables\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 4)\n\n# Mountford-Uhlig: separate fiscal vs monetary shocks\n# Shock 1 (fiscal): no impact on FFR, positive on INDPRO\n# Shock 3 (monetary): positive FFR on impact\nrestrictions = SVARRestrictions(3;\n    zeros = [zero_restriction(3, 1; horizon=0)],   # Fiscal shock has no impact on FFR\n    signs = [sign_restriction(1, 1, :positive),     # Fiscal shock → INDPRO positive\n             sign_restriction(3, 3, :positive)]     # Monetary shock → FFR positive\n)\n\n# Find optimal Q\nresult = identify_uhlig(model, restrictions, 20)\nprintln(\"Converged: \", result.converged)\nprintln(\"Penalty: \", round(result.penalty, digits=2))\nprintln(\"Impact IRF(fiscal→INDPRO): \", round(result.irf[1, 1, 1], digits=3))","category":"section"},{"location":"manual/#UhligSVARResult-Return-Values","page":"VAR","title":"UhligSVARResult Return Values","text":"Field Type Description\nQ Matrix{T} Optimal rotation matrix\nirf Array{T,3} H times n times n impulse responses\npenalty T Total penalty at optimum (negative = better)\nshock_penalties Vector{T} Per-shock penalty values\nrestrictions SVARRestrictions The imposed restrictions\nconverged Bool Whether all sign restrictions are satisfied\n\nReference: Mountford & Uhlig (2009)\n\n","category":"section"},{"location":"manual/#Innovation-Accounting","page":"VAR","title":"Innovation Accounting","text":"For detailed coverage of innovation accounting tools, see the dedicated Innovation Accounting chapter. This includes:\n\nImpulse Response Functions (IRF): Dynamic effects of structural shocks\nForecast Error Variance Decomposition (FEVD): Variance contribution of each shock\nHistorical Decomposition (HD): Decompose observed movements into shock contributions\nSummary Tables: Publication-quality output with report(), table(), print_table()\n\n","category":"section"},{"location":"manual/#Bayesian-VAR-(BVAR)","page":"VAR","title":"Bayesian VAR (BVAR)","text":"For comprehensive coverage of Bayesian VAR estimation, see the dedicated Bayesian VAR chapter. Key topics include:\n\nMinnesota/Litterman prior specification\nHyperparameter optimization via marginal likelihood (Giannone, Lenza & Primiceri, 2015)\nConjugate NIW posterior sampling\nPosterior inference and credible intervals\n\n","category":"section"},{"location":"manual/#Information-Criteria-and-Model-Selection","page":"VAR","title":"Information Criteria and Model Selection","text":"","category":"section"},{"location":"manual/#Log-Likelihood","page":"VAR","title":"Log-Likelihood","text":"For a Gaussian VAR, the log-likelihood is:\n\nlog L = -fracT cdot n2 log(2pi) - fracT2 logSigma - frac12 sum_t=1^T u_t Sigma^-1 u_t","category":"section"},{"location":"manual/#Marginal-Likelihood-(Bayesian)","page":"VAR","title":"Marginal Likelihood (Bayesian)","text":"For Bayesian model comparison, we use the marginal likelihood (also called evidence):\n\np(Y  mathcalM) = int p(Y  theta mathcalM) p(theta  mathcalM)  dtheta\n\nModels with higher marginal likelihood better balance fit and complexity.\n\n","category":"section"},{"location":"manual/#Covariance-Estimation","page":"VAR","title":"Covariance Estimation","text":"","category":"section"},{"location":"manual/#Newey-West-HAC-Estimator","page":"VAR","title":"Newey-West HAC Estimator","text":"For robust inference in the presence of heteroskedasticity and autocorrelation, we use the Newey-West (1987, 1994) estimator:\n\nhatV_NW = (XX)^-1 hatS (XX)^-1\n\nwhere the long-run covariance hatS is:\n\nhatS = hatGamma_0 + sum_j=1^m w_j (hatGamma_j + hatGamma_j)\n\nwith hatGamma_j = frac1T sum_t=j+1^T hatu_t hatu_t-j x_t x_t-j.","category":"section"},{"location":"manual/#Kernel-Functions","page":"VAR","title":"Kernel Functions","text":"The weight function w_j depends on the kernel:\n\nBartlett (Newey-West):\n\nw_j = 1 - fracjm+1\n\nParzen:\n\nw_j = begincases\n1 - 6x^2 + 6x^3  x leq 05 \n2(1-x)^3  05  x leq 1\nendcases\n\nwhere x = j(m+1).\n\nQuadratic Spectral (Andrews, 1991):\n\nw_j = frac2512pi^2 x^2 left( fracsin(6pi x5)6pi x5 - cos(6pi x5) right)","category":"section"},{"location":"manual/#Automatic-Bandwidth-Selection","page":"VAR","title":"Automatic Bandwidth Selection","text":"Newey & West (1994) provide a data-driven bandwidth:\n\nm^* = 11447 left( hatalpha cdot T right)^13\n\nwhere hatalpha is estimated from an AR(1) fit to the residuals:\n\nhatalpha = frac4hatrho^2(1-hatrho)^4","category":"section"},{"location":"manual/#White-Heteroscedasticity-Robust-Estimator-(HC0)","page":"VAR","title":"White Heteroscedasticity-Robust Estimator (HC0)","text":"When errors are heteroscedastic but serially uncorrelated, the White (1980) estimator provides consistent standard errors without requiring bandwidth selection:\n\nhatV_W = (XX)^-1 left( sum_t=1^T hatu_t^2 x_t x_t right) (XX)^-1\n\nwhere\n\nhatu_t are the OLS residuals\nx_t is the k times 1 regressor vector at time t","category":"section"},{"location":"manual/#Driscoll-Kraay-Panel-Robust-Estimator","page":"VAR","title":"Driscoll-Kraay Panel-Robust Estimator","text":"For panel data with both cross-sectional and temporal dependence, the Driscoll & Kraay (1998) estimator applies HAC estimation to the cross-sectional averages of the moment conditions. This produces standard errors robust to both heteroscedasticity, serial correlation, and cross-sectional dependence.","category":"section"},{"location":"manual/#Julia-Implementation","page":"VAR","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy variables\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Construct design matrices\nY_eff, X = construct_var_matrices(Y, 2)\nresiduals = Y_eff - X * ((X'X) \\ (X'Y_eff))\n\n# Newey-West HAC (default: Bartlett kernel, automatic bandwidth)\nV_nw = newey_west(X, residuals; bandwidth=0, kernel=:bartlett)\n\n# White heteroscedasticity-robust (HC0)\nV_w = white_vcov(X, residuals)\n\n# Driscoll-Kraay for panel data\n# V_dk = driscoll_kraay(X, residuals; bandwidth=4)\n\n# Automatic bandwidth selection\nbw = optimal_bandwidth_nw(residuals)\nprintln(\"Optimal Newey-West bandwidth: \", bw)\n\nThe Newey-West estimator is appropriate for time series with heteroscedastic and serially correlated errors — the standard choice for LP and VAR applications. The White estimator is simpler but inconsistent when errors are autocorrelated. The Driscoll-Kraay estimator extends HAC to panel settings where cross-sectional units may be correlated (e.g., country-level macro panels).","category":"section"},{"location":"manual/#Comparing-LP-and-VAR","page":"VAR","title":"Comparing LP and VAR","text":"The compare_var_lp function provides a structured comparison of VAR and LP impulse responses:\n\n# Compare VAR and LP impulse responses for the monetary policy shock (variable 3)\ncomparison = compare_var_lp(Y, 3, 20; lags=4)\n\nThis estimates both a VAR and LP model on the same FRED-MD data and returns the IRFs from each, facilitating visual and numerical comparison. Under correct specification, the IRFs should be close (Plagborg-Møller & Wolf 2021); substantial disagreement suggests dynamic misspecification in the VAR.\n\nReference: Newey & West (1987, 1994), Andrews (1991), Driscoll & Kraay (1998)\n\n","category":"section"},{"location":"manual/#Complete-Example","page":"VAR","title":"Complete Example","text":"This example demonstrates an end-to-end VAR workflow from lag selection through structural analysis.\n\nusing MacroEconometricModels\n\n# Load FRED-MD: standard monetary VAR ordering (slow to fast)\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Step 1: Select lag order\nsel = select_lag_order(Y, 13)\nprintln(\"AIC lag: \", sel.p_aic, \"  BIC lag: \", sel.p_bic)\n\n# Step 2: Estimate VAR\nmodel = estimate_var(Y, sel.p_bic)\nprintln(\"AIC: \", round(model.aic, digits=2),\n        \"  BIC: \", round(model.bic, digits=2))\n\n# Step 3: Check stability\nstab = is_stationary(model)\nprintln(\"Stationary: \", stab.is_stationary,\n        \"  Max modulus: \", round(stab.max_modulus, digits=4))\n\n# Step 4: Cholesky IRF with bootstrap CI\n# Ordering: [INDPRO, CPI, FFR] — a monetary policy shock raises FFR\nirfs = irf(model, 20; method=:cholesky, ci_type=:bootstrap, reps=500)\nprintln(\"Impact of monetary shock on FFR: \", round(irfs.values[1, 3, 3], digits=3))\nprintln(\"Response of INDPRO at h=8: \", round(irfs.values[9, 1, 3], digits=3))\n\n# Step 5: FEVD\ndecomp = fevd(model, 20)\nprintln(\"INDPRO explained by monetary shock at h=1: \",\n        round(decomp.proportions[1, 1, 3] * 100, digits=1), \"%\")\nprintln(\"INDPRO explained by monetary shock at h=20: \",\n        round(decomp.proportions[20, 1, 3] * 100, digits=1), \"%\")\n\n# Step 6: Historical decomposition\nhd = historical_decomposition(model, size(model.U, 1))\nverify_decomposition(hd)  # Should return true\n\nThe lag selection criteria typically agree for well-specified systems; BIC tends to be more conservative and is preferred when parsimony matters. The Cholesky ordering [INDPRO, CPI, FFR] implies that a monetary policy shock (shock 3) raises the federal funds rate on impact, while output and prices respond with a lag — the standard recursive identification following Christiano, Eichenbaum & Evans (1999). The FEVD reveals how much of the forecast error variance in industrial production is attributable to monetary shocks at different horizons. The historical decomposition identity y_t = sum_j textHD_j(t) + textinitial(t) should hold exactly up to numerical precision.\n\n","category":"section"},{"location":"manual/#See-Also","page":"VAR","title":"See Also","text":"VECM Analysis – Error correction models for cointegrated systems\nLocal Projections – Model-free impulse response estimation (Jorda 2005)\nNon-Gaussian Identification – ICA, ML, and heteroskedasticity-based SVAR identification\nInnovation Accounting – IRF, FEVD, and historical decomposition details\nAPI Reference – Complete function signatures","category":"section"},{"location":"manual/#References","page":"VAR","title":"References","text":"","category":"section"},{"location":"manual/#Vector-Autoregression","page":"VAR","title":"Vector Autoregression","text":"Christiano, Lawrence J., Martin Eichenbaum, and Charles L. Evans. 1999. \"Monetary Policy Shocks: What Have We Learned and to What End?\" In Handbook of Macroeconomics, Vol. 1, edited by John B. Taylor and Michael Woodford, 65–148. Amsterdam: Elsevier. https://doi.org/10.1016/S1574-0048(99)01005-8\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.\nSims, Christopher A. 1980. \"Macroeconomics and Reality.\" Econometrica 48 (1): 1–48. https://doi.org/10.2307/1912017","category":"section"},{"location":"manual/#Structural-Identification","page":"VAR","title":"Structural Identification","text":"Arias, Jonas E., Juan F. Rubio-Ramírez, and Daniel F. Waggoner. 2018. \"Inference Based on Structural Vector Autoregressions Identified with Sign and Zero Restrictions: Theory and Applications.\" Econometrica 86 (2): 685–720. https://doi.org/10.3982/ECTA14468\nAntolín-Díaz, Juan, and Juan F. Rubio-Ramírez. 2018. \"Narrative Sign Restrictions for SVARs.\" American Economic Review 108 (10): 2802–2829. https://doi.org/10.1257/aer.20161852\nBlanchard, Olivier Jean, and Danny Quah. 1989. \"The Dynamic Effects of Aggregate Demand and Supply Disturbances.\" American Economic Review 79 (4): 655–673.\nFaust, Jon. 1998. \"The Robustness of Identified VAR Conclusions about Money.\" Carnegie-Rochester Conference Series on Public Policy 49: 207–244. https://doi.org/10.1016/S0167-2231(99)00009-3\nKilian, Lutz, and Helmut Lütkepohl. 2017. Structural Vector Autoregressive Analysis. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108164818\nMountford, Andrew, and Harald Uhlig. 2009. \"What Are the Effects of Fiscal Policy Shocks?\" Journal of Applied Econometrics 24 (6): 960–992. https://doi.org/10.1002/jae.1079\nRubio-Ramírez, Juan F., Daniel F. Waggoner, and Tao Zha. 2010. \"Structural Vector Autoregressions: Theory of Identification and Algorithms for Inference.\" Review of Economic Studies 77 (2): 665–696. https://doi.org/10.1111/j.1467-937X.2009.00578.x\nUhlig, Harald. 2005. \"What Are the Effects of Monetary Policy on Output? Results from an Agnostic Identification Procedure.\" Journal of Monetary Economics 52 (2): 381–419. https://doi.org/10.1016/j.jmoneco.2004.05.007","category":"section"},{"location":"manual/#Bayesian-Methods","page":"VAR","title":"Bayesian Methods","text":"Bańbura, Marta, Domenico Giannone, and Lucrezia Reichlin. 2010. \"Large Bayesian Vector Auto Regressions.\" Journal of Applied Econometrics 25 (1): 71–92. https://doi.org/10.1002/jae.1137\nCarriero, Andrea, Todd E. Clark, and Massimiliano Marcellino. 2015. \"Bayesian VARs: Specification Choices and Forecast Accuracy.\" Journal of Applied Econometrics 30 (1): 46–73. https://doi.org/10.1002/jae.2315\nDoan, Thomas, Robert Litterman, and Christopher Sims. 1984. \"Forecasting and Conditional Projection Using Realistic Prior Distributions.\" Econometric Reviews 3 (1): 1–100. https://doi.org/10.1080/07474938408800053\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nKadiyala, K. Rao, and Sune Karlsson. 1997. \"Numerical Methods for Estimation and Inference in Bayesian VAR-Models.\" Journal of Applied Econometrics 12 (2): 99–132. https://doi.org/10.1002/(SICI)1099-1255(199703)12:2<99::AID-JAE429>3.0.CO;2-A\nLitterman, Robert B. 1986. \"Forecasting with Bayesian Vector Autoregressions—Five Years of Experience.\" Journal of Business & Economic Statistics 4 (1): 25–38. https://doi.org/10.1080/07350015.1986.10509491","category":"section"},{"location":"manual/#Inference","page":"VAR","title":"Inference","text":"Driscoll, John C., and Aart C. Kraay. 1998. \"Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data.\" Review of Economics and Statistics 80 (4): 549–560. https://doi.org/10.1162/003465398557825\nAndrews, Donald W. K. 1991. \"Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.\" Econometrica 59 (3): 817–858. https://doi.org/10.2307/2938229\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Boca Raton, FL: CRC Press. ISBN 978-1-4398-4095-5.\nHoffman, Matthew D., and Andrew Gelman. 2014. \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\" Journal of Machine Learning Research 15 (1): 1593–1623.\nKilian, Lutz. 1998. \"Small-Sample Confidence Intervals for Impulse Response Functions.\" Review of Economics and Statistics 80 (2): 218–230. https://doi.org/10.1162/003465398557465\nNewey, Whitney K., and Kenneth D. West. 1987. \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.\" Econometrica 55 (3): 703–708. https://doi.org/10.2307/1913610\nNewey, Whitney K., and Kenneth D. West. 1994. \"Automatic Lag Selection in Covariance Matrix Estimation.\" Review of Economic Studies 61 (4): 631–653. https://doi.org/10.2307/2297912","category":"section"},{"location":"filters/#Time-Series-Filters","page":"Time Series Filters","title":"Time Series Filters","text":"Macroeconomic time series are often decomposed into trend and cyclical components. This module provides five standard filters used in applied macroeconomics for trend-cycle decomposition:\n\nFilter Key Idea Observations Lost\nHodrick-Prescott Penalized least squares (smoothing spline) None\nHamilton OLS regression on lagged values h + p - 1 at start\nBeveridge-Nelson ARIMA-based permanent/transitory decomposition None\nBaxter-King Symmetric band-pass moving average 2K (K at each end)\nBoosted HP Iterated HP with data-driven stopping None","category":"section"},{"location":"filters/#Quick-Start","page":"Time Series Filters","title":"Quick Start","text":"using MacroEconometricModels\n\n# Log industrial production — a trending I(1) monthly series\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Hodrick-Prescott filter (monthly data)\nhp = hp_filter(y; lambda=129600.0)\n\n# Hamilton (2018) regression filter (monthly: h=24, p=12)\nham = hamilton_filter(y; h=24, p=12)\n\n# Beveridge-Nelson decomposition\nbn = beveridge_nelson(y)\n\n# Baxter-King band-pass filter (monthly: 18–96 month band, K=36)\nbk = baxter_king(y; pl=18, pu=96, K=36)\n\n# Boosted HP filter\nbhp = boosted_hp(y; lambda=129600.0, stopping=:BIC)\n\n# Unified accessors\ntrend(hp)   # trend component\ncycle(hp)   # cyclical component\n\n","category":"section"},{"location":"filters/#Hodrick-Prescott-Filter","page":"Time Series Filters","title":"Hodrick-Prescott Filter","text":"The HP filter (Hodrick & Prescott 1997) decomposes a time series y_t into trend tau_t and cycle c_t = y_t - tau_t by solving:\n\nmin_tau sum_t=1^T (y_t - tau_t)^2 + lambda sum_t=2^T-1 (tau_t+1 - 2tau_t + tau_t-1)^2\n\nwhere lambda controls the smoothness of the trend. The first term penalizes deviations of the trend from the data, while the second penalizes curvature in the trend.","category":"section"},{"location":"filters/#Choosing-\\lambda","page":"Time Series Filters","title":"Choosing lambda","text":"Data Frequency Recommended lambda\nAnnual 6.25\nQuarterly 1,600\nMonthly 129,600\n\nThe quarterly value of 1,600 proposed by Hodrick and Prescott has become standard, though Ravn and Uhlig (2002) provide a frequency-based justification.","category":"section"},{"location":"filters/#Implementation","page":"Time Series Filters","title":"Implementation","text":"The solution is tau = (I + lambda DD)^-1 y where D is the (T-2) times T second-difference matrix. The implementation builds a sparse pentadiagonal system and solves via Cholesky factorization, giving O(T) computational cost.\n\n# Log industrial production (monthly)\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Monthly filter (lambda=129600)\nhp = hp_filter(y; lambda=129600.0)\nhp.trend   # smooth trend\nhp.cycle   # cyclical deviations — the HP cycle captures business cycle fluctuations\n\n# Annual data would use lambda=6.25\nhp_annual = hp_filter(y; lambda=6.25)\n\nnote: Technical Note\nThe HP filter has known issues with spurious cyclicality at sample endpoints and can induce spurious dynamic relations. Hamilton (2018) provides a detailed critique and proposes an alternative (see below).\n\n","category":"section"},{"location":"filters/#Hamilton-Filter","page":"Time Series Filters","title":"Hamilton Filter","text":"Hamilton (2018) proposes a regression-based alternative to the HP filter. Instead of imposing smoothness, it regresses the future value y_t+h on current and lagged values:\n\ny_t+h = beta_0 + beta_1 y_t + beta_2 y_t-1 + cdots + beta_p y_t-p+1 + v_t\n\nThe fitted values form the trend and the residuals form the cycle. The default parameters h = 8 p = 4 correspond to a 2-year ahead projection using 4 quarterly lags.","category":"section"},{"location":"filters/#Advantages-over-HP","page":"Time Series Filters","title":"Advantages over HP","text":"Does not require choosing a smoothing parameter\nAvoids spurious cyclicality\nDoes not induce spurious dynamic relations between filtered series\nRobust to unit roots and structural breaks","category":"section"},{"location":"filters/#Implementation-2","page":"Time Series Filters","title":"Implementation","text":"# Log industrial production (monthly)\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Monthly parameters (h=24, p=12): 2-year horizon, 12 monthly lags\nham = hamilton_filter(y; h=24, p=12)\nham.trend       # fitted values (length T - h - p + 1)\nham.cycle       # residuals — the Hamilton filter avoids endpoint bias\nham.beta        # OLS coefficients\nham.valid_range # indices into original series\n\nwarning: Warning\nThe Hamilton filter loses h + p - 1 observations at the start of the sample. For monthly data with h=24, p=12, this is 35 observations. Plan accordingly with short samples.\n\n","category":"section"},{"location":"filters/#Beveridge-Nelson-Decomposition","page":"Time Series Filters","title":"Beveridge-Nelson Decomposition","text":"The Beveridge-Nelson (1981) decomposition separates an I(1) process into a permanent (random walk with drift) component and a stationary transitory component. It exploits the Wold representation of the first-differenced series:\n\nDelta y_t = mu + psi(L) varepsilon_t = mu + sum_j=0^infty psi_j varepsilon_t-j\n\nwhere psi_0 = 1. The long-run multiplier is psi(1) = 1 + sum_j=1^infty psi_j, and the decomposition is:\n\ny_t = tau_t + c_t\n\nwhere tau_t is a random walk with drift mu cdot psi(1) (permanent component) and c_t is a mean-zero stationary process (transitory component).","category":"section"},{"location":"filters/#Implementation-3","page":"Time Series Filters","title":"Implementation","text":"The function fits an ARMA model to Delta y_t, computes the psi-weights from the MA(infty) representation, and constructs the transitory component.\n\n# Log industrial production — an I(1) series suitable for BN decomposition\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Automatic ARMA order selection for Δy\nbn = beveridge_nelson(y)\nbn.permanent    # random walk + drift (trend)\nbn.transitory   # stationary cycle\nbn.drift        # estimated drift\nbn.long_run_multiplier  # ψ(1)\nbn.arima_order  # (p, 1, q) used\n\n# Manual ARMA order\nbn2 = beveridge_nelson(y; p=2, q=1)\n\nnote: Technical Note\nThe BN decomposition assumes the series is I(1). Apply unit root tests (adf_test, kpss_test) to verify this assumption before using. For I(0) series, the decomposition is degenerate.\n\n","category":"section"},{"location":"filters/#Baxter-King-Band-Pass-Filter","page":"Time Series Filters","title":"Baxter-King Band-Pass Filter","text":"The Baxter-King (1999) filter isolates cyclical fluctuations in a specified frequency band 2pip_u 2pip_l using a symmetric finite moving average approximation to the ideal band-pass filter.","category":"section"},{"location":"filters/#Ideal-Band-Pass-Weights","page":"Time Series Filters","title":"Ideal Band-Pass Weights","text":"The ideal (infinite) band-pass filter has weights:\n\nB_0 = fracomega_H - omega_Lpi quad B_j = fracsin(omega_H j) - sin(omega_L j)pi j\n\nwhere omega_H = 2pip_l and omega_L = 2pip_u.","category":"section"},{"location":"filters/#Truncation-and-Adjustment","page":"Time Series Filters","title":"Truncation and Adjustment","text":"The ideal filter is truncated at lag K and adjusted to ensure the weights sum to zero (eliminating stochastic trends):\n\na_j = B_j + theta quad theta = -fracB_0 + 2sum_j=1^K B_j2K + 1\n\nThe filtered series is:\n\nc_t = a_0 y_t + sum_j=1^K a_j (y_t-j + y_t+j)","category":"section"},{"location":"filters/#Implementation-4","page":"Time Series Filters","title":"Implementation","text":"# Log industrial production (monthly)\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Monthly business cycle band: 18–96 months (1.5–8 years), K=36\nbk = baxter_king(y; pl=18, pu=96, K=36)\nbk.cycle       # band-pass filtered (business cycle component)\nbk.trend       # residual (low + high frequency)\nbk.weights     # [a_0, a_1, ..., a_K]\nbk.valid_range # K+1 : T-K\n\nThe filter weights sum to zero by construction:\n\nw = bk.weights\ntotal = w[1] + 2 * sum(w[2:end])  # ≈ 0\n\nwarning: Warning\nThe BK filter loses K observations at each end (2K total). With K = 36 and monthly data, this is 6 years of data at the boundaries.\n\n","category":"section"},{"location":"filters/#Boosted-HP-Filter","page":"Time Series Filters","title":"Boosted HP Filter","text":"Phillips and Shi (2021) propose iterating the HP filter on the cyclical component to improve trend estimation when the data contains stochastic trends. The key insight is that a single HP pass may leave unit root behavior in the cycle; re-filtering removes it.","category":"section"},{"location":"filters/#Algorithm","page":"Time Series Filters","title":"Algorithm","text":"Apply HP filter: hattau^(1) = S cdot y, hatc^(1) = (I - S) cdot y\nRe-filter the cycle: hatc^(m) = (I - S) hatc^(m-1)\nStop when a data-driven criterion is met\nFinal trend: hattau = y - hatc^(m^*)\n\nwhere S = (I + lambda DD)^-1.","category":"section"},{"location":"filters/#Stopping-Criteria","page":"Time Series Filters","title":"Stopping Criteria","text":"Three stopping rules are available:\n\n:BIC (default) — Fit AR(1) to the cycle at each iteration; stop when BIC increases. Selects the iteration that best balances parsimony and fit.\n:ADF — Run the ADF test on the cycle; stop when the null of a unit root is rejected at significance level sig_p. Ensures the cycle is stationary.\n:fixed — Run exactly max_iter iterations. Useful for comparison or when the other criteria are too conservative.","category":"section"},{"location":"filters/#Implementation-5","page":"Time Series Filters","title":"Implementation","text":"# Log industrial production (monthly)\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# BIC stopping (default), monthly lambda\nbhp = boosted_hp(y; lambda=129600.0)\nbhp.trend       # boosted trend\nbhp.cycle       # boosted cycle\nbhp.iterations  # number of iterations used\nbhp.bic_path    # BIC at each iteration\n\n# ADF stopping\nbhp_adf = boosted_hp(y; lambda=129600.0, stopping=:ADF, sig_p=0.05)\nbhp_adf.adf_pvalues  # p-values at each iteration\n\n# Fixed iterations\nbhp_fixed = boosted_hp(y; lambda=129600.0, stopping=:fixed, max_iter=5)\n\n","category":"section"},{"location":"filters/#Unified-Accessors","page":"Time Series Filters","title":"Unified Accessors","text":"All filter results support the trend() and cycle() accessors:\n\n# Works for any AbstractFilterResult\nfor r in [hp, ham, bn, bk, bhp]\n    t = trend(r)  # trend component\n    c = cycle(r)  # cyclical component\nend\n\nFor BeveridgeNelsonResult, trend() returns the permanent component and cycle() returns the transitory component.\n\n","category":"section"},{"location":"filters/#Comparison-Example","page":"Time Series Filters","title":"Comparison Example","text":"using MacroEconometricModels\nusing Statistics\n\n# Log industrial production (monthly, FRED-MD)\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Apply all five filters with monthly parameters\nhp  = hp_filter(y; lambda=129600.0)\nham = hamilton_filter(y; h=24, p=12)\nbn  = beveridge_nelson(y)\nbk  = baxter_king(y; pl=18, pu=96, K=36)\nbhp = boosted_hp(y; lambda=129600.0, stopping=:BIC)\n\n# Compare cycle standard deviations\nprintln(\"Cycle standard deviations:\")\nprintln(\"  HP:       \", round(std(cycle(hp)), digits=4))\nprintln(\"  Hamilton: \", round(std(cycle(ham)), digits=4))\nprintln(\"  BN:       \", round(std(cycle(bn)), digits=4))\nprintln(\"  BK:       \", round(std(cycle(bk)), digits=4))\nprintln(\"  bHP:      \", round(std(cycle(bhp)), digits=4))","category":"section"},{"location":"filters/#References","page":"Time Series Filters","title":"References","text":"Hodrick, Robert J., and Edward C. Prescott. 1997. \"Postwar U.S. Business Cycles: An Empirical Investigation.\" Journal of Money, Credit and Banking 29 (1): 1–16. https://doi.org/10.2307/2953682\nHamilton, James D. 2018. \"Why You Should Never Use the Hodrick-Prescott Filter.\" Review of Economics and Statistics 100 (5): 831–843. https://doi.org/10.1162/resta00706\nBeveridge, Stephen, and Charles R. Nelson. 1981. \"A New Approach to Decomposition of Economic Time Series into Permanent and Transitory Components with Particular Attention to Measurement of the 'Business Cycle'.\" Journal of Monetary Economics 7 (2): 151–174. https://doi.org/10.1016/0304-3932(81)90040-4\nBaxter, Marianne, and Robert G. King. 1999. \"Measuring Business Cycles: Approximate Band-Pass Filters for Economic Time Series.\" Review of Economics and Statistics 81 (4): 575–593. https://doi.org/10.1162/003465399558454\nPhillips, Peter C. B., and Zhentao Shi. 2021. \"Boosting: Why You Can Use the HP Filter.\" International Economic Review 62 (2): 521–570. https://doi.org/10.1111/iere.12495\nMei, Ziwei, Peter C. B. Phillips, and Zhentao Shi. 2024. \"The boosted HP filter is more general than you might think.\" Journal of Applied Econometrics 39 (7): 1260–1281. https://doi.org/10.1002/jae.3086","category":"section"},{"location":"api/#API-Reference","page":"Overview","title":"API Reference","text":"This section provides the complete API documentation for MacroEconometricModels.jl.\n\nThe API documentation is organized into the following pages:\n\nTypes: Core type definitions for models, results, and estimators\nFunctions: Function documentation organized by module","category":"section"},{"location":"api/#Quick-Reference-Tables","page":"Overview","title":"Quick Reference Tables","text":"Typed data containers, built-in datasets (FRED-MD, FRED-QD, Penn World Table), and data cleaning utilities. See Data Management for theory and examples.","category":"section"},{"location":"api/#Data-Management","page":"Overview","title":"Data Management","text":"Function Description\nTimeSeriesData(data; varnames, frequency, tcode) Typed time series container with metadata\nPanelData / CrossSectionData Panel and cross-section containers\ndiagnose(d) Scan for NaN, Inf, constant columns\nfix(d; method=:listwise) Clean data (:listwise, :interpolate, :mean)\nvalidate_for_model(d, :var) Check dimensionality for model type\napply_tcode(y, tcode) FRED transformation codes 1–7\ninverse_tcode(y, tcode; x_prev) Undo FRED transformation\napply_filter(d, :hp; component=:cycle) Apply time series filters per-variable\ndescribe_data(d) Per-variable summary statistics\nxtset(df, group_col, time_col) Stata-style panel construction\ngroup_data(pd, g) Extract single entity from panel\nto_matrix(d) / to_vector(d) Convert to raw matrix/vector\ndesc(d) / vardesc(d, name) Dataset and per-variable descriptions\nset_desc!(d, text) / set_vardesc!(d, name, text) Set descriptions\nrename_vars!(d, old => new) Rename variables\nload_example(:fred_md) / load_example(:fred_qd) / load_example(:pwt) Load built-in datasets (FRED-MD, FRED-QD, PWT)\n\nAR, MA, ARMA, and ARIMA model estimation with automatic order selection. See ARIMA Models for estimation methods, forecasting, and model selection.","category":"section"},{"location":"api/#ARIMA-Estimation-Functions","page":"Overview","title":"ARIMA Estimation Functions","text":"Function Description\nestimate_ar(y, p; method=:ols) AR(p) via OLS or MLE\nestimate_ma(y, q; method=:css_mle) MA(q) via CSS, MLE, or CSS-MLE\nestimate_arma(y, p, q; method=:css_mle) ARMA(p,q) via CSS, MLE, or CSS-MLE\nestimate_arima(y, p, d, q; method=:css_mle) ARIMA(p,d,q) via differencing + ARMA\nforecast(model, h; conf_level=0.95) Multi-step forecasting with confidence intervals\nselect_arima_order(y, max_p, max_q) Grid search for optimal ARMA order\nauto_arima(y) Automatic ARIMA order selection\nic_table(y, max_p, max_q) Information criteria comparison table\n\nTrend-cycle decomposition via HP, Hamilton, Beveridge-Nelson, Baxter-King, and boosted HP filters. See Time Series Filters for theory and comparisons.","category":"section"},{"location":"api/#Time-Series-Filters","page":"Overview","title":"Time Series Filters","text":"Function Description\nhp_filter(y; lambda=1600.0) Hodrick-Prescott trend-cycle decomposition\nhamilton_filter(y; h=8, p=4) Hamilton (2018) regression filter\nbeveridge_nelson(y; p=:auto, q=:auto) Beveridge-Nelson permanent/transitory decomposition\nbaxter_king(y; pl=6, pu=32, K=12) Baxter-King band-pass filter\nboosted_hp(y; stopping=:BIC, lambda=1600.0) Boosted HP filter (Phillips & Shi 2021)\ntrend(result) Extract trend component from filter result\ncycle(result) Extract cyclical component from filter result\n\nVAR, VECM, BVAR, Local Projections, Factor Models, and Panel VAR estimation. See VAR, VECM, BVAR, LP, Factor Models, and Panel VAR for theory and examples.","category":"section"},{"location":"api/#Multivariate-Estimation-Functions","page":"Overview","title":"Multivariate Estimation Functions","text":"Function Description\nestimate_var(Y, p) Estimate VAR(p) via OLS\nestimate_bvar(Y, p; ...) Estimate Bayesian VAR (conjugate NIW)\nestimate_lp(Y, shock_var, H; ...) Standard Local Projection\nestimate_lp_iv(Y, shock_var, Z, H; ...) LP with instrumental variables\nestimate_smooth_lp(Y, shock_var, H; ...) Smooth LP with B-splines\nestimate_state_lp(Y, shock_var, state_var, H; ...) State-dependent LP\nestimate_propensity_lp(Y, treatment, covariates, H; ...) LP with propensity scores\ndoubly_robust_lp(Y, treatment, covariates, H; ...) Doubly robust LP estimator\nestimate_factors(X, r; ...) Static factor model via PCA\nestimate_dynamic_factors(X, r, p; ...) Dynamic factor model\nestimate_gdfm(X, q; ...) Generalized dynamic factor model\nestimate_pvar(pd, p; ...) Panel VAR via GMM (FD or System)\nestimate_pvar_feols(pd, p; ...) Panel VAR via Fixed-Effects OLS\nestimate_gmm(moment_fn, theta0, data; ...) GMM estimation\nstructural_lp(Y, H; method=:cholesky, ...) Structural LP with multi-shock IRFs\nestimate_vecm(Y, p; rank=:auto, ...) Estimate VECM via Johansen MLE or Engle-Granger\nto_var(vecm) Convert VECM to VAR in levels\nselect_vecm_rank(Y, p; ...) Select cointegrating rank\ngranger_causality_vecm(vecm, cause, effect) VECM Granger causality test\nforecast(vecm, h; ci_method=:none, ...) VECM forecast preserving cointegration\n\nImpulse response functions, forecast error variance decomposition, historical decomposition, and 18+ structural identification methods. See Innovation Accounting and Non-Gaussian Identification.","category":"section"},{"location":"api/#Structural-Analysis-Functions","page":"Overview","title":"Structural Analysis Functions","text":"Function Description\nirf(model, H; ...) Compute impulse response functions\nfevd(model, H; ...) Forecast error variance decomposition\nidentify_cholesky(model) Cholesky identification\nidentify_sign(model; ...) Sign restriction identification\nidentify_long_run(model) Blanchard-Quah identification\nidentify_narrative(model; ...) Narrative sign restrictions\nidentify_arias(model, restrictions, H; ...) Arias et al. (2018) sign + zero restrictions\nidentify_uhlig(model, restrictions, H; ...) Mountford-Uhlig (2009) penalty function sign + zero restrictions\nidentify_fastica(model; ...) FastICA SVAR identification\nidentify_jade(model; ...) JADE SVAR identification\nidentify_sobi(model; ...) SOBI SVAR identification\nidentify_dcov(model; ...) Distance covariance SVAR identification\nidentify_hsic(model; ...) HSIC SVAR identification\nidentify_student_t(model; ...) Student-t ML SVAR identification\nidentify_mixture_normal(model; ...) Mixture-normal ML SVAR identification\nidentify_pml(model; ...) Pseudo-ML SVAR identification\nidentify_skew_normal(model; ...) Skew-normal ML SVAR identification\nidentify_nongaussian_ml(model; ...) Unified non-Gaussian ML dispatcher\nidentify_markov_switching(model; ...) Markov-switching SVAR identification\nidentify_garch(model; ...) GARCH SVAR identification\nidentify_smooth_transition(model, s; ...) Smooth-transition SVAR identification\nidentify_external_volatility(model, regime) External volatility SVAR identification\npvar_oirf(model, H) Panel VAR orthogonalized IRF (Cholesky)\npvar_girf(model, H) Panel VAR generalized IRF (Pesaran & Shin 1998)\npvar_fevd(model, H) Panel VAR forecast error variance decomposition\npvar_stability(model) Panel VAR eigenvalue stability check\npvar_bootstrap_irf(model, H; ...) Panel VAR bootstrap IRF confidence intervals\nlp_fevd(slp, H; method=:r2, ...) LP-FEVD (Gorodnichenko & Lee 2019)\ncumulative_irf(lp_irfs) Cumulative IRF from LP impulse response\nhistorical_decomposition(slp) Historical decomposition from structural LP\n\nDirect multi-step forecasting from Local Projection models. See Local Projections for estimation details.","category":"section"},{"location":"api/#LP-Forecasting-Functions","page":"Overview","title":"LP Forecasting Functions","text":"Function Description\nforecast(lp, shock_path; ...) Direct multi-step LP forecast\nforecast(slp, shock_idx, shock_path; ...) Structural LP conditional forecast\n\nAugmented Dickey-Fuller, KPSS, Phillips-Perron, Zivot-Andrews, Ng-Perron, and Johansen cointegration tests. See Hypothesis Tests for interpretation and examples.","category":"section"},{"location":"api/#Unit-Root-Test-Functions","page":"Overview","title":"Unit Root Test Functions","text":"Function Description\nadf_test(y; ...) Augmented Dickey-Fuller unit root test\nkpss_test(y; ...) KPSS stationarity test\npp_test(y; ...) Phillips-Perron unit root test\nza_test(y; ...) Zivot-Andrews structural break test\nngperron_test(y; ...) Ng-Perron unit root tests (MZα, MZt, MSB, MPT)\njohansen_test(Y, p; ...) Johansen cointegration test\nis_stationary(model) Check VAR model stationarity\nunit_root_summary(y; ...) Run multiple tests with summary\ntest_all_variables(Y; ...) Apply test to all columns\n\nLikelihood ratio (LR) and Lagrange multiplier (LM/score) tests for comparing nested models across ARIMA, VAR, and GARCH families. See Hypothesis Tests.","category":"section"},{"location":"api/#Model-Comparison-Tests","page":"Overview","title":"Model Comparison Tests","text":"Function Description\nlr_test(m1, m2) Likelihood ratio test for nested models\nlm_test(m1, m2) Lagrange multiplier (score) test for nested models\n\nPairwise and block Wald tests for Granger causality in VAR models. See Hypothesis Tests for details.","category":"section"},{"location":"api/#Granger-Causality-Tests","page":"Overview","title":"Granger Causality Tests","text":"Function Description\ngranger_test(model, cause, effect) Pairwise or block Granger causality test\ngranger_test_all(model) All-pairs pairwise Granger causality matrix\n\nConvenience functions for extracting impulse responses from fitted LP models. See Local Projections.","category":"section"},{"location":"api/#LP-IRF-Extraction","page":"Overview","title":"LP IRF Extraction","text":"Function Description\nlp_irf(model; ...) Extract IRF from LPModel\nlp_iv_irf(model; ...) Extract IRF from LPIVModel\nsmooth_lp_irf(model; ...) Extract smoothed IRF\nstate_irf(model; ...) Extract state-dependent IRFs\npropensity_irf(model; ...) Extract ATE impulse response\n\nStatic PCA, Dynamic Factor, and Generalized Dynamic Factor model estimation, forecasting, and selection criteria. See Factor Models.","category":"section"},{"location":"api/#Factor-Model-Functions","page":"Overview","title":"Factor Model Functions","text":"Function Description\nestimate_factors(X, r; ...) Estimate r-factor model\nestimate_dynamic_factors(X, r, p; ...) Dynamic factor model\nestimate_gdfm(X, q; ...) Generalized dynamic factor model\nforecast(fm, h; p=1, ci_method=:none) Static FM forecast (fits VAR(p) on factors)\nforecast(dfm, h; ci_method=:none) DFM forecast (:none/:theoretical/:bootstrap/:simulation)\nforecast(gdfm, h; ci_method=:none) GDFM forecast (:none/:theoretical/:bootstrap)\nic_criteria(X, r_max) Bai-Ng information criteria\nic_criteria_dynamic(X, max_r, max_p) DFM factor/lag selection\nic_criteria_gdfm(X, max_q) GDFM dynamic factor selection\nscree_plot_data(model) Data for scree plot\nis_stationary(dfm) Check DFM factor VAR stationarity\ncommon_variance_share(gdfm) GDFM common variance share per variable\npredict(fm) Fitted values (all factor model types)\nresiduals(fm) Idiosyncratic residuals (all factor model types)\nr2(fm) Per-variable R^2 (all factor model types)\nnobs(fm) Number of observations\ndof(fm) Degrees of freedom\nloglikelihood(dfm) Log-likelihood (DFM only)\naic(dfm) / bic(dfm) Information criteria (DFM only)\n\nBayesian prior optimization, instrument strength tests, and Panel VAR specification tests. See BVAR and Panel VAR.","category":"section"},{"location":"api/#Diagnostic-Functions","page":"Overview","title":"Diagnostic Functions","text":"Function Description\noptimize_hyperparameters(Y, p; ...) Optimize Minnesota prior\nposterior_mean_model(post; ...) VARModel from posterior mean\nposterior_median_model(post; ...) VARModel from posterior median\nweak_instrument_test(model; ...) Test for weak instruments\nsargan_test(model, h) Overidentification test\ntest_regime_difference(model; ...) Test regime differences\npropensity_diagnostics(model) Propensity score diagnostics\npvar_hansen_j(model) Hansen J-test for Panel VAR\npvar_mmsc(model) Andrews-Lu MMSC for Panel VAR\npvar_lag_selection(pd, max_p; ...) Panel VAR lag order selection\nj_test(model) Hansen J-test for GMM\ngmm_summary(model) Summary statistics for GMM\n\nMultivariate normality tests for VAR residuals. See Non-Gaussian Identification for using these as pre-tests for ICA/ML identification.","category":"section"},{"location":"api/#Normality-Test-Functions","page":"Overview","title":"Normality Test Functions","text":"Function Description\njarque_bera_test(model; method=:multivariate) Multivariate Jarque-Bera test\nmardia_test(model; type=:both) Mardia skewness/kurtosis tests\ndoornik_hansen_test(model) Doornik-Hansen omnibus test\nhenze_zirkler_test(model) Henze-Zirkler characteristic function test\nnormality_test_suite(model) Run all normality tests\n\nDiagnostic tests for non-Gaussian SVAR identification validity. See Non-Gaussian Identification.","category":"section"},{"location":"api/#Identifiability-Test-Functions","page":"Overview","title":"Identifiability Test Functions","text":"Function Description\ntest_shock_gaussianity(result) Test non-Gaussianity of recovered shocks\ntest_gaussian_vs_nongaussian(model; ...) LR test: Gaussian vs non-Gaussian\ntest_shock_independence(result; ...) Test independence of recovered shocks\ntest_identification_strength(model; ...) Bootstrap identification strength test\ntest_overidentification(model, result; ...) Overidentification test\n\nARCH, GARCH, EGARCH, GJR-GARCH, and Stochastic Volatility estimation, forecasting, and diagnostics. See Volatility Models.","category":"section"},{"location":"api/#Volatility-Model-Functions","page":"Overview","title":"Volatility Model Functions","text":"Function Description\nestimate_arch(y, q) ARCH(q) via MLE\nestimate_garch(y, p, q) GARCH(p,q) via MLE\nestimate_egarch(y, p, q) EGARCH(p,q) via MLE\nestimate_gjr_garch(y, p, q) GJR-GARCH(p,q) via MLE\nestimate_sv(y; variant, ...) Stochastic Volatility via KSC Gibbs\nforecast(vol_model, h) Volatility forecast with simulation CIs\narch_lm_test(y_or_model, q) ARCH-LM test for conditional heteroskedasticity\nljung_box_squared(z_or_model, K) Ljung-Box test on squared residuals\nnews_impact_curve(model) News impact curve (GARCH family)\npersistence(model) Persistence measure\nhalflife(model) Volatility half-life\nunconditional_variance(model) Unconditional variance\narch_order(model) ARCH order q\ngarch_order(model) GARCH order p\npredict(m) Conditional variance series hatsigma^2_t\nresiduals(m) Raw residuals (ARCH/GARCH) or standardized (SV)\ncoef(m) Coefficient vector\nnobs(m) Number of observations\nloglikelihood(m) Maximized log-likelihood (ARCH/GARCH)\naic(m) / bic(m) Information criteria (ARCH/GARCH)\ndof(m) Number of estimated parameters\n\nMixed-frequency nowcasting via DFM, BVAR, and bridge equations with news decomposition. See Nowcasting for theory and examples.","category":"section"},{"location":"api/#Nowcasting-Functions","page":"Overview","title":"Nowcasting Functions","text":"Function Description\nnowcast_dfm(Y, nM, nQ; r=2, p=1, ...) DFM nowcasting via EM + Kalman smoother (Banbura & Modugno 2014)\nnowcast_bvar(Y, nM, nQ; lags=5, ...) Large BVAR nowcasting with GLP priors (Cimadomo et al. 2022)\nnowcast_bridge(Y, nM, nQ; lagM=1, ...) Bridge equation combination nowcasting (Banbura et al. 2023)\nnowcast(model) Extract current-quarter nowcast and next-quarter forecast\nforecast(dfm_or_bvar, h; ...) Multi-step ahead forecast from nowcasting model\nnowcast_news(X_new, X_old, dfm, t; ...) News decomposition: attribute revision to data releases\nbalance_panel(d; r=2, method=:dfm) Fill NaN in TimeSeriesData/PanelData via DFM\n\nPublication-quality tables, display backend switching, and bibliographic references. See Examples for LaTeX and HTML export workflows.","category":"section"},{"location":"api/#Display-and-Output-Functions","page":"Overview","title":"Display and Output Functions","text":"Function Description\nset_display_backend(sym) Switch output format (:text/:latex/:html)\nget_display_backend() Current display backend\nreport(result) Print comprehensive summary\ntable(result, ...) Extract results as matrix\nprint_table([io], result, ...) Print formatted table\nrefs(model; format=...) Bibliographic references\nrefs(io, :method; format=...) References by method name\n\nHAC (Newey-West), heteroskedasticity-robust (White), and panel-robust (Driscoll-Kraay) covariance estimators.","category":"section"},{"location":"api/#Covariance-Functions","page":"Overview","title":"Covariance Functions","text":"Function Description\nnewey_west(X, residuals; ...) Newey-West HAC estimator\nwhite_vcov(X, residuals; ...) White heteroskedasticity-robust\ndriscoll_kraay(X, residuals; ...) Driscoll-Kraay panel-robust\nlong_run_variance(x; ...) Long-run variance estimate\nlong_run_covariance(X; ...) Long-run covariance matrix\noptimal_bandwidth_nw(residuals) Automatic bandwidth selection\n\nLow-level matrix construction and numerical utilities used internally.","category":"section"},{"location":"api/#Utility-Functions","page":"Overview","title":"Utility Functions","text":"Function Description\nconstruct_var_matrices(Y, p) Build VAR design matrices\ncompanion_matrix(B, n, p) VAR companion form\nrobust_inv(A) Robust matrix inverse\nsafe_cholesky(A; ...) Stable Cholesky decomposition","category":"section"},{"location":"pvar/#Panel-VAR","page":"Panel VAR","title":"Panel VAR","text":"This page documents the Panel VAR (PVAR) implementation in MacroEconometricModels.jl, providing estimation via GMM (Arellano-Bond first-difference and Blundell-Bond system) and fixed-effects OLS.","category":"section"},{"location":"pvar/#Quick-Start","page":"Panel VAR","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load Penn World Table: 38 OECD countries, 1950–2023\npwt = load_example(:pwt)\n\n# Convert to growth rates (log first difference) for stationarity\npd = apply_tcode(pwt, 5)\n\n# FD-GMM (Arellano-Bond) — GDP, employment, human capital growth spillovers\nmodel = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"], steps=:twostep)\n\n# System GMM (Blundell-Bond)\nmodel_sys = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"],\n                          system_instruments=true, steps=:twostep)\n\n# Fixed-effects OLS\nmodel_fe = estimate_pvar_feols(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"])\n\n","category":"section"},{"location":"pvar/#Model-Specification","page":"Panel VAR","title":"Model Specification","text":"The Panel VAR(p) model for entity i at time t is:\n\nmathbfy_it = boldsymbolmu_i + sum_l=1^p mathbfA_l mathbfy_it-l + boldsymbolvarepsilon_it quad i = 1 ldots N quad t = 1 ldots T_i\n\nwhere:\n\nmathbfy_it in mathbbR^m is the vector of endogenous variables for entity i\nboldsymbolmu_i in mathbbR^m is an entity-specific fixed effect\nmathbfA_l is the m times m coefficient matrix for lag l\nboldsymbolvarepsilon_it sim (0 Sigma) are i.i.d. innovations\n\nThe key econometric challenge is that boldsymbolmu_i is correlated with mathbfy_it-l by construction. OLS on the level equation is inconsistent. Two strategies are available:\n\nTransform away the fixed effect (first-differencing or forward orthogonal deviations) and estimate by GMM using lagged levels as instruments.\nDemean within groups (within estimator) and estimate by OLS. This is consistent for large T but Nickell-biased when T is small relative to N.\n\nnote: Fixed Effects and Nickell Bias\nThe within estimator (FE-OLS) is biased of order O(1T) in dynamic panels (Nickell, 1981). For panels with small T (e.g., T  20), GMM estimation is strongly preferred. For larger T, FE-OLS and GMM converge.\n\n","category":"section"},{"location":"pvar/#Panel-Data-Preparation","page":"Panel VAR","title":"Panel Data Preparation","text":"Panel VAR estimation requires a PanelData object. The built-in Penn World Table provides a balanced panel of 38 OECD countries:\n\nusing MacroEconometricModels\n\n# Load PWT — already a PanelData object\npwt = load_example(:pwt)\nprintln(ngroups(pwt), \" countries × \", nvars(pwt), \" variables\")\n\n# Convert to growth rates for stationarity\npd = apply_tcode(pwt, 5)  # tcode 5 = log first difference\n\nAll numeric columns are treated as potential endogenous variables. Use the dependent_vars keyword to select a subset:\n\nmodel = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"])\n\n","category":"section"},{"location":"pvar/#GMM-Estimation","page":"Panel VAR","title":"GMM Estimation","text":"","category":"section"},{"location":"pvar/#First-Difference-GMM-(Arellano-Bond)","page":"Panel VAR","title":"First-Difference GMM (Arellano-Bond)","text":"The default estimator transforms the model by first-differencing to remove boldsymbolmu_i:\n\nDelta mathbfy_it = sum_l=1^p mathbfA_l Delta mathbfy_it-l + Delta boldsymbolvarepsilon_it\n\nLagged levels mathbfy_it-2 mathbfy_it-3 ldots serve as instruments for Delta mathbfy_it-l (Holtz-Eakin, Newey & Rosen, 1988; Arellano & Bond, 1991). The instrument matrix is block-diagonal, with the number of instruments growing with t.\n\n# One-step GMM (heteroskedasticity-robust SEs)\nm1 = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"], steps=:onestep)\n\n# Two-step GMM (Windmeijer-corrected SEs)\nm2 = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"], steps=:twostep)\n\n# Forward orthogonal deviations (Arellano & Bover, 1995)\nm3 = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"],\n                   transformation=:fod, steps=:twostep)\n\nnote: One-Step vs Two-Step\nThe two-step estimator is asymptotically efficient but its naive standard errors are severely downward-biased in finite samples. The package automatically applies the Windmeijer (2005) correction for two-step GMM, which restores proper inference.","category":"section"},{"location":"pvar/#System-GMM-(Blundell-Bond)","page":"Panel VAR","title":"System GMM (Blundell-Bond)","text":"System GMM adds level equations instrumented by lagged differences, improving efficiency when the data are persistent (Blundell & Bond, 1998):\n\nm_sys = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"],\n                      system_instruments=true, steps=:twostep)\n\nThe system estimator stacks transformed equations (instrumented by lagged levels) with level equations (instrumented by lagged differences). This exploits additional moment conditions but requires the assumption that first differences are uncorrelated with fixed effects.","category":"section"},{"location":"pvar/#Instrument-Management","page":"Panel VAR","title":"Instrument Management","text":"When the number of instruments is large relative to N, standard errors can be unreliable. Several options control instrument proliferation:\n\n# Restrict instrument lags\nm = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"],\n                  min_lag_endo=2, max_lag_endo=4)\n\n# Collapse instruments (one column per lag distance)\nm = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"], collapse=true)\n\n# PCA instrument reduction\nm = estimate_pvar(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"], pca_instruments=true)\n\nwarning: Instrument Proliferation\nA rule of thumb: the number of instruments should not exceed N (the number of groups). When it does, consider collapsing instruments or restricting lag depth.\n\n","category":"section"},{"location":"pvar/#Fixed-Effects-OLS","page":"Panel VAR","title":"Fixed-Effects OLS","text":"For panels with large T, the within (FE-OLS) estimator provides a simpler alternative:\n\nm_fe = estimate_pvar_feols(pd, 2; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"])\n\nThe estimator demeans each country's data (removing boldsymbolmu_i) and runs pooled OLS on the stacked system. Standard errors are clustered at the group level.\n\n","category":"section"},{"location":"pvar/#Structural-Analysis","page":"Panel VAR","title":"Structural Analysis","text":"","category":"section"},{"location":"pvar/#Orthogonalized-Impulse-Response-Functions","page":"Panel VAR","title":"Orthogonalized Impulse Response Functions","text":"OIRF uses the Cholesky decomposition of the residual covariance Sigma = PP:\n\nirfs = pvar_oirf(model, 20)   # 20-period horizon\n# irfs[h+1] is m × m: response of variable i to shock j at horizon h\n\nThe impulse responses are computed from the companion form Phi_h = J A^h J and the Cholesky factor P:\n\nPsi_h = Phi_h P","category":"section"},{"location":"pvar/#Generalized-Impulse-Response-Functions","page":"Panel VAR","title":"Generalized Impulse Response Functions","text":"GIRF (Pesaran & Shin, 1998) does not depend on variable ordering:\n\ngirfs = pvar_girf(model, 20)\n\ntextGIRF_h(mathbfe_j) = fracPhi_h Sigma mathbfe_jsqrtsigma_jj","category":"section"},{"location":"pvar/#Forecast-Error-Variance-Decomposition","page":"Panel VAR","title":"Forecast Error Variance Decomposition","text":"FEVD based on the orthogonalized IRF:\n\ndecomp = pvar_fevd(model, 20)\n# decomp[h+1] is m × m: share of FEV of variable i due to shock j at horizon h\n\nEach row sums to 1 (100% of forecast error variance accounted for).","category":"section"},{"location":"pvar/#Stability-Analysis","page":"Panel VAR","title":"Stability Analysis","text":"Check whether all eigenvalues of the companion matrix lie inside the unit circle:\n\nstab = pvar_stability(model)\nstab.is_stable      # true if all |λ| < 1\nstab.moduli          # moduli of eigenvalues\n\n","category":"section"},{"location":"pvar/#Bootstrap-Confidence-Intervals","page":"Panel VAR","title":"Bootstrap Confidence Intervals","text":"Group-level block bootstrap preserves the within-group time structure:\n\nboot = pvar_bootstrap_irf(model, 20;\n    irf_type=:oirf,   # or :girf\n    n_draws=500,\n    ci=0.95\n)\n# boot.lower[h+1], boot.upper[h+1] are m × m CI bounds\n\nFor each bootstrap draw, N groups are resampled with replacement, the PVAR is re-estimated, and IRFs are computed. Quantile-based confidence intervals are constructed from the bootstrap distribution.\n\n","category":"section"},{"location":"pvar/#Specification-Tests","page":"Panel VAR","title":"Specification Tests","text":"","category":"section"},{"location":"pvar/#Hansen-J-Test","page":"Panel VAR","title":"Hansen J-Test","text":"The Hansen (1982) J-test evaluates whether the overidentifying restrictions (moment conditions) are valid:\n\nj = pvar_hansen_j(model)\nj.statistic     # J-statistic\nj.pvalue        # p-value (χ² distribution)\nj.df            # degrees of freedom = instruments - parameters\n\nUnder H_0: all moment conditions are valid. Rejection suggests instrument invalidity or model misspecification.\n\nwarning: J-Test and Instrument Count\nThe J-test has low power when the number of instruments is large relative to N. A non-rejection does not necessarily validate the instruments.","category":"section"},{"location":"pvar/#Andrews-Lu-MMSC","page":"Panel VAR","title":"Andrews-Lu MMSC","text":"Andrews-Lu (2001) Model and Moment Selection Criteria extend information criteria to GMM settings:\n\nmmsc = pvar_mmsc(model)\nmmsc.bic     # MMSC-BIC\nmmsc.aic     # MMSC-AIC\nmmsc.hqic    # MMSC-HQIC\n\ntextMMSC-BIC = J - (c - b) ln(n) quad\ntextMMSC-AIC = J - 2(c - b)\n\nwhere c = number of instruments, b = number of parameters, n = observations. Lower values are preferred.\n\n","category":"section"},{"location":"pvar/#Lag-Selection","page":"Panel VAR","title":"Lag Selection","text":"Select the optimal lag order by comparing MMSC criteria across candidate models:\n\nsel = pvar_lag_selection(pd, 4; dependent_vars=[\"rgdpna\", \"emp\", \"hc\"])\nsel.best_bic    # optimal lag by BIC\nsel.best_aic    # optimal lag by AIC\nsel.best_hqic   # optimal lag by HQIC\nsel.table       # comparison table\n\n","category":"section"},{"location":"pvar/#Complete-Example","page":"Panel VAR","title":"Complete Example","text":"using MacroEconometricModels\n\n# Load Penn World Table and convert to growth rates\npwt = load_example(:pwt)\npd = apply_tcode(pwt, 5)  # log first difference → growth rates\ndep_vars = [\"rgdpna\", \"emp\", \"hc\"]\n\n# Estimate via two-step FD-GMM: cross-country growth spillovers\nmodel = estimate_pvar(pd, 1; dependent_vars=dep_vars, steps=:twostep)\n\n# Specification tests\nj = pvar_hansen_j(model)\nprintln(\"Hansen J: stat=$(round(j.statistic, digits=3)), p=$(round(j.pvalue, digits=3))\")\n\n# Stability\nstab = pvar_stability(model)\nprintln(\"Stable: $(stab.is_stable)\")\n\n# Structural analysis: orthogonalized IRFs for GDP → employment → human capital\nirfs = pvar_oirf(model, 10)\ndecomp = pvar_fevd(model, 10)\n\n# Bootstrap confidence intervals\nusing Random; Random.seed!(42)\nboot = pvar_bootstrap_irf(model, 10; n_draws=200, ci=0.90)\n\n# Lag selection\nsel = pvar_lag_selection(pd, 3; dependent_vars=dep_vars)\nprintln(\"Best lag (BIC): $(sel.best_bic)\")\n\n# References\nrefs(model)\n\n","category":"section"},{"location":"pvar/#References","page":"Panel VAR","title":"References","text":"Arellano, Manuel, and Stephen Bond. 1991. \"Some Tests of Specification for Panel Data.\" Review of Economic Studies 58 (2): 277–297. https://doi.org/10.2307/2297968\nAndrews, Donald W. K., and Biao Lu. 2001. \"Consistent Model and Moment Selection Procedures for GMM Estimation.\" Journal of Econometrics 101 (1): 123–164. https://doi.org/10.1016/S0304-4076(00)00077-4\nBlundell, Richard, and Stephen Bond. 1998. \"Initial Conditions and Moment Restrictions in Dynamic Panel Data Models.\" Journal of Econometrics 87 (1): 115–143. https://doi.org/10.1016/S0304-4076(98)00009-8\nHansen, Lars Peter. 1982. \"Large Sample Properties of Generalized Method of Moments Estimators.\" Econometrica 50 (4): 1029–1054. https://doi.org/10.2307/1912775\nHoltz-Eakin, Douglas, Whitney Newey, and Harvey S. Rosen. 1988. \"Estimating Vector Autoregressions with Panel Data.\" Econometrica 56 (6): 1371–1395. https://doi.org/10.2307/1913103\nPesaran, M. Hashem, and Yongcheol Shin. 1998. \"Generalized Impulse Response Analysis in Linear Multivariate Models.\" Economics Letters 58 (1): 17–29. https://doi.org/10.1016/S0165-1765(97)00214-0\nWindmeijer, Frank. 2005. \"A Finite Sample Correction for the Variance of Linear Efficient Two-Step GMM Estimators.\" Journal of Econometrics 126 (1): 25–51. https://doi.org/10.1016/j.jeconom.2004.02.005","category":"section"},{"location":"factormodels/#Factor-Models","page":"Factor Models","title":"Factor Models","text":"This chapter covers static factor models for dimensionality reduction in large macroeconomic panels, including estimation via principal components and information criteria for selecting the number of factors.","category":"section"},{"location":"factormodels/#Introduction","page":"Factor Models","title":"Introduction","text":"Factor models are fundamental tools in macroeconometrics for extracting common sources of variation from large panels of economic indicators. They enable:\n\nDimensionality Reduction: Summarize N variables with r ll N factors\nForecasting: Use factors as predictors in regressions (diffusion indices)\nStructural Analysis: Identify common shocks driving multiple series\nFAVAR: Combine factors with VARs for high-dimensional structural analysis\n\nReference: Stock & Watson (2002a, 2002b), Bai & Ng (2002)","category":"section"},{"location":"factormodels/#Quick-Start","page":"Factor Models","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD and prepare a transformed panel of macro indicators\nfred = load_example(:fred_md)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\nfm = estimate_factors(X, 3; standardize=true)                      # Static factor model via PCA\nic = ic_criteria(X, 10)                                            # Bai-Ng IC for factor count\ndfm = estimate_dynamic_factors(X, 3, 1; method=:twostep)           # Dynamic factor model\ngdfm = estimate_gdfm(X, 2; kernel=:bartlett)                       # Generalized DFM (spectral)\nfc = forecast(fm, 12; ci_method=:theoretical)                      # Static FM forecast with analytical CIs\nfc = forecast(dfm, 12; ci_method=:bootstrap, n_boot=1000)          # DFM forecast with bootstrap CIs\nfc = forecast(gdfm, 12; ci_method=:theoretical)                    # GDFM forecast with analytical CIs\n\n","category":"section"},{"location":"factormodels/#The-Static-Factor-Model","page":"Factor Models","title":"The Static Factor Model","text":"","category":"section"},{"location":"factormodels/#Model-Specification","page":"Factor Models","title":"Model Specification","text":"The static factor model decomposes an N-dimensional vector of observables x_t into common and idiosyncratic components:\n\nx_it = lambda_i F_t + e_it quad i = 1 ldots N quad t = 1 ldots T\n\nIn matrix form:\n\nX = F Lambda + E\n\nwhere:\n\nX is the T times N data matrix\nF is the T times r matrix of latent factors\nLambda is the N times r matrix of factor loadings\nE is the T times N matrix of idiosyncratic errors\nr is the number of factors (with r ll min(T N))","category":"section"},{"location":"factormodels/#Assumptions","page":"Factor Models","title":"Assumptions","text":"Factors and Loadings:\n\nEF_t = 0, textVar(F_t) = I_r (normalization)\nfrac1T sum_t F_t F_t xrightarrowp Sigma_F positive definite\nfrac1N Lambda Lambda xrightarrowp Sigma_Lambda positive definite\n\nIdiosyncratic Errors:\n\nEe_it = 0\nWeak cross-sectional and temporal dependence allowed\nWeak correlation with factors: frac1NT sum_it EF_t e_it to 0\n\nReference: Bai & Ng (2002), Bai (2003)\n\n","category":"section"},{"location":"factormodels/#Estimation-via-Principal-Components","page":"Factor Models","title":"Estimation via Principal Components","text":"","category":"section"},{"location":"factormodels/#Principal-Components-Analysis-(PCA)","page":"Factor Models","title":"Principal Components Analysis (PCA)","text":"The factors and loadings are estimated by minimizing the sum of squared idiosyncratic errors:\n\nmin_F Lambda sum_i=1^N sum_t=1^T (x_it - lambda_i F_t)^2\n\nsubject to the normalization FFT = I_r.","category":"section"},{"location":"factormodels/#Solution","page":"Factor Models","title":"Solution","text":"The solution involves the eigenvalue decomposition of XX (or XX):\n\nCase 1: T  N (short panel)\n\nCompute XX (T times T matrix)\nhatF = sqrtT times (first r eigenvectors of XX)\nhatLambda = X hatF  T\n\nCase 2: N leq T (tall panel)\n\nCompute XX (N times N matrix)\nhatLambda = sqrtN times (first r eigenvectors of XX)\nhatF = X hatLambda  N","category":"section"},{"location":"factormodels/#Data-Preprocessing","page":"Factor Models","title":"Data Preprocessing","text":"Before estimation, data is typically:\n\nDemeaned: Center each series to have zero mean\nStandardized: Scale each series to have unit variance\n\nThis prevents high-variance series from dominating the factor extraction.","category":"section"},{"location":"factormodels/#Identification","page":"Factor Models","title":"Identification","text":"The factors and loadings are identified only up to an r times r invertible rotation. If (F Lambda) is a solution, so is (FH Lambda H^-1) for any invertible H.\n\nThe normalization FFT = I_r and LambdaLambda diagonal pins down rotation up to sign.\n\nReference: Stock & Watson (2002a), Bai & Ng (2002)","category":"section"},{"location":"factormodels/#Julia-Implementation","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD and prepare transformed panel\nfred = load_example(:fred_md)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\n# Estimate 3-factor model from FRED-MD indicators\nmodel = estimate_factors(X, 3;\n    standardize = true,    # Standardize data\n    method = :pca          # Principal components\n)\n\n# Access results\nF = model.factors          # T×3 estimated factors\nΛ = model.loadings         # 20×3 estimated loadings","category":"section"},{"location":"factormodels/#FactorModel-Return-Values","page":"Factor Models","title":"FactorModel Return Values","text":"Field Type Description\nX Matrix{T} Original T times N data matrix\nfactors Matrix{T} T times r estimated factor matrix\nloadings Matrix{T} N times r estimated loading matrix\neigenvalues Vector{T} Eigenvalues from PCA (in descending order)\nexplained_variance Vector{T} Fraction of variance explained by each factor\ncumulative_variance Vector{T} Cumulative fraction of variance explained\nr Int Number of factors\nstandardized Bool Whether data was standardized before estimation\n\nnote: Technical Note\nFactor models are identified only up to an r times r rotation: if (hatF hatLambda) is a solution, then (hatFH hatLambdaH^-1) is equally valid for any invertible H. The normalization FFT = I_r pins down orientation but not sign. Consequently, individual factor loadings should not be interpreted as structural parameters. To compare estimated factors with \"true\" factors (e.g., in simulations), compute absolute correlations rather than raw correlations.\n\n","category":"section"},{"location":"factormodels/#Determining-the-Number-of-Factors","page":"Factor Models","title":"Determining the Number of Factors","text":"","category":"section"},{"location":"factormodels/#The-Selection-Problem","page":"Factor Models","title":"The Selection Problem","text":"Choosing r is crucial:\n\nToo few factors: Omitted common variation, biased estimates\nToo many factors: Overfitting, including noise as signal","category":"section"},{"location":"factormodels/#Bai-and-Ng-(2002)-Information-Criteria","page":"Factor Models","title":"Bai & Ng (2002) Information Criteria","text":"Bai & Ng propose three information criteria:\n\nIC1:\n\nIC_1(r) = log hatsigma^2(r) + r cdot fracN + TNT logleft( fracNTN+T right)\n\nIC2:\n\nIC_2(r) = log hatsigma^2(r) + r cdot fracN + TNT log(C_NT^2)\n\nIC3:\n\nIC_3(r) = log hatsigma^2(r) + r cdot fraclog(C_NT^2)C_NT^2\n\nwhere:\n\nhatsigma^2(r) = frac1NT sum_it hate_it^2 is the average squared residual\nC_NT^2 = min(N T)\n\nSelection Rule: Choose hatr that minimizes IC_k(r) over r in 1 ldots r_max.\n\nProperties:\n\nIC2 and IC3 perform best in simulations\nAll three are consistent: hatr xrightarrowp r_0 as N T to infty\n\nReference: Bai & Ng (2002)","category":"section"},{"location":"factormodels/#Julia-Implementation-2","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load and transform FRED-MD panel\nfred = load_example(:fred_md)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\n# Bai-Ng information criteria select the number of factors from FRED-MD indicators\nr_max = 10\nic = ic_criteria(X, r_max)\n\n# Optimal number by each criterion\nprintln(\"IC1 selects: \", ic.r_IC1, \" factors\")\nprintln(\"IC2 selects: \", ic.r_IC2, \" factors\")\nprintln(\"IC3 selects: \", ic.r_IC3, \" factors\")\n\n# IC values for all r\nfor r in 1:r_max\n    println(\"r=$r: IC1=$(ic.IC1[r]), IC2=$(ic.IC2[r]), IC3=$(ic.IC3[r])\")\nend\n\n","category":"section"},{"location":"factormodels/#Scree-Plot-Analysis","page":"Factor Models","title":"Scree Plot Analysis","text":"","category":"section"},{"location":"factormodels/#Visual-Factor-Selection","page":"Factor Models","title":"Visual Factor Selection","text":"The scree plot displays eigenvalues (or variance explained) against factor number. The \"elbow\" in the plot suggests the number of significant factors.","category":"section"},{"location":"factormodels/#Variance-Explained","page":"Factor Models","title":"Variance Explained","text":"For each factor j:\n\nIndividual Variance:\n\ntextVarExp_j = fracmu_jsum_k=1^N mu_k\n\nCumulative Variance:\n\ntextCumVarExp_r = sum_j=1^r textVarExp_j\n\nwhere mu_j is the j-th largest eigenvalue of XXT (or XXN).","category":"section"},{"location":"factormodels/#Julia-Implementation-3","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate factor model from FRED-MD panel (using X from above)\nmodel = estimate_factors(X, 5)\n\n# Get scree plot data\nscree = scree_plot_data(model)\n\n# Variance explained by each factor\nfor j in 1:min(5, length(scree.factors))\n    println(\"Factor $j: $(round(scree.explained_variance[j]*100, digits=2))% \",\n            \"(cumulative: $(round(scree.cumulative_variance[j]*100, digits=2))%)\")\nend\n\n","category":"section"},{"location":"factormodels/#Model-Diagnostics","page":"Factor Models","title":"Model Diagnostics","text":"","category":"section"},{"location":"factormodels/#R-squared-for-Each-Variable","page":"Factor Models","title":"R-squared for Each Variable","text":"The R^2 measures how much of variable i's variation is explained by the common factors:\n\nR^2_i = 1 - fracsum_t hate_it^2sum_t (x_it - barx_i)^2\n\nVariables with low R^2 are mainly driven by idiosyncratic shocks.","category":"section"},{"location":"factormodels/#Julia-Implementation-4","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate 3-factor model from FRED-MD panel (using X from above)\nmodel = estimate_factors(X, 3)\n\n# R² for each variable — how much of each FRED-MD indicator is driven by common factors\nr2_values = r2(model)\n\n# Summary statistics\nprintln(\"Mean R²: \", round(mean(r2_values), digits=3))\nprintln(\"Median R²: \", round(median(r2_values), digits=3))\nprintln(\"Min R²: \", round(minimum(r2_values), digits=3))\nprintln(\"Max R²: \", round(maximum(r2_values), digits=3))\n\n# Variables well-explained by factors\nwell_explained = findall(r2_values .> 0.7)","category":"section"},{"location":"factormodels/#Fitted-Values-and-Residuals","page":"Factor Models","title":"Fitted Values and Residuals","text":"# Fitted values: X̂ = FΛ'\nX_fitted = predict(model)\n\n# Residuals: E = X - X̂\nresid = residuals(model)\n\n# Model statistics\nprintln(\"Number of observations: \", nobs(model))\nprintln(\"Degrees of freedom: \", dof(model))\n\n","category":"section"},{"location":"factormodels/#Applications","page":"Factor Models","title":"Applications","text":"","category":"section"},{"location":"factormodels/#Diffusion-Index-Forecasting","page":"Factor Models","title":"Diffusion Index Forecasting","text":"Use factors as predictors for forecasting a target variable y_t+h:\n\ny_t+h = alpha + beta hatF_t + gamma y_tt-p + varepsilon_t+h\n\nFactors summarize information from a large panel, improving forecast accuracy.\n\nReference: Stock & Watson (2002b)","category":"section"},{"location":"factormodels/#Factor-Augmented-VAR-(FAVAR)","page":"Factor Models","title":"Factor-Augmented VAR (FAVAR)","text":"Combine factors with key observable variables in a VAR:\n\nbeginbmatrix y_t  F_t endbmatrix = A_1 beginbmatrix y_t-1  F_t-1 endbmatrix + cdots + A_p beginbmatrix y_t-p  F_t-p endbmatrix + u_t\n\nThis allows structural analysis with high-dimensional information sets.\n\nReference: Bernanke, Boivin & Eliasz (2005)","category":"section"},{"location":"factormodels/#Example:-FAVAR-Setup","page":"Factor Models","title":"Example: FAVAR Setup","text":"using MacroEconometricModels\n\n# Estimate factors from FRED-MD panel (using X from above)\nfm = estimate_factors(X, 3)\nF = fm.factors\n\n# Load key monetary policy observables\nfred = load_example(:fred_md)\nY_key = to_matrix(apply_tcode(fred[:, [\"FEDFUNDS\", \"INDPRO\", \"CPIAUCSL\"]]))\nY_key = Y_key[all.(isfinite, eachrow(Y_key)), :]\n\n# Align lengths and combine factors with key observables\nT_min = min(size(F, 1), size(Y_key, 1))\nY_favar = hcat(Y_key[end-T_min+1:end, :], F[end-T_min+1:end, :])\n\n# Estimate FAVAR: 6 variables (3 observables + 3 factors)\nfavar_model = estimate_var(Y_favar, 4)\n\n# Structural analysis: monetary policy shock via Cholesky\nirf_favar = irf(favar_model, 20; method=:cholesky)\n\n","category":"section"},{"location":"factormodels/#Forecasting-with-Static-Factor-Models","page":"Factor Models","title":"Forecasting with Static Factor Models","text":"","category":"section"},{"location":"factormodels/#Forecast-Method","page":"Factor Models","title":"Forecast Method","text":"The static factor model does not directly specify factor dynamics, but forecasting is possible by fitting a VAR(p) on the extracted factors:\n\nhatF_T+hT = hatA_1 hatF_T+h-1T + cdots + hatA_p hatF_T+h-pT\n\nObservable forecasts are obtained via the loading matrix:\n\nhatX_T+hT = hatLambda hatF_T+hT","category":"section"},{"location":"factormodels/#Confidence-Intervals","page":"Factor Models","title":"Confidence Intervals","text":"Theoretical CIs use the VMA(infty) representation of the factor VAR to compute the h-step forecast error covariance analytically:\n\ntextMSE_h = sum_j=0^h-1 Psi_j Sigma_eta Psi_j\n\nwhere Psi_j = J C^j are the VMA coefficient matrices from the companion form.\n\nBootstrap CIs resample factor VAR residuals to construct simulated forecast paths and compute percentile intervals.","category":"section"},{"location":"factormodels/#Julia-Implementation-5","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate static factor model from FRED-MD panel (using X from above)\nfm = estimate_factors(X, 3)\n\n# Point forecast (fits VAR(1) on factors internally)\nfc = forecast(fm, 12)\nfc.factors       # 12×3 factor forecasts\nfc.observables   # 12×N observable forecasts\n\n# Forecast with theoretical (analytical) confidence intervals\nfc = forecast(fm, 12; ci_method=:theoretical, conf_level=0.95)\nfc.factors_lower   # 12×3 lower CI for factors\nfc.factors_upper   # 12×3 upper CI for factors\nfc.observables_se  # 12×N standard errors for observables\n\n# Forecast with bootstrap CIs\nusing Random; Random.seed!(42)\nfc = forecast(fm, 12; ci_method=:bootstrap, n_boot=1000, conf_level=0.90)\n\n# Use higher-order VAR for factor dynamics\nfc = forecast(fm, 12; p=2, ci_method=:theoretical)\n\nThe theoretical SEs increase with the forecast horizon, reflecting growing uncertainty. For stationary factor dynamics, the SEs converge to the unconditional forecast error standard deviation. Bootstrap CIs are preferred when the Gaussian assumption may not hold.\n\nReference: Stock & Watson (2002b)\n\n","category":"section"},{"location":"factormodels/#Asymptotic-Theory","page":"Factor Models","title":"Asymptotic Theory","text":"","category":"section"},{"location":"factormodels/#Consistency-of-Factor-Estimates","page":"Factor Models","title":"Consistency of Factor Estimates","text":"Under the assumptions of Bai & Ng (2002), as T N to infty:\n\nfrac1T sum_t=1^T hatF_t - H F_t^2 = O_pleft( frac1min(N T) right)\n\nwhere H is an r times r rotation matrix.\n\nThe factors are consistently estimated up to rotation at rate min(sqrtN sqrtT).","category":"section"},{"location":"factormodels/#Distribution-Theory","page":"Factor Models","title":"Distribution Theory","text":"For large N T, the factor estimates are asymptotically normal:\n\nsqrtT (hatF_t - H F_t) xrightarrowd N(0 V)\n\nwhere V depends on the cross-sectional and temporal dependence structure.\n\nReference: Bai (2003), Bai & Ng (2006)\n\n","category":"section"},{"location":"factormodels/#Comparison-with-Other-Methods","page":"Factor Models","title":"Comparison with Other Methods","text":"","category":"section"},{"location":"factormodels/#Static-vs.-Dynamic-Factor-Models","page":"Factor Models","title":"Static vs. Dynamic Factor Models","text":"Aspect Static FM Dynamic FM\nModel X_t = Lambda F_t + e_t X_t = Lambda(L) f_t + e_t\nFactors Contemporaneous May include lags\nEstimation PCA Spectral methods, Kalman filter\nUse case Large N, moderate T Time series dynamics important\n\nReference: Forni, Hallin, Lippi & Reichlin (2000)","category":"section"},{"location":"factormodels/#Maximum-Likelihood-Estimation","page":"Factor Models","title":"Maximum Likelihood Estimation","text":"ML estimation assumes Gaussian factors and errors:\n\nF_t sim N(0 I_r) quad e_t sim N(0 Psi)\n\nEstimated via EM algorithm. More efficient than PCA if model is correctly specified, but computationally intensive.\n\n","category":"section"},{"location":"factormodels/#Dynamic-Factor-Models","page":"Factor Models","title":"Dynamic Factor Models","text":"","category":"section"},{"location":"factormodels/#Model-Specification-2","page":"Factor Models","title":"Model Specification","text":"The dynamic factor model extends the static model by allowing factors to follow a VAR process:\n\nObservation Equation:\n\nX_t = Lambda F_t + e_t\n\nState Equation (Factor Dynamics):\n\nF_t = A_1 F_t-1 + A_2 F_t-2 + cdots + A_p F_t-p + eta_t\n\nwhere:\n\nF_t is the r times 1 vector of latent factors\nLambda is the N times r loading matrix\nA_1 ldots A_p are r times r autoregressive coefficient matrices\neta_t sim N(0 Sigma_eta) are factor innovations\ne_t sim N(0 Sigma_e) are idiosyncratic errors (typically diagonal)","category":"section"},{"location":"factormodels/#Estimation-Methods","page":"Factor Models","title":"Estimation Methods","text":"Two-Step Estimation:\n\nExtract factors using PCA (as in static model)\nEstimate VAR(p) on extracted factors\n\nEM Algorithm:\n\nIterates between E-step (Kalman smoother) and M-step (parameter updates)\nMore efficient but computationally intensive","category":"section"},{"location":"factormodels/#Julia-Implementation-6","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate dynamic factor model from FRED-MD panel (using X from above)\n# 3 factors with VAR(1) dynamics\nmodel = estimate_dynamic_factors(X, 3, 1;\n    method = :twostep,      # or :em\n    standardize = true,\n    diagonal_idio = true    # Diagonal idiosyncratic covariance\n)\n\n# Access results\nF = model.factors           # T×3 estimated factors\nΛ = model.loadings          # N×3 loadings\nA = model.A                 # Vector of 3×3 AR coefficient matrices\nΣ_η = model.Sigma_eta       # 3×3 factor innovation covariance\nΣ_e = model.Sigma_e         # N×N idiosyncratic covariance","category":"section"},{"location":"factormodels/#DynamicFactorModel-Return-Values","page":"Factor Models","title":"DynamicFactorModel Return Values","text":"Field Type Description\nX Matrix{T} Original T times N data matrix\nfactors Matrix{T} T times r estimated factors\nloadings Matrix{T} N times r loading matrix\nA Vector{Matrix{T}} r times r autoregressive coefficient matrices\nfactor_residuals Matrix{T} Factor VAR residuals\nSigma_eta Matrix{T} r times r factor innovation covariance\nSigma_e Matrix{T} N times N idiosyncratic covariance\neigenvalues Vector{T} Eigenvalues from initial PCA\nexplained_variance Vector{T} Variance explained by each factor\ncumulative_variance Vector{T} Cumulative variance explained\nr Int Number of factors\np Int Number of factor VAR lags\nmethod Symbol Estimation method (:twostep or :em)\nstandardized Bool Whether data was standardized\nconverged Bool Convergence status (relevant for :em)\niterations Int Number of iterations (relevant for :em)\nloglik T Log-likelihood value","category":"section"},{"location":"factormodels/#Model-Selection-for-DFM","page":"Factor Models","title":"Model Selection for DFM","text":"Select the number of factors r and lag order p using information criteria:\n\n# Grid search over (r, p) combinations\nic = ic_criteria_dynamic(X, max_r, max_p;\n    method = :twostep,\n    standardize = true\n)\n\nprintln(\"AIC selects: r=$(ic.r_AIC), p=$(ic.p_AIC)\")\nprintln(\"BIC selects: r=$(ic.r_BIC), p=$(ic.p_BIC)\")\n\n# View full IC matrices\nic.AIC  # r×p matrix of AIC values\nic.BIC  # r×p matrix of BIC values","category":"section"},{"location":"factormodels/#Forecasting-with-DFM","page":"Factor Models","title":"Forecasting with DFM","text":"The DFM forecast extrapolates the factor VAR dynamics forward and projects to observables via the loading matrix. Four CI methods are available:\n\nci_method Description Best for\n:none Point forecast only Quick exploration\n:theoretical Analytical VMA CIs (Gaussian) Large samples, fast\n:bootstrap Residual resampling Non-Gaussian innovations\n:simulation Monte Carlo draws from estimated model Full uncertainty propagation\n\n# Point forecasts h steps ahead\nfc = forecast(model, h)\nfc.factors       # h×r factor forecasts\nfc.observables   # h×N observable forecasts\n\n# Theoretical (analytical) confidence intervals\nfc = forecast(model, h; ci_method=:theoretical, conf_level=0.95)\nfc.factors_se           # h×r standard errors\nfc.observables_lower    # h×N lower CI bounds\nfc.observables_upper    # h×N upper CI bounds\n\n# Bootstrap confidence intervals\nfc = forecast(model, h; ci_method=:bootstrap, n_boot=1000, conf_level=0.90)\n\n# Simulation-based CIs (original method, also accessible via legacy ci=true)\nfc = forecast(model, h; ci_method=:simulation, n_boot=2000)\nfc = forecast(model, h; ci=true, ci_level=0.90)  # Legacy interface\n\nAll forecast methods return a FactorForecast struct. When ci_method=:none, the CI and SE fields are zero matrices.","category":"section"},{"location":"factormodels/#FactorForecast-Return-Values","page":"Factor Models","title":"FactorForecast Return Values","text":"Field Type Description\nfactors Matrix{T} h times r factor point forecasts\nobservables Matrix{T} h times N observable point forecasts\nfactors_lower Matrix{T} h times r lower CI bounds for factors\nfactors_upper Matrix{T} h times r upper CI bounds for factors\nobservables_lower Matrix{T} h times N lower CI bounds for observables\nobservables_upper Matrix{T} h times N upper CI bounds for observables\nfactors_se Matrix{T} h times r factor forecast standard errors\nobservables_se Matrix{T} h times N observable forecast standard errors\nhorizon Int Forecast horizon h\nconf_level T Confidence level (e.g., 0.95)\nci_method Symbol CI method used (:none, :theoretical, :bootstrap, :simulation)\n\nnote: Technical Note\nThe theoretical CIs compute the h-step forecast MSE via the VMA(infty) representation: textMSE_h = sum_j=0^h-1 Psi_j Sigma_eta Psi_j where Psi_j = J C^j with C the companion matrix and J the selector for the first r rows. Observable SEs combine factor uncertainty with idiosyncratic variance: textVar(hatX_T+h) = Lambda cdot textMSE_h cdot Lambda + Sigma_e.","category":"section"},{"location":"factormodels/#Stationarity-Check","page":"Factor Models","title":"Stationarity Check","text":"# Check if factor dynamics are stationary\nis_stationary(model)  # true if max|eigenvalue| < 1\n\n# Get companion matrix for factor VAR\nC = companion_matrix_factors(model)\neigvals(C)  # Eigenvalues determine stability\n\nReference: Stock & Watson (2002a), Doz, Giannone & Reichlin (2011)\n\n","category":"section"},{"location":"factormodels/#Generalized-Dynamic-Factor-Model-(GDFM)","page":"Factor Models","title":"Generalized Dynamic Factor Model (GDFM)","text":"","category":"section"},{"location":"factormodels/#Theoretical-Foundation","page":"Factor Models","title":"Theoretical Foundation","text":"The Generalized Dynamic Factor Model of Forni, Hallin, Lippi & Reichlin (2000, 2005) provides a fully dynamic approach to factor analysis using spectral methods. Unlike the standard DFM which uses static PCA followed by VAR, the GDFM extracts factors directly in the frequency domain.","category":"section"},{"location":"factormodels/#Model-Specification-3","page":"Factor Models","title":"Model Specification","text":"The GDFM decomposes each observable as:\n\nx_it = chi_it + xi_it\n\nwhere:\n\nchi_it is the common component driven by q common shocks\nxi_it is the idiosyncratic component\n\nThe common component has the representation:\n\nchi_it = b_i1(L) u_1t + b_i2(L) u_2t + cdots + b_iq(L) u_qt\n\nwhere b_ij(L) are square-summable filters and u_jt are orthonormal white noise shocks.","category":"section"},{"location":"factormodels/#Spectral-Representation","page":"Factor Models","title":"Spectral Representation","text":"In the frequency domain, the spectral density of X_t decomposes as:\n\nSigma_X(omega) = Sigma_chi(omega) + Sigma_xi(omega)\n\nThe key insight is that common factors produce diverging eigenvalues (growing with N) while idiosyncratic components produce bounded eigenvalues.","category":"section"},{"location":"factormodels/#Estimation-Algorithm","page":"Factor Models","title":"Estimation Algorithm","text":"Spectral Density Estimation: Estimate hatSigma_X(omega) using kernel smoothing of the periodogram\nDynamic Eigenanalysis: Compute eigenvalue decomposition at each frequency\nFactor Extraction: Select top q eigenvectors (dynamic principal components)\nCommon Component: Reconstruct chi_t via inverse Fourier transform","category":"section"},{"location":"factormodels/#Julia-Implementation-7","page":"Factor Models","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate GDFM from FRED-MD panel (using X from above)\n# 2 dynamic factors via spectral analysis\nmodel = estimate_gdfm(X, 2;\n    standardize = true,\n    bandwidth = 0,           # Auto-select: T^(1/3)\n    kernel = :bartlett,      # :bartlett, :parzen, or :tukey\n    r = 0                    # Static factors (0 = same as q)\n)\n\n# Access results\nF = model.factors                 # T×2 time-domain factors\nχ = model.common_component        # T×N common component\nξ = model.idiosyncratic           # T×N idiosyncratic component\nΛ = model.loadings_spectral       # N×2×n_freq frequency-domain loadings\n\n# Variance explained by dynamic factors\nmodel.variance_explained          # 2-vector of variance shares","category":"section"},{"location":"factormodels/#GeneralizedDynamicFactorModel-Return-Values","page":"Factor Models","title":"GeneralizedDynamicFactorModel Return Values","text":"Field Type Description\nX Matrix{T} Original T times N data matrix\nfactors Matrix{T} T times q time-domain factors\ncommon_component Matrix{T} T times N common component chi_t\nidiosyncratic Matrix{T} T times N idiosyncratic component xi_t\nloadings_spectral Array{Complex{T},3} N times q times n_freq frequency-domain loadings\nspectral_density_X Array{Complex{T},3} Spectral density of X_t\nspectral_density_chi Array{Complex{T},3} Spectral density of common component\neigenvalues_spectral Matrix{T} N times n_freq eigenvalues across frequencies\nfrequencies Vector{T} Frequency grid (0 to pi)\nq Int Number of dynamic factors\nr Int Number of static factors\nbandwidth Int Kernel smoothing bandwidth\nkernel Symbol Kernel type (:bartlett, :parzen, :tukey)\nstandardized Bool Whether data was standardized\nvariance_explained Vector{T} Variance share per dynamic factor","category":"section"},{"location":"factormodels/#Selecting-the-Number-of-Dynamic-Factors","page":"Factor Models","title":"Selecting the Number of Dynamic Factors","text":"The GDFM uses eigenvalue-based criteria rather than information criteria:\n\n# Compute selection criteria\nic = ic_criteria_gdfm(X, max_q;\n    standardize = true,\n    bandwidth = 0,\n    kernel = :bartlett\n)\n\n# Eigenvalue ratio criterion (Ahn & Horenstein 2013)\nprintln(\"Ratio criterion selects: q=$(ic.q_ratio)\")\n\n# Variance threshold criterion (90% of spectral variance)\nprintln(\"Variance criterion selects: q=$(ic.q_variance)\")\n\n# Diagnostic data\nic.eigenvalue_ratios      # λ_i / λ_{i+1} ratios\nic.cumulative_variance    # Cumulative variance explained\nic.avg_eigenvalues        # Average eigenvalues across frequencies","category":"section"},{"location":"factormodels/#Spectral-Diagnostics","page":"Factor Models","title":"Spectral Diagnostics","text":"# Get data for eigenvalue plots across frequencies\nplot_data = spectral_eigenvalue_plot_data(model)\nplot_data.frequencies     # Vector of frequencies (0 to π)\nplot_data.eigenvalues     # N×n_freq matrix of eigenvalues\n\n# First eigenvalue should dominate if one strong factor\n# Gap between q-th and (q+1)-th eigenvalue indicates factor count","category":"section"},{"location":"factormodels/#Common-Variance-Share","page":"Factor Models","title":"Common Variance Share","text":"# Fraction of variance explained by common component for each variable\nshares = common_variance_share(model)\n\n# Variables well-explained by common factors\nwell_explained = findall(shares .> 0.5)\n\n# Summary statistics\nprintln(\"Mean common variance share: \", round(mean(shares), digits=3))\nprintln(\"Variables with >50% common: \", length(well_explained))","category":"section"},{"location":"factormodels/#Forecasting-with-GDFM","page":"Factor Models","title":"Forecasting with GDFM","text":"The GDFM forecast uses AR(1) extrapolation of each factor series, with observable forecasts computed via the average spectral loadings. Confidence intervals are available via analytical or bootstrap methods.\n\n# Point forecast\nfc = forecast(model, h; method=:ar)\nfc.factors       # h×q factor forecasts\nfc.observables   # h×N observable forecasts\n\n# Theoretical CIs (closed-form AR(1) variance)\nfc = forecast(model, h; ci_method=:theoretical, conf_level=0.95)\nfc.factors_se           # h×q SEs (non-decreasing with horizon)\nfc.observables_lower    # h×N lower CI bounds\nfc.observables_upper    # h×N upper CI bounds\n\n# Bootstrap CIs (resample AR(1) residuals per factor)\nfc = forecast(model, h; ci_method=:bootstrap, n_boot=1000)\n\nThe theoretical CIs use the closed-form AR(1) forecast variance: textVar(hatF_T+hi) = sigma_i^2 sum_j=0^h-1 phi_i^2j where phi_i and sigma_i^2 are the AR(1) coefficient and innovation variance for factor i. Observable SEs combine factor uncertainty with idiosyncratic variance.","category":"section"},{"location":"factormodels/#Comparison:-DFM-vs-GDFM","page":"Factor Models","title":"Comparison: DFM vs GDFM","text":"Aspect Dynamic Factor Model Generalized DFM\nApproach Time domain (PCA + VAR) Frequency domain (spectral)\nFactor dynamics Explicit VAR structure Implicit through spectral density\nEstimation Two-step or EM Kernel-smoothed periodogram\nComputational cost Moderate Higher (FFT at each frequency)\nAsymptotics T to infty N T to infty jointly\nBest for Moderate N, focus on forecasting Large N, structural decomposition","category":"section"},{"location":"factormodels/#Example:-Complete-GDFM-Workflow","page":"Factor Models","title":"Example: Complete GDFM Workflow","text":"using MacroEconometricModels\n\n# Load and transform FRED-MD panel\nfred = load_example(:fred_md)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\n# Step 1: Select number of factors\nic = ic_criteria_gdfm(X, 10)\nq = ic.q_ratio\nprintln(\"Selected q = $q dynamic factors\")\n\n# Step 2: Estimate GDFM\nmodel = estimate_gdfm(X, q; kernel=:parzen)\n\n# Step 3: Diagnostics\nprintln(\"Variance explained: \", round.(model.variance_explained, digits=3))\nprintln(\"Mean R²: \", round(mean(r2(model)), digits=3))\n\n# Step 4: Extract common component for further analysis\nχ = model.common_component  # Use in FAVAR, forecasting, etc.\n\n# Step 5: Identify variables driven by common vs idiosyncratic shocks\nshares = common_variance_share(model)\ncommon_driven = findall(shares .> 0.7)\nidio_driven = findall(shares .< 0.3)\n\nReferences:\n\nForni, M., Hallin, M., Lippi, M., & Reichlin, L. (2000). \"The Generalized Dynamic-Factor Model: Identification and Estimation.\"\nForni, M., Hallin, M., Lippi, M., & Reichlin, L. (2005). \"The Generalized Dynamic Factor Model: One-Sided Estimation and Forecasting.\"\nHallin, M., & Liška, R. (2007). \"Determining the Number of Factors in the General Dynamic Factor Model.\"\n\n","category":"section"},{"location":"factormodels/#StatsAPI-Interface","page":"Factor Models","title":"StatsAPI Interface","text":"All three factor model types implement the standard StatsAPI interface, enabling uniform access to fitted values, residuals, and model diagnostics.\n\nFunction FactorModel DynamicFactorModel GeneralizedDynamicFactorModel\npredict(m) Fitted values hatX = FLambda Fitted values hatX = FLambda Common component hatchi_t\nresiduals(m) Idiosyncratic residuals Idiosyncratic residuals Idiosyncratic component\nr2(m) Per-variable R^2 Per-variable R^2 Per-variable R^2\nnobs(m) Number of observations Number of observations Number of observations\ndof(m) Degrees of freedom Degrees of freedom Degrees of freedom\nloglikelihood(m) — Log-likelihood —\naic(m) — AIC —\nbic(m) — BIC —\n\nnote: Technical Note\nloglikelihood, aic, and bic are only available for DynamicFactorModel since static PCA and spectral GDFM estimation do not produce a well-defined likelihood.\n\nusing MacroEconometricModels\n\n# Load and transform FRED-MD panel (using X from above)\n\n# Static factor model\nfm = estimate_factors(X, 3)\nX_hat = predict(fm)                # T × N fitted values\nresid = residuals(fm)              # T × N residuals\nr2_vals = r2(fm)                   # N-vector of R² values\nprintln(\"Static FM — nobs: \", nobs(fm), \", dof: \", dof(fm))\nprintln(\"Mean R²: \", round(mean(r2_vals), digits=3))\n\n# Dynamic factor model\ndfm = estimate_dynamic_factors(X, 3, 1; method=:twostep)\nprintln(\"\\nDFM — nobs: \", nobs(dfm), \", dof: \", dof(dfm))\nprintln(\"Log-likelihood: \", round(loglikelihood(dfm), digits=1))\nprintln(\"AIC: \", round(aic(dfm), digits=1), \", BIC: \", round(bic(dfm), digits=1))\nprintln(\"Mean R²: \", round(mean(r2(dfm)), digits=3))\n\n# GDFM\ngdfm = estimate_gdfm(X, 3)\nprintln(\"\\nGDFM — nobs: \", nobs(gdfm), \", dof: \", dof(gdfm))\nprintln(\"Mean R²: \", round(mean(r2(gdfm)), digits=3))\n\nThe interface is useful for model comparison:\n\n# Compare DFM specifications via information criteria on FRED-MD data\nfor r in 1:5\n    m = estimate_dynamic_factors(X, r, 1; method=:twostep)\n    println(\"r=$r: AIC=$(round(aic(m), digits=1)), BIC=$(round(bic(m), digits=1))\")\nend\n\n","category":"section"},{"location":"factormodels/#References","page":"Factor Models","title":"References","text":"","category":"section"},{"location":"factormodels/#Core-References","page":"Factor Models","title":"Core References","text":"Bai, Jushan. 2003. \"Inferential Theory for Factor Models of Large Dimensions.\" Econometrica 71 (1): 135–171. https://doi.org/10.1111/1468-0262.00392\nBai, Jushan, and Serena Ng. 2002. \"Determining the Number of Factors in Approximate Factor Models.\" Econometrica 70 (1): 191–221. https://doi.org/10.1111/1468-0262.00273\nBai, Jushan, and Serena Ng. 2006. \"Confidence Intervals for Diffusion Index Forecasts and Inference for Factor-Augmented Regressions.\" Econometrica 74 (4): 1133–1150. https://doi.org/10.1111/j.1468-0262.2006.00696.x\nStock, James H., and Mark W. Watson. 2002a. \"Forecasting Using Principal Components from a Large Number of Predictors.\" Journal of the American Statistical Association 97 (460): 1167–1179. https://doi.org/10.1198/016214502388618960\nStock, James H., and Mark W. Watson. 2002b. \"Macroeconomic Forecasting Using Diffusion Indexes.\" Journal of Business & Economic Statistics 20 (2): 147–162. https://doi.org/10.1198/073500102317351921","category":"section"},{"location":"factormodels/#Dynamic-Factor-Models-2","page":"Factor Models","title":"Dynamic Factor Models","text":"Doz, Catherine, Domenico Giannone, and Lucrezia Reichlin. 2011. \"A Two-Step Estimator for Large Approximate Dynamic Factor Models Based on Kalman Filtering.\" Journal of Econometrics 164 (1): 188–205. https://doi.org/10.1016/j.jeconom.2011.02.012\nDoz, Catherine, Domenico Giannone, and Lucrezia Reichlin. 2012. \"A Quasi-Maximum Likelihood Approach for Large, Approximate Dynamic Factor Models.\" Review of Economics and Statistics 94 (4): 1014–1024. https://doi.org/10.1162/RESTa00225\nForni, Mario, Marc Hallin, Marco Lippi, and Lucrezia Reichlin. 2000. \"The Generalized Dynamic-Factor Model: Identification and Estimation.\" Review of Economics and Statistics 82 (4): 540–554. https://doi.org/10.1162/003465300559037\nForni, Mario, Marc Hallin, Marco Lippi, and Lucrezia Reichlin. 2005. \"The Generalized Dynamic Factor Model: One-Sided Estimation and Forecasting.\" Journal of the American Statistical Association 100 (471): 830–840. https://doi.org/10.1198/016214504000002050\nHallin, Marc, and Roman Liška. 2007. \"Determining the Number of Factors in the General Dynamic Factor Model.\" Journal of the American Statistical Association 102 (478): 603–617. https://doi.org/10.1198/016214506000001275","category":"section"},{"location":"factormodels/#Applications-2","page":"Factor Models","title":"Applications","text":"Bernanke, Ben S., Jean Boivin, and Piotr Eliasz. 2005. \"Measuring the Effects of Monetary Policy: A Factor-Augmented Vector Autoregressive (FAVAR) Approach.\" Quarterly Journal of Economics 120 (1): 387–422. https://doi.org/10.1162/0033553053327452\nMcCracken, Michael W., and Serena Ng. 2016. \"FRED-MD: A Monthly Database for Macroeconomic Research.\" Journal of Business & Economic Statistics 34 (4): 574–589. https://doi.org/10.1080/07350015.2015.1086655","category":"section"},{"location":"hypothesis_tests/#Hypothesis-Tests","page":"Hypothesis Tests","title":"Hypothesis Tests","text":"This chapter covers statistical hypothesis tests for time series analysis: unit root tests for stationarity detection, cointegration tests for multivariate equilibrium relationships, model-specific specification tests, and generic model comparison tests.\n\nBefore fitting dynamic models like VARs or Local Projections, it is essential to understand the stationarity properties of the data. Non-stationary series (those with unit roots) require different treatment than stationary series, as standard regression methods can lead to spurious results. After estimation, specification tests help validate the model and explore causal relationships.","category":"section"},{"location":"hypothesis_tests/#Quick-Start","page":"Hypothesis Tests","title":"Quick Start","text":"using MacroEconometricModels\n\n# --- Pre-estimation: Unit root & stationarity ---\nfred = load_example(:fred_md)\ncpi = to_vector(fred[:, \"CPIAUCSL\"])\ncpi_clean = filter(isfinite, cpi)                         # CPI price level (I(1))\nadf_result = adf_test(cpi_clean; lags=:aic, regression=:constant)\nkpss_result = kpss_test(cpi_clean; regression=:constant)\npp_result = pp_test(cpi_clean; regression=:constant)\n\n# --- Pre-estimation: Cointegration ---\nqd = load_example(:fred_qd)\nY_coint = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\"]]))\nY_coint = Y_coint[all.(isfinite, eachrow(Y_coint)), :]\njohansen_result = johansen_test(Y_coint, 2; deterministic=:constant)\n\n# --- Post-estimation: VAR specification tests ---\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nm = estimate_var(Y, 2)\nis_stationary(m)                     # VAR stability check\ngranger_test(m, 3, 1)               # Does FFR Granger-cause INDPRO?\ngranger_test_all(m)                  # all-pairs causality matrix\n\n# --- Post-estimation: Model comparison ---\nm1 = estimate_var(Y, 1)\nlr_test(m1, m)                       # likelihood ratio test","category":"section"},{"location":"hypothesis_tests/#Test-Summary","page":"Hypothesis Tests","title":"Test Summary","text":"Unit Root Tests (pre-estimation)\n\nADF (Augmented Dickey-Fuller): Tests the null of a unit root against stationarity\nKPSS: Tests the null of stationarity against a unit root\nPhillips-Perron: Non-parametric unit root test with autocorrelation correction\nZivot-Andrews: Unit root test allowing for endogenous structural break\nNg-Perron: Modified tests with improved size properties\n\nCointegration Tests (pre-estimation)\n\nJohansen Cointegration: Tests for cointegrating relationships among variables\n\nVAR (post-estimation)\n\nStationarity Check: Eigenvalue stability of companion matrix\nGranger Causality: Pairwise and block Wald tests for predictive causality\n\nPanel VAR (post-estimation)\n\nHansen J-Test: Overidentifying restrictions test for GMM instruments\nAndrews-Lu MMSC: Model and moment selection criteria\nLag Selection: Optimal lag order via MMSC comparison\n\nModel Comparison Tests (post-estimation, generic)\n\nLikelihood Ratio (LR): Compares nested models via log-likelihood difference\nLagrange Multiplier (LM): Score-based test requiring only the restricted model\n\n","category":"section"},{"location":"hypothesis_tests/#Unit-Root-Tests","page":"Hypothesis Tests","title":"Unit Root Tests","text":"","category":"section"},{"location":"hypothesis_tests/#Augmented-Dickey-Fuller-Test","page":"Hypothesis Tests","title":"Augmented Dickey-Fuller Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory","page":"Hypothesis Tests","title":"Theory","text":"The Augmented Dickey-Fuller (ADF) test examines whether a time series has a unit root. Consider the autoregressive model:\n\ny_t = rho y_t-1 + u_t\n\nThe null hypothesis is H_0 rho = 1 (unit root) against H_1 rho  1 (stationary).\n\nThe test is performed via the regression:\n\nDelta y_t = alpha + beta t + gamma y_t-1 + sum_j=1^p delta_j Delta y_t-j + varepsilon_t\n\nwhere:\n\ngamma = rho - 1 is the coefficient of interest\nalpha is an optional constant\nbeta t is an optional linear trend\nLagged differences are included to control for serial correlation\n\nThe ADF statistic is the t-ratio tau = hatgamma  textse(hatgamma).\n\nCritical values depend on the specification (none, constant, or trend) and are tabulated using MacKinnon (1994, 2010) response surfaces.\n\nReference: Dickey & Fuller (1979), MacKinnon (2010)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# CPI price level — expected to have a unit root\nfred = load_example(:fred_md)\ncpi = filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))\n\n# ADF test with automatic lag selection via AIC\nresult = adf_test(cpi; lags=:aic, regression=:constant)\n\n# The result displays with publication-quality formatting:\n# - Test statistic and significance stars\n# - Critical values at 1%, 5%, 10% levels\n# - Automatic conclusion (expect: fail to reject unit root)","category":"section"},{"location":"hypothesis_tests/#Function-Signature","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options","page":"Hypothesis Tests","title":"Options","text":"Argument Description Default\nlags Number of augmenting lags, or :aic/:bic/:hqic for automatic selection :aic\nmax_lags Maximum lags for automatic selection floor(12*(T/100)^0.25)\nregression Deterministic terms: :none, :constant, or :trend :constant","category":"section"},{"location":"hypothesis_tests/#ADFResult-Return-Values","page":"Hypothesis Tests","title":"ADFResult Return Values","text":"Field Type Description\nstatistic T ADF test statistic (tau-ratio)\npvalue T Asymptotic p-value (MacKinnon response surface)\nlags Int Number of augmenting lags used\nregression Symbol Deterministic specification (:none, :constant, :trend)\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10% significance levels\nnobs Int Number of observations used","category":"section"},{"location":"hypothesis_tests/#Interpreting-Results","page":"Hypothesis Tests","title":"Interpreting Results","text":"Reject H₀ (p-value < 0.05): Evidence against unit root; series appears stationary\nFail to reject H₀ (p-value > 0.05): Cannot reject unit root; series may be non-stationary\n\n","category":"section"},{"location":"hypothesis_tests/#KPSS-Stationarity-Test","page":"Hypothesis Tests","title":"KPSS Stationarity Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-2","page":"Hypothesis Tests","title":"Theory","text":"The KPSS test (Kwiatkowski, Phillips, Schmidt & Shin, 1992) reverses the hypotheses of the ADF test:\n\nH_0: Series is stationary (level or trend stationary)\nH_1: Series has a unit root\n\nThis complementary approach is valuable because failure to reject in the ADF test does not confirm stationarity—it may simply reflect low power.\n\nThe test decomposes the series:\n\ny_t = xi t + r_t + varepsilon_t\n\nwhere r_t = r_t-1 + u_t is a random walk. Under H_0, the variance of u_t is zero.\n\nThe KPSS statistic is:\n\ntextKPSS = fracsum_t=1^T S_t^2T^2 hatsigma^2_LR\n\nwhere S_t = sum_s=1^t hate_s are partial sums of residuals and hatsigma^2_LR is the long-run variance estimated using a Bartlett kernel.\n\nReference: Kwiatkowski et al. (1992)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-2","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# CPI inflation rate (Δlog CPI) — expected stationary\nfred = load_example(:fred_md)\ncpi_growth = diff(log.(filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))))\nresult = kpss_test(cpi_growth; regression=:constant)\n\n# For trend stationarity\nresult_trend = kpss_test(cpi_growth; regression=:trend)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-2","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-2","page":"Hypothesis Tests","title":"Options","text":"Argument Description Default\nregression Stationarity type: :constant (level) or :trend :constant\nbandwidth Bartlett kernel bandwidth, or :auto for Newey-West selection :auto","category":"section"},{"location":"hypothesis_tests/#KPSSResult-Return-Values","page":"Hypothesis Tests","title":"KPSSResult Return Values","text":"Field Type Description\nstatistic T KPSS test statistic\npvalue T Asymptotic p-value\nregression Symbol Stationarity type (:constant or :trend)\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10%\nbandwidth Int Bartlett kernel bandwidth used\nnobs Int Number of observations","category":"section"},{"location":"hypothesis_tests/#Interpreting-Results-2","page":"Hypothesis Tests","title":"Interpreting Results","text":"Reject H₀ (p-value < 0.05): Evidence against stationarity; series has a unit root\nFail to reject H₀ (p-value > 0.05): Cannot reject stationarity","category":"section"},{"location":"hypothesis_tests/#Combining-ADF-and-KPSS","page":"Hypothesis Tests","title":"Combining ADF and KPSS","text":"Using both tests together provides stronger inference:\n\nADF Result KPSS Result Conclusion\nReject H₀ (stationary) Fail to reject H₀ (stationary) Stationary\nFail to reject H₀ (unit root) Reject H₀ (unit root) Unit root\nReject H₀ Reject H₀ Conflicting (possible structural break)\nFail to reject H₀ Fail to reject H₀ Inconclusive\n\n","category":"section"},{"location":"hypothesis_tests/#Phillips-Perron-Test","page":"Hypothesis Tests","title":"Phillips-Perron Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-3","page":"Hypothesis Tests","title":"Theory","text":"The Phillips-Perron (PP) test is a non-parametric alternative to the ADF test. Instead of augmenting with lagged differences, the PP test corrects the t-statistic for serial correlation using Newey-West standard errors.\n\nThe regression is:\n\ny_t = alpha + rho y_t-1 + u_t\n\nThe PP Z_t statistic adjusts the OLS t-ratio:\n\nZ_t = sqrtfrachatgamma_0hatlambda^2 t_rho - frachatlambda^2 - hatgamma_02hatlambda cdot textse(hatrho) cdot sqrtT\n\nwhere hatgamma_0 is the short-run variance and hatlambda^2 is the long-run variance.\n\nAdvantage: Does not require specifying the number of lags.\n\nReference: Phillips & Perron (1988)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-3","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# CPI price level — non-parametric unit root test\nfred = load_example(:fred_md)\ncpi = filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))\nresult = pp_test(cpi; regression=:constant)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-3","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-3","page":"Hypothesis Tests","title":"Options","text":"Argument Description Default\nregression Deterministic terms: :none, :constant, or :trend :constant\nbandwidth Newey-West bandwidth, or :auto :auto","category":"section"},{"location":"hypothesis_tests/#PPResult-Return-Values","page":"Hypothesis Tests","title":"PPResult Return Values","text":"Field Type Description\nstatistic T Phillips-Perron Z_t test statistic\npvalue T Asymptotic p-value\nregression Symbol Deterministic specification\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10%\nbandwidth Int Newey-West bandwidth used\nnobs Int Number of observations\n\n","category":"section"},{"location":"hypothesis_tests/#Zivot-Andrews-Test","page":"Hypothesis Tests","title":"Zivot-Andrews Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-4","page":"Hypothesis Tests","title":"Theory","text":"The Zivot-Andrews test extends the ADF test by allowing for an endogenous structural break in the series. This is important because standard unit root tests have low power against stationary alternatives with structural breaks.\n\nThree specifications are available:\n\nBreak in intercept (:constant):\n\nDelta y_t = alpha + beta t + theta DU_t + gamma y_t-1 + sum_j delta_j Delta y_t-j + varepsilon_t\n\nBreak in trend (:trend):\n\nDelta y_t = alpha + beta t + phi DT_t + gamma y_t-1 + sum_j delta_j Delta y_t-j + varepsilon_t\n\nBreak in both (:both):\n\nDelta y_t = alpha + beta t + theta DU_t + phi DT_t + gamma y_t-1 + sum_j delta_j Delta y_t-j + varepsilon_t\n\nwhere:\n\nDU_t = 1 if t  T_B (level shift dummy)\nDT_t = t - T_B if t  T_B (trend shift dummy)\nT_B is the break point, selected to minimize the t-statistic on gamma\n\nReference: Zivot & Andrews (1992)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-4","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# CPI price level — test for unit root allowing a structural break\nfred = load_example(:fred_md)\ncpi = filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))\nresult = za_test(cpi; regression=:constant)\n\n# Access break point\nprintln(\"Break detected at observation: \", result.break_index)\nprintln(\"Break location: \", result.break_fraction * 100, \"% of sample\")","category":"section"},{"location":"hypothesis_tests/#Function-Signature-4","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-4","page":"Hypothesis Tests","title":"Options","text":"Argument Description Default\nregression Break type: :constant, :trend, or :both :both\ntrim Trimming fraction for break search 0.15\nlags Augmenting lags, or :aic/:bic :aic","category":"section"},{"location":"hypothesis_tests/#ZAResult-Return-Values","page":"Hypothesis Tests","title":"ZAResult Return Values","text":"Field Type Description\nstatistic T Minimum ADF t-statistic over break candidates\npvalue T Asymptotic p-value\nbreak_index Int Estimated break point (observation index)\nbreak_fraction T Break location as fraction of sample (0 to 1)\nregression Symbol Break type (:constant, :trend, :both)\ncritical_values Dict{Int,T} Critical values at 1%, 5%, 10%\nlags Int Number of augmenting lags\nnobs Int Number of observations\n\n","category":"section"},{"location":"hypothesis_tests/#Ng-Perron-Tests","page":"Hypothesis Tests","title":"Ng-Perron Tests","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-5","page":"Hypothesis Tests","title":"Theory","text":"The Ng-Perron tests (2001) are modified unit root tests with improved size and power properties, especially in small samples. They use GLS detrending and report four test statistics:\n\nMZα: Modified Phillips Zα statistic\nMZt: Modified Phillips Zt statistic (most commonly used)\nMSB: Modified Sargan-Bhargava statistic\nMPT: Modified Point-optimal statistic\n\nThe GLS detrending uses the quasi-difference:\n\ntildey_t = y_t - barcT cdot y_t-1\n\nwhere barc = -7 (constant) or barc = -135 (trend).\n\nAdvantage: Better size properties than ADF when the initial condition is far from zero.\n\nReference: Ng & Perron (2001)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-5","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# CPI price level — GLS-detrended unit root tests\nfred = load_example(:fred_md)\ncpi = filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))\nresult = ngperron_test(cpi; regression=:constant)\n\n# All four statistics are reported\nprintln(\"MZα: \", result.MZa)\nprintln(\"MZt: \", result.MZt)\nprintln(\"MSB: \", result.MSB)\nprintln(\"MPT: \", result.MPT)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-5","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#NgPerronResult-Return-Values","page":"Hypothesis Tests","title":"NgPerronResult Return Values","text":"Field Type Description\nMZa T Modified Phillips Z_alpha statistic\nMZt T Modified Phillips Z_t statistic (most commonly reported)\nMSB T Modified Sargan-Bhargava statistic\nMPT T Modified Point-optimal statistic\nregression Symbol Deterministic specification\ncritical_values Dict{Symbol,Dict{Int,T}} Critical values keyed by statistic name (:MZa, :MZt, :MSB, :MPT)\nnobs Int Number of observations\n\nnote: Technical Note\nThe Ng-Perron tests use GLS detrending which provides substantially better size properties than the standard ADF test in small samples (T  100). When the ADF test has borderline results, the Ng-Perron MZt statistic is a more reliable indicator. However, ADF remains preferable when the data-generating process has a large negative MA root, as GLS-based tests can be oversized in that case (Perron & Ng, 1996).\n\n","category":"section"},{"location":"hypothesis_tests/#Convenience-Functions","page":"Hypothesis Tests","title":"Convenience Functions","text":"","category":"section"},{"location":"hypothesis_tests/#Summary-of-Multiple-Tests","page":"Hypothesis Tests","title":"Summary of Multiple Tests","text":"using MacroEconometricModels\n\n# CPI price level — comprehensive unit root summary\nfred = load_example(:fred_md)\ncpi = filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))\n\n# Run multiple tests and get summary\nsummary = unit_root_summary(cpi; tests=[:adf, :kpss, :pp])\n\n# Access individual results\nsummary.results[:adf]\nsummary.results[:kpss]\n\n# Overall conclusion\nprintln(summary.conclusion)","category":"section"},{"location":"hypothesis_tests/#Test-All-Variables","page":"Hypothesis Tests","title":"Test All Variables","text":"using MacroEconometricModels\n\n# Test all FRED-MD variables in a subset for unit roots\nfred = load_example(:fred_md)\nvars = fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\", \"UNRATE\", \"M2SL\"]]\nY = to_matrix(vars)\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Apply ADF test to all columns\nresults = test_all_variables(Y; test=:adf)\n\n# Check which variables have unit roots\nvarnames = [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\", \"UNRATE\", \"M2SL\"]\nfor (i, r) in enumerate(results)\n    status = r.pvalue > 0.05 ? \"I(1)\" : \"I(0)\"\n    println(\"$(varnames[i]): p=$(round(r.pvalue, digits=3)) → $status\")\nend","category":"section"},{"location":"hypothesis_tests/#Function-Signatures","page":"Hypothesis Tests","title":"Function Signatures","text":"","category":"section"},{"location":"hypothesis_tests/#Cointegration-Tests","page":"Hypothesis Tests","title":"Cointegration Tests","text":"","category":"section"},{"location":"hypothesis_tests/#Johansen-Cointegration-Test","page":"Hypothesis Tests","title":"Johansen Cointegration Test","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-6","page":"Hypothesis Tests","title":"Theory","text":"The Johansen test examines whether multiple I(1) series share common stochastic trends, i.e., are cointegrated. Consider a VAR(p) in levels:\n\ny_t = A_1 y_t-1 + cdots + A_p y_t-p + u_t\n\nThis can be rewritten in Vector Error Correction Model (VECM) form:\n\nDelta y_t = Pi y_t-1 + sum_i=1^p-1 Gamma_i Delta y_t-i + u_t\n\nwhere Pi = alpha beta is the long-run matrix:\n\nbeta: Cointegrating vectors (equilibrium relationships)\nalpha: Adjustment coefficients (speed of adjustment to equilibrium)\ntextrank(Pi) = r: Number of cointegrating relationships\n\nTwo test statistics are computed:\n\nTrace Test: Tests H_0 textrank leq r against H_1 textrank  r\n\nlambda_trace(r) = -T sum_i=r+1^n ln(1 - hatlambda_i)\n\nMaximum Eigenvalue Test: Tests H_0 textrank = r against H_1 textrank = r+1\n\nlambda_max(r) = -T ln(1 - hatlambda_r+1)\n\nReference: Johansen (1991), Osterwald-Lenum (1992)","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-6","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Test cointegration between log real GDP and log real consumption (FRED-QD)\nqd = load_example(:fred_qd)\nY = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Johansen test with 2 lags in VECM\nresult = johansen_test(Y, 2; deterministic=:constant)\n\n# Access results\nprintln(\"Estimated cointegration rank: \", result.rank)\nprintln(\"Cointegrating vectors:\\n\", result.eigenvectors[:, 1:result.rank])\nprintln(\"Adjustment coefficients:\\n\", result.adjustment)","category":"section"},{"location":"hypothesis_tests/#Function-Signature-6","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#Options-5","page":"Hypothesis Tests","title":"Options","text":"Argument Description Default\np Lags in VECM representation Required\ndeterministic :none, :constant, or :trend :constant","category":"section"},{"location":"hypothesis_tests/#JohansenResult-Return-Values","page":"Hypothesis Tests","title":"JohansenResult Return Values","text":"Field Type Description\ntrace_stats Vector{T} Trace test statistics for each rank hypothesis\ntrace_pvalues Vector{T} P-values for trace statistics\nmax_eigen_stats Vector{T} Maximum eigenvalue test statistics\nmax_eigen_pvalues Vector{T} P-values for max eigenvalue statistics\nrank Int Estimated cointegration rank\neigenvectors Matrix{T} n times n matrix of cointegrating vectors (columns)\nadjustment Matrix{T} n times n adjustment (loading) matrix alpha\neigenvalues Vector{T} Ordered eigenvalues from reduced-rank regression\ncritical_values_trace Matrix{T} n times 3 critical values for trace test (1%, 5%, 10%)\ncritical_values_max Matrix{T} n times 3 critical values for max eigenvalue test\ndeterministic Symbol Deterministic specification (:none, :constant, :trend)\nlags Int Number of VECM lags\nnobs Int Number of observations","category":"section"},{"location":"hypothesis_tests/#Interpreting-Results-3","page":"Hypothesis Tests","title":"Interpreting Results","text":"The test sequentially tests:\n\nH_0 r = 0 (no cointegration)\nH_0 r leq 1\nH_0 r leq 2, etc.\n\nStop at the first non-rejected hypothesis; that gives the cointegration rank.\n\n","category":"section"},{"location":"hypothesis_tests/#VAR","page":"Hypothesis Tests","title":"VAR","text":"","category":"section"},{"location":"hypothesis_tests/#Stationarity-Check","page":"Hypothesis Tests","title":"Stationarity Check","text":"","category":"section"},{"location":"hypothesis_tests/#Theory-7","page":"Hypothesis Tests","title":"Theory","text":"A VAR(p) model is stable (stationary) if and only if all eigenvalues of the companion matrix lie strictly inside the unit circle:\n\nF = beginbmatrix\nA_1  A_2  cdots  A_p-1  A_p \nI_n  0  cdots  0  0 \n0  I_n  cdots  0  0 \nvdots   ddots   vdots \n0  0  cdots  I_n  0\nendbmatrix\n\nStability Condition: lambda_i  1 for all eigenvalues lambda_i of F.\n\nIf violated, the VAR is explosive or contains unit roots, and standard asymptotic theory does not apply.","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-7","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Estimate VAR on stationary-transformed FRED-MD data\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\n\n# Check stationarity\nresult = is_stationary(model)\n\nif result.is_stationary\n    println(\"VAR is stationary\")\n    println(\"Maximum eigenvalue modulus: \", result.max_modulus)\nelse\n    println(\"WARNING: VAR is non-stationary!\")\n    println(\"Maximum eigenvalue modulus: \", result.max_modulus)\n    println(\"Consider differencing or VECM specification\")\nend","category":"section"},{"location":"hypothesis_tests/#Function-Signature-7","page":"Hypothesis Tests","title":"Function Signature","text":"","category":"section"},{"location":"hypothesis_tests/#VARStationarityResult-Return-Values","page":"Hypothesis Tests","title":"VARStationarityResult Return Values","text":"Field Type Description\nis_stationary Bool true if all eigenvalues lie inside unit circle\neigenvalues Vector{E} Eigenvalues of the companion matrix (may be complex)\nmax_modulus T Maximum eigenvalue modulus (should be  1 for stability)\ncompanion_matrix Matrix{T} np times np companion matrix\n\n","category":"section"},{"location":"hypothesis_tests/#Granger-Causality-Tests","page":"Hypothesis Tests","title":"Granger Causality Tests","text":"The Granger causality test (Granger 1969) examines whether lagged values of one variable help predict another variable in a VAR system.","category":"section"},{"location":"hypothesis_tests/#Theory-8","page":"Hypothesis Tests","title":"Theory","text":"Given a VAR(p) model with n variables, the pairwise test examines whether variable j Granger-causes variable i:\n\nH_0 A_1ij = A_2ij = cdots = A_pij = 0\n\nUnder H_0, the Wald statistic W = boldsymbolthetamathbfV^-1boldsymboltheta sim chi^2(p), where boldsymboltheta collects the lag coefficients and mathbfV = sigma_ii (mathbfXmathbfX)^-1 is the coefficient covariance.\n\nThe block (multivariate) test generalizes to groups of cause variables, with textdf = p times textcause group.\n\nnote: Technical Note\nGranger causality is a statistical concept based on predictability, not true causation. Variable j \"Granger-causes\" variable i if past values of j contain information useful for predicting i beyond what is contained in past values of i and other variables. The test is valid under the assumption that the VAR model is correctly specified and the error terms are white noise.","category":"section"},{"location":"hypothesis_tests/#Quick-Start-2","page":"Hypothesis Tests","title":"Quick Start","text":"using MacroEconometricModels\n\n# FRED-MD monetary policy VAR: INDPRO, CPIAUCSL, FEDFUNDS\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nm = estimate_var(Y, 2)\n\n# Pairwise: does the federal funds rate Granger-cause industrial production?\ng = granger_test(m, 3, 1)\n\n# Block: do CPIAUCSL and FEDFUNDS jointly Granger-cause INDPRO?\ng_block = granger_test(m, [2, 3], 1)\n\n# All pairwise tests at once\nresults = granger_test_all(m)\n\nInterpretation. If the p-value is below your significance level (e.g., 0.05), reject H_0 and conclude the cause variable(s) Granger-cause the effect variable. The granger_test_all function returns an n×n matrix where entry [i,j] tests whether variable j Granger-causes variable i.","category":"section"},{"location":"hypothesis_tests/#Function-Signatures-2","page":"Hypothesis Tests","title":"Function Signatures","text":"granger_test(model, cause, effect) — Pairwise or block Granger causality test\ngranger_test_all(model) — All-pairs pairwise Granger causality matrix","category":"section"},{"location":"hypothesis_tests/#Return-Values","page":"Hypothesis Tests","title":"Return Values","text":"GrangerCausalityResult\n\nField Type Description\nstatistic T Wald χ² statistic\npvalue T p-value from χ²(df)\ndf Int Degrees of freedom (p for pairwise, p×|cause| for block)\ncause Vector{Int} Indices of causing variable(s)\neffect Int Index of effect variable\nn Int Number of variables in VAR\np Int Lag order\nnobs Int Effective number of observations\ntest_type Symbol :pairwise or :block","category":"section"},{"location":"hypothesis_tests/#Complete-Example","page":"Hypothesis Tests","title":"Complete Example","text":"using MacroEconometricModels\n\n# FRED-MD monetary policy VAR: INDPRO (1), CPIAUCSL (2), FEDFUNDS (3)\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nm = estimate_var(Y, 2)\n\n# Test all pairs\nresults = granger_test_all(m)\n\n# Does FFR Granger-cause industrial production?\nprintln(\"FEDFUNDS → INDPRO:   p = \", round(results[1, 3].pvalue, digits=4))\n\n# Does industrial production Granger-cause CPI?\nprintln(\"INDPRO → CPIAUCSL:   p = \", round(results[2, 1].pvalue, digits=4))\n\n# Does CPI Granger-cause the federal funds rate?\nprintln(\"CPIAUCSL → FEDFUNDS: p = \", round(results[3, 2].pvalue, digits=4))\n\n# Block test: do INDPRO and CPIAUCSL jointly Granger-cause FEDFUNDS?\ng_block = granger_test(m, [1, 2], 3)\nprintln(\"Block [INDPRO,CPIAUCSL] → FEDFUNDS: p = \", round(g_block.pvalue, digits=4))\n\n","category":"section"},{"location":"hypothesis_tests/#Panel-VAR","page":"Hypothesis Tests","title":"Panel VAR","text":"","category":"section"},{"location":"hypothesis_tests/#Hansen-J-Test","page":"Hypothesis Tests","title":"Hansen J-Test","text":"The Hansen (1982) J-test evaluates whether the overidentifying restrictions (moment conditions) in GMM estimation are valid.","category":"section"},{"location":"hypothesis_tests/#Theory-9","page":"Hypothesis Tests","title":"Theory","text":"J = N barg W_opt barg sim chi^2(q - k)\n\nwhere barg = N^-1 sum_i Z_i e_i is the average moment condition, W_opt is the optimal weighting matrix, q is the number of instruments, and k is the number of parameters per equation.\n\nH_0: All moment conditions are valid\nH_1: Some moment conditions are invalid","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-8","page":"Hypothesis Tests","title":"Julia Implementation","text":"using MacroEconometricModels, DataFrames, Random\nRandom.seed!(42)\n\n# Set up panel data (see Panel VAR page for full example)\nN, T_total, m = 50, 20, 3\ndata = zeros(N * T_total, m)\nfor i in 1:N\n    mu = randn(m) * 0.5\n    for t in 2:T_total\n        idx = (i-1)*T_total + t\n        data[idx, :] = mu + 0.5 * data[(i-1)*T_total + t - 1, :] + 0.2 * randn(m)\n    end\nend\ndf = DataFrame(data, [\"y1\", \"y2\", \"y3\"])\ndf.id = repeat(1:N, inner=T_total)\ndf.time = repeat(1:T_total, outer=N)\npd = xtset(df, :id, :time)\n\nmodel = estimate_pvar(pd, 2; steps=:twostep)\n\n# Hansen J-test\nj = pvar_hansen_j(model)\nj.statistic     # J-statistic\nj.pvalue        # p-value (χ² distribution)\nj.df            # degrees of freedom = instruments - parameters\n\nInterpretation. Failure to reject H_0 (p-value > 0.05) supports the validity of the instruments. Rejection suggests instrument invalidity or model misspecification.\n\nwarning: J-Test and Instrument Count\nThe J-test has low power when the number of instruments is large relative to N. A non-rejection does not necessarily validate the instruments.","category":"section"},{"location":"hypothesis_tests/#Return-Values-2","page":"Hypothesis Tests","title":"Return Values","text":"Field Type Description\nstatistic T J-statistic\npvalue T p-value from χ²(df)\ndf Int Degrees of freedom (instruments − parameters)\nn_instruments Int Number of moment conditions\nn_params Int Number of estimated parameters\n\n","category":"section"},{"location":"hypothesis_tests/#Andrews-Lu-MMSC","page":"Hypothesis Tests","title":"Andrews-Lu MMSC","text":"Andrews-Lu (2001) Model and Moment Selection Criteria extend information criteria to GMM settings, enabling comparison of PVAR models with different lag orders or instrument sets.","category":"section"},{"location":"hypothesis_tests/#Theory-10","page":"Hypothesis Tests","title":"Theory","text":"textMMSC-BIC = J - (c - b) ln(n) quad\ntextMMSC-AIC = J - 2(c - b) quad\ntextMMSC-HQIC = J - Q(c - b) ln(ln(n))\n\nwhere J is the Hansen J-statistic, c = number of instruments, b = number of parameters, and n = observations. Lower values are preferred.","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-9","page":"Hypothesis Tests","title":"Julia Implementation","text":"mmsc = pvar_mmsc(model)\nmmsc.bic     # MMSC-BIC\nmmsc.aic     # MMSC-AIC\nmmsc.hqic    # MMSC-HQIC\n\nSee pvar_mmsc in the API Reference.\n\n","category":"section"},{"location":"hypothesis_tests/#Lag-Selection","page":"Hypothesis Tests","title":"Lag Selection","text":"Select the optimal PVAR lag order by estimating models for p = 1 ldots p_max and comparing Andrews-Lu MMSC criteria.","category":"section"},{"location":"hypothesis_tests/#Julia-Implementation-10","page":"Hypothesis Tests","title":"Julia Implementation","text":"sel = pvar_lag_selection(pd, 4)\nsel.best_bic    # optimal lag by BIC\nsel.best_aic    # optimal lag by AIC\nsel.best_hqic   # optimal lag by HQIC\nsel.table       # comparison table (p × 4: lag, BIC, AIC, HQIC)\n\nSee pvar_lag_selection in the API Reference.\n\n","category":"section"},{"location":"hypothesis_tests/#Model-Comparison-Tests","page":"Hypothesis Tests","title":"Model Comparison Tests","text":"The likelihood ratio (LR) test and Lagrange multiplier (LM) test form two legs of the classical \"trinity\" of specification tests (alongside the Wald test). Both test whether a restricted (simpler) model is adequate relative to an unrestricted (more general) model. These tests are generic and work across model families.","category":"section"},{"location":"hypothesis_tests/#Theory-11","page":"Hypothesis Tests","title":"Theory","text":"Given nested models mathcalM_R subset mathcalM_U with log-likelihoods ell_R and ell_U:\n\nLR test (Wilks 1938): Evaluate both models at their respective MLEs.\n\ntextLR = -2(ell_R - ell_U) xrightarrowd chi^2(textdf)\n\nLM test (Rao 1948, Silvey 1959): Evaluate the score of the unrestricted model at the restricted estimates.\n\ntextLM = mathbfs(-mathbfH)^-1mathbfs xrightarrowd chi^2(textdf)\n\nwhere textdf = k_U - k_R is the difference in the number of parameters, mathbfs is the score vector (gradient of the log-likelihood), and mathbfH is the Hessian of the log-likelihood, all evaluated at the restricted estimates.\n\nnote: Technical Note\nThe LR test requires estimating both models, while the LM test only requires the restricted model (plus the unrestricted likelihood function). Under the null, LR, LM, and Wald statistics are asymptotically equivalent. In finite samples, the ordering textWald geq textLR geq textLM typically holds.","category":"section"},{"location":"hypothesis_tests/#Quick-Start-3","page":"Hypothesis Tests","title":"Quick Start","text":"using MacroEconometricModels\n\n# --- ARIMA: Is AR(2) adequate vs AR(4) for CPI inflation? ---\nfred = load_example(:fred_md)\ncpi_growth = diff(log.(filter(isfinite, to_vector(fred[:, \"CPIAUCSL\"]))))\nar2 = estimate_ar(cpi_growth, 2; method=:mle)\nar4 = estimate_ar(cpi_growth, 4; method=:mle)\n\nlr_result = lr_test(ar2, ar4)   # generic: any model with loglikelihood\nlm_result = lm_test(ar2, ar4)   # score-based: model-family specific\n\n# --- VAR: VAR(1) vs VAR(2) for monetary policy model ---\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nvar1 = estimate_var(Y, 1)\nvar2 = estimate_var(Y, 2)\nlr_test(var1, var2)\n\n# --- Volatility: ARCH(1) vs GARCH(1,1) for INDPRO growth ---\nindpro_growth = diff(log.(filter(isfinite, to_vector(fred[:, \"INDPRO\"]))))\narch1 = estimate_arch(indpro_growth, 1)\ngarch11 = estimate_garch(indpro_growth, 1, 1)\nlr_test(arch1, garch11)     # LR works across ARCH/GARCH\nlm_test(arch1, garch11)     # LM supports ARCH→GARCH nesting\n\nInterpretation. If the p-value is below your significance level (e.g., 0.05), reject H₀ and conclude the unrestricted model provides a significantly better fit. If the p-value is large, the restricted model is adequate.","category":"section"},{"location":"hypothesis_tests/#Supported-Model-Families","page":"Hypothesis Tests","title":"Supported Model Families","text":"Test Supported pairs Notes\nlr_test Any pair with loglikelihood, dof, nobs Generic — works for VAR, VECM, ARIMA, ARCH, GARCH, EGARCH, GJR-GARCH, DFM\nlm_test AbstractARIMAModel × AbstractARIMAModel Same differencing order d required\nlm_test VARModel × VARModel Different lag orders, same data\nlm_test ARCHModel × ARCHModel Different ARCH orders\nlm_test GARCHModel × GARCHModel Different p or q\nlm_test ARCHModel × GARCHModel Cross-type nesting (ARCH ⊂ GARCH)\nlm_test EGARCHModel × EGARCHModel Different p or q\nlm_test GJRGARCHModel × GJRGARCHModel Different p or q","category":"section"},{"location":"hypothesis_tests/#Function-Signatures-3","page":"Hypothesis Tests","title":"Function Signatures","text":"lr_test(m1, m2) — Likelihood ratio test\nlm_test(m1, m2) — Lagrange multiplier (score) test","category":"section"},{"location":"hypothesis_tests/#Return-Values-3","page":"Hypothesis Tests","title":"Return Values","text":"LRTestResult\n\nField Type Description\nstatistic T LR = −2(ℓR − ℓU)\npvalue T p-value from χ²(df)\ndf Int Degrees of freedom\nloglik_restricted T Log-likelihood of restricted model\nloglik_unrestricted T Log-likelihood of unrestricted model\ndof_restricted Int Parameters in restricted model\ndof_unrestricted Int Parameters in unrestricted model\nnobs_restricted Int Observations in restricted model\nnobs_unrestricted Int Observations in unrestricted model\n\nLMTestResult\n\nField Type Description\nstatistic T LM = s'(−H)⁻¹s\npvalue T p-value from χ²(df)\ndf Int Degrees of freedom\nnobs Int Number of observations\nscore_norm T ‖s‖₂ diagnostic\n\n","category":"section"},{"location":"hypothesis_tests/#See-Also","page":"Hypothesis Tests","title":"See Also","text":"VAR Estimation – Pre-estimation diagnostics and lag selection\nVECM Analysis – Johansen cointegration testing in the VECM context\nVolatility Models – ARCH-LM test applications for volatility modeling\nAPI Reference – Complete function signatures","category":"section"},{"location":"hypothesis_tests/#Reference","page":"Hypothesis Tests","title":"Reference","text":"","category":"section"},{"location":"hypothesis_tests/#Result-Types","page":"Hypothesis Tests","title":"Result Types","text":"All test results implement the StatsAPI interface:\n\nusing StatsAPI\n\nresult = adf_test(y)\n\n# StatsAPI interface\nnobs(result)    # Number of observations\ndof(result)     # Degrees of freedom\npvalue(result)  # P-value","category":"section"},{"location":"hypothesis_tests/#Type-Hierarchy","page":"Hypothesis Tests","title":"Type Hierarchy","text":"See the API Reference for detailed type documentation.\n\nUnit Root Tests — inherit from AbstractUnitRootTest <: StatsAPI.HypothesisTest:\n\nADFResult - Augmented Dickey-Fuller test result\nKPSSResult - KPSS stationarity test result\nPPResult - Phillips-Perron test result\nZAResult - Zivot-Andrews structural break test result\nNgPerronResult - Ng-Perron test result (MZα, MZt, MSB, MPT)\nJohansenResult - Johansen cointegration test result\nVARStationarityResult - VAR model stationarity check result\n\nSpecification Tests — inherit from StatsAPI.HypothesisTest:\n\nGrangerCausalityResult - Granger causality test result (pairwise or block)\nPVARTestResult - Panel VAR specification test result (Hansen J, etc.)\nLRTestResult - Likelihood ratio test result\nLMTestResult - Lagrange multiplier (score) test result\n\n","category":"section"},{"location":"hypothesis_tests/#Practical-Workflow","page":"Hypothesis Tests","title":"Practical Workflow","text":"","category":"section"},{"location":"hypothesis_tests/#Step-by-Step-Unit-Root-Analysis","page":"Hypothesis Tests","title":"Step-by-Step Unit Root Analysis","text":"using MacroEconometricModels\n\n# 1. Load/generate data\ny = your_time_series\n\n# 2. Visual inspection (plot the series)\n# Look for trends, structural breaks, etc.\n\n# 3. Test for unit root with ADF\nadf_result = adf_test(y; regression=:constant)\n\n# 4. Confirm with KPSS (opposite null)\nkpss_result = kpss_test(y; regression=:constant)\n\n# 5. If structural break suspected, use Zivot-Andrews\nza_result = za_test(y; regression=:both)\n\n# 6. For small samples, use Ng-Perron\nnp_result = ngperron_test(y; regression=:constant)\n\n# 7. Decision matrix\nif pvalue(adf_result) < 0.05 && pvalue(kpss_result) > 0.05\n    println(\"Series is stationary - proceed with VAR in levels\")\nelseif pvalue(adf_result) > 0.05 && pvalue(kpss_result) < 0.05\n    println(\"Series has unit root - consider differencing or VECM\")\nelse\n    println(\"Inconclusive - examine further or use robust methods\")\nend","category":"section"},{"location":"hypothesis_tests/#Pre-VAR-Analysis","page":"Hypothesis Tests","title":"Pre-VAR Analysis","text":"using MacroEconometricModels\n\n# Multi-variable dataset\nY = your_data_matrix\n\n# 1. Test each variable for unit root\nresults = test_all_variables(Y; test=:adf)\nn_nonstationary = sum(r.pvalue > 0.05 for r in results)\nprintln(\"Variables with unit roots: $n_nonstationary / $(size(Y, 2))\")\n\n# 2. If all I(1), test for cointegration\nif n_nonstationary == size(Y, 2)\n    johansen_result = johansen_test(Y, 2)\n\n    if johansen_result.rank > 0\n        println(\"Cointegration detected! Use VECM with rank=$(johansen_result.rank)\")\n    else\n        println(\"No cointegration - use VAR in first differences\")\n    end\nend\n\n# 3. If mixed I(0)/I(1), be cautious\n# Consider ARDL bounds test or transform I(1) variables","category":"section"},{"location":"hypothesis_tests/#Post-Estimation-Diagnostics","page":"Hypothesis Tests","title":"Post-Estimation Diagnostics","text":"using MacroEconometricModels\n\n# After estimating VAR on FRED-MD data\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nm = estimate_var(Y, 2)\n\n# 1. Check stability\nstat = is_stationary(m)\nprintln(\"Stable: \", stat.is_stationary, \" (max modulus: \", round(stat.max_modulus, digits=3), \")\")\n\n# 2. Granger causality\nresults = granger_test_all(m)\n\n# 3. Compare lag specifications\nm1 = estimate_var(Y, 1)\nm3 = estimate_var(Y, 3)\nprintln(\"VAR(1) vs VAR(2): \", lr_test(m1, m))\nprintln(\"VAR(2) vs VAR(3): \", lr_test(m, m3))\n\n","category":"section"},{"location":"hypothesis_tests/#References","page":"Hypothesis Tests","title":"References","text":"","category":"section"},{"location":"hypothesis_tests/#Unit-Root-Tests-2","page":"Hypothesis Tests","title":"Unit Root Tests","text":"Dickey, David A., and Wayne A. Fuller. 1979. \"Distribution of the Estimators for Autoregressive Time Series with a Unit Root.\" Journal of the American Statistical Association 74 (366): 427–431. https://doi.org/10.1080/01621459.1979.10482531\nKwiatkowski, Denis, Peter C. B. Phillips, Peter Schmidt, and Yongcheol Shin. 1992. \"Testing the Null Hypothesis of Stationarity Against the Alternative of a Unit Root.\" Journal of Econometrics 54 (1–3): 159–178. https://doi.org/10.1016/0304-4076(92)90104-Y\nMacKinnon, James G. 2010. \"Critical Values for Cointegration Tests.\" Queen's Economics Department Working Paper No. 1227.\nNg, Serena, and Pierre Perron. 2001. \"Lag Length Selection and the Construction of Unit Root Tests with Good Size and Power.\" Econometrica 69 (6): 1519–1554. https://doi.org/10.1111/1468-0262.00256\nPhillips, Peter C. B., and Pierre Perron. 1988. \"Testing for a Unit Root in Time Series Regression.\" Biometrika 75 (2): 335–346. https://doi.org/10.1093/biomet/75.2.335\nZivot, Eric, and Donald W. K. Andrews. 1992. \"Further Evidence on the Great Crash, the Oil-Price Shock, and the Unit-Root Hypothesis.\" Journal of Business & Economic Statistics 10 (3): 251–270. https://doi.org/10.1080/07350015.1992.10509904","category":"section"},{"location":"hypothesis_tests/#Cointegration","page":"Hypothesis Tests","title":"Cointegration","text":"Johansen, Soren. 1991. \"Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models.\" Econometrica 59 (6): 1551–1580. https://doi.org/10.2307/2938278\nJohansen, Soren. 1995. Likelihood-Based Inference in Cointegrated Vector Autoregressive Models. Oxford: Oxford University Press. ISBN 978-0-19-877450-5.\nOsterwald-Lenum, Michael. 1992. \"A Note with Quantiles of the Asymptotic Distribution of the Maximum Likelihood Cointegration Rank Test Statistics.\" Oxford Bulletin of Economics and Statistics 54 (3): 461–472. https://doi.org/10.1111/j.1468-0084.1992.tb00013.x","category":"section"},{"location":"hypothesis_tests/#VAR-2","page":"Hypothesis Tests","title":"VAR","text":"Granger, C. W. J. 1969. \"Investigating Causal Relations by Econometric Models and Cross-spectral Methods.\" Econometrica 37 (3): 424–438. https://doi.org/10.2307/1912791","category":"section"},{"location":"hypothesis_tests/#Panel-VAR-2","page":"Hypothesis Tests","title":"Panel VAR","text":"Hansen, Lars Peter. 1982. \"Large Sample Properties of Generalized Method of Moments Estimators.\" Econometrica 50 (4): 1029–1054. https://doi.org/10.2307/1912775\nAndrews, Donald W. K., and Biao Lu. 2001. \"Consistent Model and Moment Selection Procedures for GMM Estimation.\" Journal of Econometrics 101 (1): 123–164. https://doi.org/10.1016/S0304-4076(00)00077-4","category":"section"},{"location":"hypothesis_tests/#Model-Comparison","page":"Hypothesis Tests","title":"Model Comparison","text":"Wilks, Samuel S. 1938. \"The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.\" Annals of Mathematical Statistics 9 (1): 60–62. https://doi.org/10.1214/aoms/1177732360\nNeyman, Jerzy, and Egon S. Pearson. 1933. \"On the Problem of the Most Efficient Tests of Statistical Hypotheses.\" Philosophical Transactions of the Royal Society A 231 (694–706): 289–337. https://doi.org/10.1098/rsta.1933.0009\nRao, C. Radhakrishna. 1948. \"Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.\" Mathematical Proceedings of the Cambridge Philosophical Society 44 (1): 50–57. https://doi.org/10.1017/S0305004100023987\nSilvey, S. D. 1959. \"The Lagrangian Multiplier Test.\" Annals of Mathematical Statistics 30 (2): 389–407. https://doi.org/10.1214/aoms/1177706259","category":"section"},{"location":"hypothesis_tests/#Textbooks","page":"Hypothesis Tests","title":"Textbooks","text":"Hamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nLutkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.\nEnders, Walter. 2014. Applied Econometric Time Series. 4th ed. Hoboken, NJ: Wiley. ISBN 978-1-118-80856-6.","category":"section"},{"location":"hypothesis_tests/#MacroEconometricModels.adf_test","page":"Hypothesis Tests","title":"MacroEconometricModels.adf_test","text":"adf_test(y; lags=:aic, max_lags=nothing, regression=:constant) -> ADFResult\n\nAugmented Dickey-Fuller test for unit root.\n\nTests H₀: y has a unit root (non-stationary) against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nlags: Number of augmenting lags, or :aic/:bic/:hqic for automatic selection\nmax_lags: Maximum lags for automatic selection (default: floor(12*(T/100)^0.25))\nregression: Deterministic terms - :none, :constant (default), or :trend\n\nReturns\n\nADFResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk (has unit root)\nresult = adf_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nDickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. JASA, 74(366), 427-431.\nMacKinnon, J. G. (2010). Critical values for cointegration tests. Queen's Economics Department Working Paper No. 1227.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.kpss_test","page":"Hypothesis Tests","title":"MacroEconometricModels.kpss_test","text":"kpss_test(y; regression=:constant, bandwidth=:auto) -> KPSSResult\n\nKwiatkowski-Phillips-Schmidt-Shin test for stationarity.\n\nTests H₀: y is stationary against H₁: y has a unit root.\n\nArguments\n\ny: Time series vector\nregression: :constant (level stationarity) or :trend (trend stationarity)\nbandwidth: Bartlett kernel bandwidth, or :auto for Newey-West selection\n\nReturns\n\nKPSSResult containing test statistic, p-value, critical values, etc.\n\nExample\n\ny = randn(200)  # Stationary series\nresult = kpss_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀ (stationarity)\n\nReferences\n\nKwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54(1-3), 159-178.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.pp_test","page":"Hypothesis Tests","title":"MacroEconometricModels.pp_test","text":"pp_test(y; regression=:constant, bandwidth=:auto) -> PPResult\n\nPhillips-Perron test for unit root with non-parametric correction.\n\nTests H₀: y has a unit root against H₁: y is stationary.\n\nArguments\n\ny: Time series vector\nregression: :none, :constant (default), or :trend\nbandwidth: Newey-West bandwidth, or :auto for automatic selection\n\nReturns\n\nPPResult containing test statistic (Zt), p-value, critical values, etc.\n\nExample\n\ny = cumsum(randn(200))  # Random walk\nresult = pp_test(y)\nresult.pvalue > 0.05  # Should fail to reject H₀\n\nReferences\n\nPhillips, P. C., & Perron, P. (1988). Testing for a unit root in time series regression. Biometrika, 75(2), 335-346.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.za_test","page":"Hypothesis Tests","title":"MacroEconometricModels.za_test","text":"za_test(y; regression=:both, trim=0.15, lags=:aic, max_lags=nothing) -> ZAResult\n\nZivot-Andrews test for unit root with endogenous structural break.\n\nTests H₀: y has a unit root without break against H₁: y is stationary with break.\n\nArguments\n\ny: Time series vector\nregression: Type of break - :constant (intercept), :trend (slope), or :both\ntrim: Trimming fraction for break search (default 0.15)\nlags: Number of augmenting lags, or :aic/:bic for automatic selection\nmax_lags: Maximum lags for selection\n\nReturns\n\nZAResult containing minimum t-statistic, break point, p-value, etc.\n\nExample\n\n# Series with structural break\ny = vcat(randn(100), randn(100) .+ 2)\nresult = za_test(y; regression=:constant)\n\nReferences\n\nZivot, E., & Andrews, D. W. K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. JBES, 10(3), 251-270.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.ngperron_test","page":"Hypothesis Tests","title":"MacroEconometricModels.ngperron_test","text":"ngperron_test(y; regression=:constant) -> NgPerronResult\n\nNg-Perron unit root tests with GLS detrending (MZα, MZt, MSB, MPT).\n\nTests H₀: y has a unit root against H₁: y is stationary. These tests have better size properties than ADF/PP in small samples.\n\nArguments\n\ny: Time series vector\nregression: :constant (default) or :trend\n\nReturns\n\nNgPerronResult containing MZα, MZt, MSB, MPT statistics and critical values.\n\nExample\n\ny = cumsum(randn(100))\nresult = ngperron_test(y)\n# Check if MZt rejects at 5%\nresult.MZt < result.critical_values[:MZt][5]\n\nReferences\n\nNg, S., & Perron, P. (2001). Lag length selection and the construction of unit root tests with good size and power. Econometrica, 69(6), 1519-1554.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.unit_root_summary","page":"Hypothesis Tests","title":"MacroEconometricModels.unit_root_summary","text":"unit_root_summary(y; tests=[:adf, :kpss, :pp], kwargs...) -> NamedTuple\n\nRun multiple unit root tests and return summary with PrettyTables output.\n\nArguments\n\ny: Time series vector\ntests: Vector of test symbols to run (default: [:adf, :kpss, :pp])\nkwargs...: Additional arguments passed to individual tests\n\nReturns\n\nNamedTuple with test results, conclusion, and summary table.\n\nExample\n\ny = cumsum(randn(200))\nsummary = unit_root_summary(y)\nsummary.conclusion  # Overall conclusion\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.test_all_variables","page":"Hypothesis Tests","title":"MacroEconometricModels.test_all_variables","text":"test_all_variables(Y; test=:adf, kwargs...) -> Vector\n\nApply unit root test to each column of Y.\n\nArguments\n\nY: Data matrix (T × n)\ntest: Test to apply (:adf, :kpss, :pp, :za, :ngperron)\nkwargs...: Additional arguments passed to the test\n\nReturns\n\nVector of test results, one per variable.\n\nExample\n\nY = randn(200, 3)\nY[:, 1] = cumsum(Y[:, 1])  # Make first column non-stationary\nresults = test_all_variables(Y; test=:adf)\n[r.pvalue for r in results]  # P-values for each variable\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.johansen_test","page":"Hypothesis Tests","title":"MacroEconometricModels.johansen_test","text":"johansen_test(Y, p; deterministic=:constant) -> JohansenResult\n\nJohansen cointegration test for VAR system.\n\nTests for the number of cointegrating relationships among variables using trace and maximum eigenvalue tests.\n\nArguments\n\nY: Data matrix (T × n)\np: Number of lags in the VECM representation\ndeterministic: Specification for deterministic terms\n:none - No deterministic terms\n:constant - Constant in cointegrating relation (default)\n:trend - Linear trend in levels\n\nReturns\n\nJohansenResult containing trace and max-eigenvalue statistics, cointegrating vectors, adjustment coefficients, and estimated rank.\n\nExample\n\n# Generate cointegrated system\nn, T = 3, 200\nY = randn(T, n)\nY[:, 2] = Y[:, 1] + 0.1 * randn(T)  # Y2 cointegrated with Y1\n\nresult = johansen_test(Y, 2)\nresult.rank  # Should detect 1 or 2 cointegrating relations\n\nReferences\n\nJohansen, S. (1991). Estimation and hypothesis testing of cointegration vectors in Gaussian vector autoregressive models. Econometrica, 59(6), 1551-1580.\nOsterwald-Lenum, M. (1992). A note with quantiles of the asymptotic distribution of the ML cointegration rank test statistics. Oxford BEJM.\n\n\n\n\n\n","category":"function"},{"location":"hypothesis_tests/#MacroEconometricModels.is_stationary","page":"Hypothesis Tests","title":"MacroEconometricModels.is_stationary","text":"is_stationary(model::VARModel) -> VARStationarityResult\n\nCheck if estimated VAR model is stationary.\n\nA VAR(p) is stationary if and only if all eigenvalues of the companion matrix have modulus strictly less than 1.\n\nReturns\n\nVARStationarityResult with:\n\nis_stationary: Boolean indicating stationarity\neigenvalues: Complex eigenvalues of companion matrix\nmax_modulus: Maximum eigenvalue modulus\ncompanion_matrix: The (np × np) companion form matrix\n\nExample\n\nmodel = estimate_var(Y, 2)\nresult = is_stationary(model)\nif !result.is_stationary\n    println(\"Warning: VAR is non-stationary, max modulus = \", result.max_modulus)\nend\n\n\n\n\n\nis_stationary(model::DynamicFactorModel) -> Bool\n\nCheck if factor dynamics are stationary (max |eigenvalue| < 1).\n\n\n\n\n\n","category":"function"},{"location":"innovation_accounting/#Innovation-Accounting","page":"Innovation Accounting","title":"Innovation Accounting","text":"Innovation accounting refers to the collection of tools for analyzing the dynamic effects of structural shocks in VAR models. This includes Impulse Response Functions (IRF), Forecast Error Variance Decomposition (FEVD), and Historical Decomposition (HD).","category":"section"},{"location":"innovation_accounting/#Quick-Start","page":"Innovation Accounting","title":"Quick Start","text":"irfs = irf(model, 20; method=:cholesky)                          # Frequentist IRF\nirfs_ci = irf(model, 20; ci_type=:bootstrap, reps=1000)          # With bootstrap CI\nbirfs = irf(post, 20; method=:cholesky)                          # Bayesian IRF\ndecomp = fevd(model, 20)                                         # FEVD\nhd = historical_decomposition(model, 198)                        # Historical decomposition\nreport(irfs)                                                      # Publication-quality summary\n\n","category":"section"},{"location":"innovation_accounting/#Impulse-Response-Functions-(IRF)","page":"Innovation Accounting","title":"Impulse Response Functions (IRF)","text":"","category":"section"},{"location":"innovation_accounting/#Definition","page":"Innovation Accounting","title":"Definition","text":"The impulse response function Theta_h measures the effect of a one-unit structural shock at time t on the endogenous variables at time t+h:\n\nTheta_h = fracpartial y_t+hpartial varepsilon_t\n\nwhere\n\nTheta_h is the n times n impulse response matrix at horizon h\ny_t+h is the n times 1 vector of endogenous variables at time t+h\nvarepsilon_t is the n times 1 vector of structural shocks at time t\n\nFor a VAR, the IRF at horizon h is computed recursively:\n\nTheta_h = sum_i=1^min(hp) A_i Theta_h-i\n\nwhere\n\nA_i are the n times n VAR coefficient matrices for lag i\nTheta_0 = B_0 is the n times n structural impact matrix\np is the VAR lag order","category":"section"},{"location":"innovation_accounting/#Companion-Form-Representation","page":"Innovation Accounting","title":"Companion Form Representation","text":"Using the companion form, IRFs can be computed as:\n\nTheta_h = J F^h J B_0\n\nwhere\n\nJ = I_n 0 ldots 0 is the n times np selection matrix\nF is the np times np companion matrix\nB_0 is the n times n structural impact matrix","category":"section"},{"location":"innovation_accounting/#Cumulative-IRF","page":"Innovation Accounting","title":"Cumulative IRF","text":"The cumulative impulse response up to horizon H is:\n\nTheta^cum_H = sum_h=0^H Theta_h\n\nwhere Theta^cum_H accumulates the impulse responses from impact through horizon H, measuring the total cumulative effect of a structural shock. This is particularly relevant for variables in growth rates, where the cumulative IRF represents the effect on the level.","category":"section"},{"location":"innovation_accounting/#Confidence-Intervals","page":"Innovation Accounting","title":"Confidence Intervals","text":"Bootstrap (Frequentist): Residual bootstrap of Kilian (1998):\n\nEstimate the VAR and save residuals hatu_t\nGenerate bootstrap sample by resampling residuals with replacement\nRe-estimate the VAR and compute IRFs\nRepeat B times to build the distribution\n\nCredible Intervals (Bayesian): For each posterior draw, compute IRFs and report posterior quantiles (e.g., 16th and 84th percentiles for 68% intervals).","category":"section"},{"location":"innovation_accounting/#Usage","page":"Innovation Accounting","title":"Usage","text":"using MacroEconometricModels\n\n# Load FRED-MD: industrial production, CPI inflation, federal funds rate\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\n\n# Basic IRF (Cholesky identification)\nirf_result = irf(model, 20)\n\n# With bootstrap confidence intervals\nirf_ci = irf(model, 20; ci_type=:bootstrap, reps=1000)\n\n# Sign restrictions\nsign_constraints = [1 1 0; -1 0 0; 0 0 1]\nirf_sign = irf(model, 20; method=:sign, sign_restrictions=sign_constraints)\n\nThe basic irf(model, 20) call uses Cholesky identification by default, with the ordering INDPRO → CPIAUCSL → FEDFUNDS implying that monetary policy (FFR) shocks do not contemporaneously affect output or prices. Adding ci_type=:bootstrap generates pointwise confidence bands via Kilian's (1998) residual bootstrap — reps=1000 draws are recommended for publication-quality bands. Sign restrictions produce a set of admissible IRFs satisfying the constraints; the returned values are the median (or a representative draw), with the set-identified nature reflected in wider credible bands.\n\nnote: Technical Note\nThe ci_lower and ci_upper arrays are only populated when ci_type=:bootstrap (frequentist) or when using the Bayesian irf(post, ...) method. With ci_type=:none (the default), these arrays contain zeros. Always check irf_result.ci_type before interpreting confidence bands.","category":"section"},{"location":"innovation_accounting/#ImpulseResponse-Return-Values","page":"Innovation Accounting","title":"ImpulseResponse Return Values","text":"Field Type Description\nvalues Array{T,3} (H+1) times n times n IRF array: values[h+1, i, j] = response of variable i to shock j at horizon h\nci_lower Array{T,3} Lower confidence bound (same shape as values)\nci_upper Array{T,3} Upper confidence bound\nhorizon Int Maximum IRF horizon H\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nci_type Symbol CI method used (:bootstrap, :none, etc.)","category":"section"},{"location":"innovation_accounting/#BayesianImpulseResponse-Return-Values","page":"Innovation Accounting","title":"BayesianImpulseResponse Return Values","text":"Field Type Description\nquantiles Array{T,4} (H+1) times n times n times 3: dimension 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} (H+1) times n times n posterior mean IRF\nhorizon Int Maximum IRF horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels (e.g., [0.16, 0.5, 0.84])\n\nReference: Kilian (1998), Lütkepohl (2005, Chapter 3)\n\n","category":"section"},{"location":"innovation_accounting/#Forecast-Error-Variance-Decomposition-(FEVD)","page":"Innovation Accounting","title":"Forecast Error Variance Decomposition (FEVD)","text":"","category":"section"},{"location":"innovation_accounting/#Definition-2","page":"Innovation Accounting","title":"Definition","text":"The FEVD measures the proportion of the h-step ahead forecast error variance of variable i attributable to structural shock j:\n\ntextFEVD_ij(h) = fracsum_s=0^h-1 (Theta_s)_ij^2sum_s=0^h-1 sum_k=1^n (Theta_s)_ik^2\n\nwhere\n\ntextFEVD_ij(h) is the share of variable i's h-step forecast error variance due to shock j\n(Theta_s)_ij is the (ij) element of the impulse response matrix at horizon s\nThe numerator sums the squared contributions of shock j through horizon h-1\nThe denominator sums contributions from all n shocks, ensuring sum_j textFEVD_ij(h) = 1","category":"section"},{"location":"innovation_accounting/#Properties","page":"Innovation Accounting","title":"Properties","text":"0 leq textFEVD_ij(h) leq 1 for all i j h\nsum_j=1^n textFEVD_ij(h) = 1 for all i h\nAs h to infty, FEVD converges to the unconditional variance decomposition","category":"section"},{"location":"innovation_accounting/#Usage-2","page":"Innovation Accounting","title":"Usage","text":"# Basic FEVD\nfevd_result = fevd(model, 20)\n\n# With bootstrap CI\nfevd_ci = fevd(model, 20; ci_type=:bootstrap, reps=500)\n\n# Access decomposition for variable 1\nfevd_var1 = fevd_result.decomposition[:, 1, :]  # horizons × shocks\n\nThe proportions array satisfies sum_j textproportionsh i j = 1 for all horizons h and variables i. At short horizons, own shocks typically dominate (large diagonal entries). As h to infty, the FEVD converges to the unconditional variance decomposition, revealing which shocks are the dominant long-run drivers of each variable's fluctuations. Adding ci_type=:bootstrap produces bootstrap CIs that quantify estimation uncertainty in the FEVD shares.","category":"section"},{"location":"innovation_accounting/#FEVD-Return-Values","page":"Innovation Accounting","title":"FEVD Return Values","text":"Field Type Description\ndecomposition Array{T,3} H times n times n raw variance contributions\nproportions Array{T,3} H times n times n proportion of FEV: proportions[h, i, j] = share of variable i's FEV due to shock j at horizon h","category":"section"},{"location":"innovation_accounting/#BayesianFEVD-Return-Values","page":"Innovation Accounting","title":"BayesianFEVD Return Values","text":"Field Type Description\nquantiles Array{T,4} H times n times n times 3: dimension 4 = [16th pctl, median, 84th pctl]\nmean Array{T,3} H times n times n posterior mean FEVD proportions\nhorizon Int Maximum horizon\nvariables Vector{String} Variable names\nshocks Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels\n\nReference: Lütkepohl (2005, Section 2.3.3)\n\n","category":"section"},{"location":"innovation_accounting/#Historical-Decomposition-(HD)","page":"Innovation Accounting","title":"Historical Decomposition (HD)","text":"","category":"section"},{"location":"innovation_accounting/#Definition-3","page":"Innovation Accounting","title":"Definition","text":"Historical decomposition decomposes observed variable movements into contributions from individual structural shocks over time:\n\ny_t = sum_s=0^t-1 Theta_s varepsilon_t-s + textinitial conditions\n\nwhere\n\ny_t is the n times 1 vector of observed variables at time t\nTheta_s = Phi_s P are the n times n structural MA coefficients at lag s\nPhi_s are the reduced-form MA coefficients (from the VMA representation)\nP = L Q is the n times n impact matrix (Cholesky factor L times rotation Q)\nvarepsilon_t = Q L^-1 u_t are the n times 1 structural shocks\nThe initial conditions capture the contribution of pre-sample values","category":"section"},{"location":"innovation_accounting/#Contribution-of-Shock-j-to-Variable-i-at-Time-t","page":"Innovation Accounting","title":"Contribution of Shock j to Variable i at Time t","text":"textHD_ij(t) = sum_s=0^t-1 (Theta_s)_ij  varepsilon_j(t-s)\n\nwhere\n\ntextHD_ij(t) is the contribution of shock j to variable i at time t\n(Theta_s)_ij is the (ij) element of the structural MA coefficient at lag s\nvarepsilon_j(t-s) is the realized structural shock j at time t-s\n\nThe decomposition satisfies the identity:\n\ny_t = sum_j=1^n textHD_ij(t) + textinitial_i(t)","category":"section"},{"location":"innovation_accounting/#Usage-3","page":"Innovation Accounting","title":"Usage","text":"# Basic historical decomposition\nhd = historical_decomposition(model, 198)\n\n# Verify decomposition identity\nverify_decomposition(hd)  # returns true if identity holds\n\n# Get contribution of shock 1 to variable 2\ncontrib = contribution(hd, 2, 1)\n\n# Total shock contribution (excluding initial conditions)\ntotal = total_shock_contribution(hd, 1)\n\n# With different identification\nhd_sign = historical_decomposition(model, 198; method=:sign,\n    sign_restrictions=sign_constraints)\n\nThe contributions[t, i, j] array gives the contribution of shock j to variable i at time t. Summing across shocks plus the initial conditions recovers the actual data: verify_decomposition(hd) checks this identity holds to numerical precision. The total_shock_contribution(hd, i) function sums all shock contributions for variable i, providing the \"shock-driven\" component of the series with initial conditions removed.","category":"section"},{"location":"innovation_accounting/#HistoricalDecomposition-Return-Values","page":"Innovation Accounting","title":"HistoricalDecomposition Return Values","text":"Field Type Description\ncontributions Array{T,3} T_eff times n times n shock contributions: contributions[t, i, j] = contribution of shock j to variable i at time t\ninitial_conditions Matrix{T} T_eff times n initial condition component\nactual Matrix{T} T_eff times n actual data values\nshocks Matrix{T} T_eff times n structural shocks\nT_eff Int Effective number of time periods\nvariables Vector{String} Variable names\nshock_names Vector{String} Shock names\nmethod Symbol Identification method (:cholesky, :sign, etc.)","category":"section"},{"location":"innovation_accounting/#BayesianHistoricalDecomposition-Return-Values","page":"Innovation Accounting","title":"BayesianHistoricalDecomposition Return Values","text":"Field Type Description\nquantiles Array{T,4} T_eff times n times n times n_q contribution quantiles\nmean Array{T,3} T_eff times n times n mean contributions\ninitial_quantiles Array{T,3} T_eff times n times n_q initial condition quantiles\ninitial_mean Matrix{T} T_eff times n mean initial conditions\nshocks_mean Matrix{T} T_eff times n mean structural shocks\nactual Matrix{T} T_eff times n actual data values\nT_eff Int Effective number of time periods\nvariables Vector{String} Variable names\nshock_names Vector{String} Shock names\nquantile_levels Vector{T} Quantile levels\nmethod Symbol Identification method\n\nReference: Kilian & Lütkepohl (2017, Chapter 4)\n\n","category":"section"},{"location":"innovation_accounting/#LP-Based-Innovation-Accounting","page":"Innovation Accounting","title":"LP-Based Innovation Accounting","text":"Structural Local Projections provide the same innovation accounting tools (IRF, FEVD, HD) as standard VAR, but via LP estimation. This offers robustness to VAR dynamic misspecification at the cost of some efficiency. For full theoretical background, see Local Projections.","category":"section"},{"location":"innovation_accounting/#IRF-from-Structural-LP","page":"Innovation Accounting","title":"IRF from Structural LP","text":"The irf() function dispatches on StructuralLP to return the pre-computed 3D impulse response:\n\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\nirf_result = irf(slp)   # Returns ImpulseResponse from the StructuralLP\n\n# Access: irf_result.values[h, i, j] = response of var i to shock j at horizon h\nprintln(\"Impact of shock 1 on var 2: \", irf_result.values[1, 2, 1])\n\nThe LP-based IRFs are numerically close to VAR-based IRFs under correct specification (Plagborg-Møller & Wolf 2021), but the LP standard errors stored in slp.se are wider because each horizon is estimated independently without imposing cross-horizon restrictions.","category":"section"},{"location":"innovation_accounting/#FEVD-from-Structural-LP","page":"Innovation Accounting","title":"FEVD from Structural LP","text":"The fevd() method for StructuralLP dispatches to the R²-based LP-FEVD of Gorodnichenko & Lee (2019):\n\ndecomp = fevd(slp, 20)  # Returns LPFEVD\n\n# Bias-corrected shares\nprintln(\"Var 1 explained by Shock 1 at h=8: \",\n        round(decomp.bias_corrected[1, 1, 8] * 100, digits=1), \"%\")\n\nUnlike VMA-based FEVD, LP-FEVD estimates variance shares directly via R² regressions, so they do not depend on the invertibility of the VAR lag polynomial. See LP-Based FEVD for the three estimator variants (R², LP-A, LP-B) and bias correction details.","category":"section"},{"location":"innovation_accounting/#Historical-Decomposition-from-Structural-LP","page":"Innovation Accounting","title":"Historical Decomposition from Structural LP","text":"hd = historical_decomposition(slp)\nverify_decomposition(hd)  # Check additive identity\n\nThe LP-based historical decomposition uses the structural shocks recovered from the VAR identification step (hatvarepsilon_t = QL^-1hatu_t) combined with LP-estimated IRF coefficients to decompose observed variable movements into shock contributions.","category":"section"},{"location":"innovation_accounting/#Cumulative-IRF-2","page":"Innovation Accounting","title":"Cumulative IRF","text":"For variables measured in growth rates (e.g., log-differenced GDP), the cumulative IRF shows the effect on the level:\n\nlp_model = estimate_lp(Y, 1, 20; lags=4)\nlp_irfs = lp_irf(lp_model)\ncum_irfs = cumulative_irf(lp_irfs)\n\nThe cumulative_irf function sums the pointwise IRF from horizon 0 through h, propagating standard errors via the delta method. This is especially useful for comparing LP and VAR results in levels versus differences.\n\n","category":"section"},{"location":"innovation_accounting/#Summary-Tables","page":"Innovation Accounting","title":"Summary Tables","text":"The package provides publication-quality summary tables using a unified interface with multiple dispatch.","category":"section"},{"location":"innovation_accounting/#Functions","page":"Innovation Accounting","title":"Functions","text":"Function Description\nreport(obj) Print comprehensive summary to stdout\ntable(obj, ...) Extract results as a matrix\nprint_table(io, obj, ...) Print formatted table to IO stream","category":"section"},{"location":"innovation_accounting/#Usage-Examples","page":"Innovation Accounting","title":"Usage Examples","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy model\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\nirf_result = irf(model, 20)\nfevd_result = fevd(model, 20)\nhd_result = historical_decomposition(model, size(model.U, 1))\n\n# Print summaries\nreport(model)\nreport(irf_result)\nreport(fevd_result)\nreport(hd_result)\n\n# Extract as DataFrames for further analysis\ndf_irf = table(irf_result, 1, 1)                    # response of var 1 to shock 1\ndf_irf_sel = table(irf_result, 1, 1; horizons=[1, 4, 8, 12, 20])\n\ndf_fevd = table(fevd_result, 1)                     # FEVD for variable 1\ndf_fevd_sel = table(fevd_result, 1; horizons=[1, 4, 8, 12])\n\ndf_hd = table(hd_result, 1)                         # HD for variable 1\ndf_hd_sel = table(hd_result, 1; periods=180:198)    # specific periods\n\n# Print formatted tables to stdout or file\nprint_table(stdout, irf_result, 1, 1; horizons=[1, 4, 8, 12])\nprint_table(stdout, fevd_result, 1; horizons=[1, 4, 8, 12])\nprint_table(stdout, hd_result, 1; periods=190:198)\n\n# Write to file\nopen(\"results.txt\", \"w\") do io\n    print_table(io, irf_result, 1, 1)\n    print_table(io, fevd_result, 1)\nend","category":"section"},{"location":"innovation_accounting/#String-Indexing","page":"Innovation Accounting","title":"String Indexing","text":"Variables and shocks can be indexed by name:\n\n# Using FRED-MD variable names\ndf = table(irf_result, \"INDPRO\", \"FEDFUNDS\")\ndf = table(fevd_result, \"INDPRO\")\ndf = table(hd_result, \"INDPRO\")\n\n","category":"section"},{"location":"innovation_accounting/#Display-Backends","page":"Innovation Accounting","title":"Display Backends","text":"All show, print_table, and report methods route through a unified PrettyTables backend. Switching from terminal text to LaTeX or HTML output requires a single call:\n\n# Switch output format globally\nset_display_backend(:text)    # Terminal-friendly (default)\nset_display_backend(:latex)   # LaTeX \\begin{tabular} output\nset_display_backend(:html)    # HTML <table> output\n\n# Check current backend\nget_display_backend()         # :text\n\nThis applies to all innovation accounting results — IRF, FEVD, and HD tables:\n\n# IRF table in LaTeX for a paper\nset_display_backend(:latex)\nopen(\"tables/irf_table.tex\", \"w\") do io\n    print_table(io, irfs, 1, 1; horizons=[1, 4, 8, 12, 20])\nend\n\n# FEVD table in HTML for slides\nset_display_backend(:html)\nopen(\"slides/fevd.html\", \"w\") do io\n    print_table(io, fevd_result, 1; horizons=[1, 4, 8, 12, 20])\nend\n\n# Reset to text for interactive work\nset_display_backend(:text)\n\nFor a comprehensive display backend workflow, see Example 15: Table Output.\n\n","category":"section"},{"location":"innovation_accounting/#Bibliographic-References","page":"Innovation Accounting","title":"Bibliographic References","text":"The refs() function returns bibliographic references for any model or identification method. This integrates with innovation accounting results:\n\n# References for a VAR model\nrefs(model)                           # AEA text format (default)\nrefs(model; format=:bibtex)           # BibTeX format\nrefs(model; format=:latex)            # LaTeX \\bibitem format\n\n# References by identification method\nrefs(:cholesky)                       # Cholesky decomposition references\nrefs(:fastica)                        # FastICA references\nrefs(:sign)                           # Sign restriction references\n\n# Write BibTeX to file\nopen(\"references.bib\", \"w\") do io\n    refs(io, model; format=:bibtex)\nend\n\nFor a comprehensive references workflow, see Example 16: Bibliographic References.\n\nnote: Technical Note\nFor non-Gaussian identification methods (ICA, ML, heteroskedasticity-based), see Non-Gaussian Identification. All 18 identification methods work seamlessly with irf(), fevd(), and historical_decomposition() via the method keyword.\n\n","category":"section"},{"location":"innovation_accounting/#Complete-Example","page":"Innovation Accounting","title":"Complete Example","text":"This example combines IRF, FEVD, and HD for a three-variable monetary policy VAR using FRED-MD data: industrial production (INDPRO), CPI inflation (CPIAUCSL), and the federal funds rate (FEDFUNDS).\n\nusing MacroEconometricModels\nusing Random\n\n# Load FRED-MD: industrial production, CPI inflation, federal funds rate\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\n\n# IRF with bootstrap confidence intervals\n# Response of industrial production to a monetary policy (FFR) shock\nH = 20\nRandom.seed!(42)\nirfs = irf(model, H; method=:cholesky, ci_type=:bootstrap, reps=500)\nprintln(\"FFR → INDPRO at h=0: \", round(irfs.values[1, 1, 3], digits=3))\nprintln(\"FFR → INDPRO at h=8: \", round(irfs.values[9, 1, 3], digits=3))\n\n# FEVD\n# Fraction of INDPRO forecast variance explained by each shock\ndecomp = fevd(model, H)\nprintln(\"\\nFEVD for INDPRO at h=1: shock shares = \",\n        round.(decomp.proportions[1, 1, :] .* 100, digits=1), \"%\")\nprintln(\"FEVD for INDPRO at h=20: shock shares = \",\n        round.(decomp.proportions[20, 1, :] .* 100, digits=1), \"%\")\n\n# Historical decomposition\n# Contribution of monetary shocks to industrial production fluctuations\nhd = historical_decomposition(model, size(model.U, 1))\nprintln(\"\\nDecomposition identity holds: \", verify_decomposition(hd))\n\n# Summary tables\ndf_irf = table(irfs, \"INDPRO\", \"FEDFUNDS\"; horizons=[0, 4, 8, 12, 20])\ndf_fevd = table(decomp, \"INDPRO\"; horizons=[1, 4, 8, 20])\n\nThe IRF traces the dynamic response of industrial production to a one-standard-deviation federal funds rate shock identified via Cholesky ordering (INDPRO, CPIAUCSL, FEDFUNDS). At impact (h=0), the recursive identification ensures the FFR shock does not contemporaneously affect INDPRO or CPIAUCSL. By h=8, the monetary transmission mechanism is visible in the INDPRO response. The FEVD reveals what fraction of INDPRO forecast uncertainty is attributable to each structural shock — monetary policy shocks versus own shocks and price shocks. The HD passes the verification check, confirming the additive identity y_t = sum_j textHD_j(t) + textinitial(t) holds to numerical precision.\n\n","category":"section"},{"location":"innovation_accounting/#References","page":"Innovation Accounting","title":"References","text":"Kilian, Lutz. 1998. \"Small-Sample Confidence Intervals for Impulse Response Functions.\" Review of Economics and Statistics 80 (2): 218–230. https://doi.org/10.1162/003465398557465\nKilian, Lutz, and Helmut Lütkepohl. 2017. Structural Vector Autoregressive Analysis. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108164818\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.","category":"section"},{"location":"api_types/#api_types","page":"Types","title":"API Types","text":"This page documents all core types in MacroEconometricModels.jl.","category":"section"},{"location":"api_types/#Module","page":"Types","title":"Module","text":"","category":"section"},{"location":"api_types/#Data-Containers","page":"Types","title":"Data Containers","text":"","category":"section"},{"location":"api_types/#Time-Series-Filters","page":"Types","title":"Time Series Filters","text":"","category":"section"},{"location":"api_types/#ARIMA-Models","page":"Types","title":"ARIMA Models","text":"","category":"section"},{"location":"api_types/#VAR-Models","page":"Types","title":"VAR Models","text":"","category":"section"},{"location":"api_types/#VECM-Models","page":"Types","title":"VECM Models","text":"","category":"section"},{"location":"api_types/#Analysis-Result-Types","page":"Types","title":"Analysis Result Types","text":"","category":"section"},{"location":"api_types/#Impulse-Response-and-FEVD","page":"Types","title":"Impulse Response and FEVD","text":"","category":"section"},{"location":"api_types/#Historical-Decomposition","page":"Types","title":"Historical Decomposition","text":"","category":"section"},{"location":"api_types/#Factor-Models","page":"Types","title":"Factor Models","text":"","category":"section"},{"location":"api_types/#Local-Projections","page":"Types","title":"Local Projections","text":"","category":"section"},{"location":"api_types/#Panel-VAR-Types","page":"Types","title":"Panel VAR Types","text":"","category":"section"},{"location":"api_types/#GMM-Types","page":"Types","title":"GMM Types","text":"","category":"section"},{"location":"api_types/#Prior-Types","page":"Types","title":"Prior Types","text":"","category":"section"},{"location":"api_types/#Bayesian-Posterior-Types","page":"Types","title":"Bayesian Posterior Types","text":"","category":"section"},{"location":"api_types/#Covariance-Estimators","page":"Types","title":"Covariance Estimators","text":"","category":"section"},{"location":"api_types/#Unit-Root-Test-Types","page":"Types","title":"Unit Root Test Types","text":"","category":"section"},{"location":"api_types/#Model-Comparison-Types","page":"Types","title":"Model Comparison Types","text":"","category":"section"},{"location":"api_types/#Granger-Causality-Types","page":"Types","title":"Granger Causality Types","text":"","category":"section"},{"location":"api_types/#Nowcasting-Types","page":"Types","title":"Nowcasting Types","text":"","category":"section"},{"location":"api_types/#SVAR-Identification-Types","page":"Types","title":"SVAR Identification Types","text":"","category":"section"},{"location":"api_types/#Volatility-Models","page":"Types","title":"Volatility Models","text":"","category":"section"},{"location":"api_types/#Non-Gaussian-SVAR-Types","page":"Types","title":"Non-Gaussian SVAR Types","text":"","category":"section"},{"location":"api_types/#Type-Hierarchy","page":"Types","title":"Type Hierarchy","text":"AbstractMacroData\n├── TimeSeriesData{T}\n├── PanelData{T}\n└── CrossSectionData{T}\n\nDataDiagnostic\nDataSummary\nFrequency (enum: Daily, Monthly, Quarterly, Yearly, Mixed, Other)\n\nAbstractARIMAModel <: StatsAPI.RegressionModel\n├── ARModel{T}\n├── MAModel{T}\n├── ARMAModel{T}\n└── ARIMAModel{T}\n\nAbstractVARModel\n├── VARModel{T}\n└── VECMModel{T}\n\nVECMForecast{T}\nVECMGrangerResult{T}\n\nPVARModel{T} <: StatsAPI.RegressionModel\nPVARStability{T}\nPVARTestResult{T} <: StatsAPI.HypothesisTest\n\nAbstractAnalysisResult\n├── AbstractFrequentistResult\n│     ├── ImpulseResponse{T}, FEVD{T}, HistoricalDecomposition{T}\n└── AbstractBayesianResult\n      ├── BayesianImpulseResponse{T}, BayesianFEVD{T}, BayesianHistoricalDecomposition{T}\n\nAbstractImpulseResponse\n├── ImpulseResponse{T}\n├── BayesianImpulseResponse{T}\n└── AbstractLPImpulseResponse\n    └── LPImpulseResponse{T}\n\nAbstractFEVD\n├── FEVD{T}\n├── BayesianFEVD{T}\n└── LPFEVD{T}\n\nAbstractHistoricalDecomposition\n├── HistoricalDecomposition{T}\n└── BayesianHistoricalDecomposition{T}\n\nAbstractFactorModel\n├── FactorModel{T}\n├── DynamicFactorModel{T}\n└── GeneralizedDynamicFactorModel{T}\n\nFactorForecast{T}\n\nAbstractLPModel\n├── LPModel{T}\n├── LPIVModel{T}\n├── SmoothLPModel{T}\n├── StateLPModel{T}\n└── PropensityLPModel{T}\n\nStructuralLP{T}\nLPForecast{T}\n\nAbstractCovarianceEstimator\n├── NeweyWestEstimator{T}\n├── WhiteEstimator\n└── DriscollKraayEstimator{T}\n\nAbstractGMMModel\n└── GMMModel{T}\n\nAbstractPrior\n└── MinnesotaHyperparameters{T}\n\nBVARPosterior{T}\n\nAbstractUnitRootTest <: StatsAPI.HypothesisTest\n├── ADFResult{T}\n├── KPSSResult{T}\n├── PPResult{T}\n├── ZAResult{T}\n├── NgPerronResult{T}\n└── JohansenResult{T}\n\nVARStationarityResult{T}\n\nLRTestResult{T} <: StatsAPI.HypothesisTest\nLMTestResult{T} <: StatsAPI.HypothesisTest\nGrangerCausalityResult{T} <: StatsAPI.HypothesisTest\n\nAbstractNormalityTest <: StatsAPI.HypothesisTest\n└── NormalityTestResult{T}\n\nNormalityTestSuite{T}\n\nAbstractNonGaussianSVAR\n├── ICASVARResult{T}\n├── NonGaussianMLResult{T}\n├── MarkovSwitchingSVARResult{T}\n├── GARCHSVARResult{T}\n├── SmoothTransitionSVARResult{T}\n└── ExternalVolatilitySVARResult{T}\n\nIdentifiabilityTestResult{T}\n\nAbstractVolatilityModel <: StatsAPI.RegressionModel\n├── ARCHModel{T}\n├── GARCHModel{T}\n├── EGARCHModel{T}\n├── GJRGARCHModel{T}\n└── SVModel{T}\n\nVolatilityForecast{T}\n\nAbstractNowcastModel\n├── NowcastDFM{T}\n├── NowcastBVAR{T}\n└── NowcastBridge{T}\n\nNowcastResult{T}\nNowcastNews{T}","category":"section"},{"location":"api_types/#MacroEconometricModels.MacroEconometricModels","page":"Types","title":"MacroEconometricModels.MacroEconometricModels","text":"MacroEconometricModels\n\nA Julia package for macroeconomic time series analysis, providing tools for:\n\nVector Autoregression (VAR) estimation\nBayesian VAR (BVAR) with Minnesota priors\nStructural identification (Cholesky, sign restrictions, narrative, long-run)\nImpulse Response Functions (IRF)\nForecast Error Variance Decomposition (FEVD)\nFactor models via Principal Component Analysis\nLocal Projections (LP) with various extensions:\nHAC standard errors (Jordà 2005)\nInstrumental Variables (Stock & Watson 2018)\nSmooth IRF via B-splines (Barnichon & Brownlees 2019)\nState-dependent LP (Auerbach & Gorodnichenko 2013)\nPropensity Score Matching (Angrist et al. 2018)\nARIMA/ARMA model estimation, forecasting, and order selection\nGeneralized Method of Moments (GMM) estimation\n\nQuick Start\n\nusing MacroEconometricModels\n\n# Estimate a VAR model\nY = randn(100, 3)\nmodel = estimate_var(Y, 2)\n\n# Compute IRFs with bootstrap confidence intervals\nirf_result = irf(model, 20; ci_type=:bootstrap)\n\n# Local Projection IRFs with HAC standard errors\nlp_result = estimate_lp(Y, 1, 20; cov_type=:newey_west)\nlp_irf_result = lp_irf(lp_result)\n\n# Bayesian estimation\npost = estimate_bvar(Y, 2; prior=:minnesota)\n\nReferences\n\nBańbura, M., Giannone, D., & Reichlin, L. (2010). Large Bayesian vector auto regressions.\nLütkepohl, H. (2005). New Introduction to Multiple Time Series Analysis.\nRubio-Ramírez, J. F., Waggoner, D. F., & Zha, T. (2010). Structural vector autoregressions.\nJordà, Ò. (2005). Estimation and Inference of Impulse Responses by Local Projections.\nStock, J. H., & Watson, M. W. (2018). Identification and Estimation of Dynamic Causal Effects.\nBarnichon, R., & Brownlees, C. (2019). Impulse Response Estimation by Smooth Local Projections.\nAuerbach, A. J., & Gorodnichenko, Y. (2013). Fiscal Multipliers in Recession and Expansion.\nAngrist, J. D., Jordà, Ò., & Kuersteiner, G. M. (2018). Semiparametric Estimates of Monetary Policy Effects.\n\n\n\n\n\n","category":"module"},{"location":"api_types/#MacroEconometricModels.AbstractMacroData","page":"Types","title":"MacroEconometricModels.AbstractMacroData","text":"AbstractMacroData\n\nAbstract supertype for all MacroEconometricModels data containers. Subtypes: TimeSeriesData, PanelData, CrossSectionData.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.TimeSeriesData","page":"Types","title":"MacroEconometricModels.TimeSeriesData","text":"TimeSeriesData{T<:AbstractFloat} <: AbstractMacroData\n\nContainer for time series data with metadata.\n\nFields\n\ndata::Matrix{T} — Tobs × nvars data matrix\nvarnames::Vector{String} — variable names\nfrequency::Frequency — data frequency\ntcode::Vector{Int} — FRED transformation codes per variable (default: all 1 = levels)\ntime_index::Vector{Int} — integer time identifiers (default: 1:T)\nT_obs::Int — number of observations\nn_vars::Int — number of variables\ndesc::Vector{String} — dataset description (length-1 vector for mutability)\nvardesc::Dict{String,String} — per-variable descriptions keyed by variable name\nsource_refs::Vector{Symbol} — reference keys for bibliographic citations (see refs())\ndates::Vector{String} — date labels (default: empty; use set_dates! to populate)\n\nConstructors\n\nTimeSeriesData(data::Matrix; varnames, frequency=Other, tcode, time_index, desc, vardesc, source_refs, dates)\nTimeSeriesData(data::Vector; varname=\"x1\", frequency=Other, desc, vardesc, source_refs, dates)\nTimeSeriesData(df::DataFrame; frequency=Other, varnames, desc, vardesc, source_refs, dates)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PanelData","page":"Types","title":"MacroEconometricModels.PanelData","text":"PanelData{T<:AbstractFloat} <: AbstractMacroData\n\nContainer for panel (longitudinal) data with group and time identifiers.\n\nFields\n\ndata::Matrix{T} — stacked data matrix (sum of obs across groups × n_vars)\nvarnames::Vector{String} — variable names\nfrequency::Frequency — data frequency\ntcode::Vector{Int} — FRED transformation codes per variable\ngroup_id::Vector{Int} — group identifier per row\ntime_id::Vector{Int} — time identifier per row\ngroup_names::Vector{String} — unique group labels\nn_groups::Int — number of groups\nn_vars::Int — number of variables\nT_obs::Int — total number of rows\nbalanced::Bool — true if all groups have same number of observations\ndesc::Vector{String} — dataset description (length-1 vector for mutability)\nvardesc::Dict{String,String} — per-variable descriptions keyed by variable name\nsource_refs::Vector{Symbol} — reference keys for bibliographic citations (see refs())\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.CrossSectionData","page":"Types","title":"MacroEconometricModels.CrossSectionData","text":"CrossSectionData{T<:AbstractFloat} <: AbstractMacroData\n\nContainer for cross-sectional data (single time point, multiple observations).\n\nFields\n\ndata::Matrix{T} — Nobs × nvars data matrix\nvarnames::Vector{String} — variable names\nobs_id::Vector{Int} — observation identifiers\nN_obs::Int — number of observations\nn_vars::Int — number of variables\ndesc::Vector{String} — dataset description (length-1 vector for mutability)\nvardesc::Dict{String,String} — per-variable descriptions keyed by variable name\nsource_refs::Vector{Symbol} — reference keys for bibliographic citations (see refs())\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.Frequency","page":"Types","title":"MacroEconometricModels.Frequency","text":"Frequency\n\nEnumeration of time series data frequencies.\n\nValues: Daily, Monthly, Quarterly, Yearly, Mixed, Other.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.DataDiagnostic","page":"Types","title":"MacroEconometricModels.DataDiagnostic","text":"DataDiagnostic\n\nResult of diagnose(d) — per-variable issue counts and overall cleanliness.\n\nFields\n\nn_nan::Vector{Int} — NaN count per variable\nn_inf::Vector{Int} — Inf count per variable\nis_constant::Vector{Bool} — true if variable has zero variance\nis_short::Bool — true if series has fewer than 10 observations\nvarnames::Vector{String} — variable names\nis_clean::Bool — true if no issues detected\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.DataSummary","page":"Types","title":"MacroEconometricModels.DataSummary","text":"DataSummary\n\nResult of describe_data(d) — per-variable summary statistics.\n\nFields\n\nvarnames::Vector{String} — variable names\nn::Vector{Int} — non-NaN observation count per variable\nmean::Vector{Float64} — mean of finite values\nstd::Vector{Float64} — standard deviation\nmin::Vector{Float64} — minimum\np25::Vector{Float64} — 25th percentile\nmedian::Vector{Float64} — median (50th percentile)\np75::Vector{Float64} — 75th percentile\nmax::Vector{Float64} — maximum\nskewness::Vector{Float64} — skewness\nkurtosis::Vector{Float64} — excess kurtosis\nT_obs::Int — total observations\nn_vars::Int — number of variables\nfrequency::Frequency — data frequency\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractFilterResult","page":"Types","title":"MacroEconometricModels.AbstractFilterResult","text":"Abstract supertype for trend-cycle decomposition filter results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.HPFilterResult","page":"Types","title":"MacroEconometricModels.HPFilterResult","text":"HPFilterResult{T} <: AbstractFilterResult\n\nResult of the Hodrick-Prescott filter (Hodrick & Prescott 1997).\n\nFields\n\ntrend::Vector{T}: Estimated trend component (length T_obs)\ncycle::Vector{T}: Cyclical component y - trend (length T_obs)\nlambda::T: Smoothing parameter used\nT_obs::Int: Number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.HamiltonFilterResult","page":"Types","title":"MacroEconometricModels.HamiltonFilterResult","text":"HamiltonFilterResult{T} <: AbstractFilterResult\n\nResult of the Hamilton (2018) regression filter.\n\nRegresses y_t+h on 1 y_t y_t-1 ldots y_t-p+1. Residuals are the cyclical component; fitted values are the trend.\n\nFields\n\ntrend::Vector{T}: Fitted values (length T_obs - h - p + 1)\ncycle::Vector{T}: Residuals (length T_obs - h - p + 1)\nbeta::Vector{T}: OLS coefficients [intercept; lag coefficients]\nh::Int: Forecast horizon\np::Int: Number of lags\nT_obs::Int: Original series length\nvalid_range::UnitRange{Int}: Indices into original series where results are valid\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BeveridgeNelsonResult","page":"Types","title":"MacroEconometricModels.BeveridgeNelsonResult","text":"BeveridgeNelsonResult{T} <: AbstractFilterResult\n\nResult of the Beveridge-Nelson (1981) trend-cycle decomposition.\n\nDecomposes a unit-root process into a permanent (random walk + drift) component and a transitory (stationary) component using the ARIMA representation.\n\nFields\n\npermanent::Vector{T}: Permanent (trend) component\ntransitory::Vector{T}: Transitory (cycle) component\ndrift::T: Estimated drift (mean of first differences)\nlong_run_multiplier::T: Long-run multiplier psi(1) = 1 + sum(psi weights)\narima_order::Tuple{Int,Int,Int}: (p, d, q) order used\nT_obs::Int: Number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BaxterKingResult","page":"Types","title":"MacroEconometricModels.BaxterKingResult","text":"BaxterKingResult{T} <: AbstractFilterResult\n\nResult of the Baxter-King (1999) band-pass filter.\n\nIsolates cyclical fluctuations in a specified frequency band while removing both low-frequency trend and high-frequency noise.\n\nFields\n\ncycle::Vector{T}: Band-pass filtered component (length T_obs - 2K)\ntrend::Vector{T}: Residual trend y - cycle (length T_obs - 2K)\nweights::Vector{T}: Symmetric filter weights [a0, a1, ..., a_K]\npl::Int: Lower period bound (e.g., 6 quarters)\npu::Int: Upper period bound (e.g., 32 quarters)\nK::Int: Truncation length (observations lost at each end)\nT_obs::Int: Original series length\nvalid_range::UnitRange{Int}: Indices into original series where results are valid\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BoostedHPResult","page":"Types","title":"MacroEconometricModels.BoostedHPResult","text":"BoostedHPResult{T} <: AbstractFilterResult\n\nResult of the boosted HP filter (Phillips & Shi 2021).\n\nIteratively re-applies the HP filter to the cyclical component until a stopping criterion is met, improving trend estimation by removing remaining unit root behavior.\n\nFields\n\ntrend::Vector{T}: Final trend estimate (length T_obs)\ncycle::Vector{T}: Final cyclical component (length T_obs)\nlambda::T: Smoothing parameter used\niterations::Int: Number of boosting iterations performed\nstopping::Symbol: Stopping criterion used (:ADF, :BIC, or :fixed)\nbic_path::Vector{T}: BIC value at each iteration\nadf_pvalues::Vector{T}: ADF p-values at each iteration\nT_obs::Int: Number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractARIMAModel","page":"Types","title":"MacroEconometricModels.AbstractARIMAModel","text":"AbstractARIMAModel{T<:AbstractFloat} <: StatsAPI.RegressionModel\n\nAbstract supertype for all univariate ARIMA-class models, parametric on the floating-point element type T.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARModel","page":"Types","title":"MacroEconometricModels.ARModel","text":"ARModel{T} <: AbstractARIMAModel\n\nAutoregressive AR(p) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ\n\nFields\n\ny::Vector{T}: Original data\np::Int: AR order\nc::T: Intercept\nphi::Vector{T}: AR coefficients [φ₁, ..., φₚ]\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method (:ols, :mle)\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations (0 for OLS)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.MAModel","page":"Types","title":"MacroEconometricModels.MAModel","text":"MAModel{T} <: AbstractARIMAModel\n\nMoving average MA(q) model: yₜ = c + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nFields\n\ny::Vector{T}: Original data\nq::Int: MA order\nc::T: Intercept\ntheta::Vector{T}: MA coefficients [θ₁, ..., θq]\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method (:css, :mle, :css_mle)\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARMAModel","page":"Types","title":"MacroEconometricModels.ARMAModel","text":"ARMAModel{T} <: AbstractARIMAModel\n\nAutoregressive moving average ARMA(p,q) model: yₜ = c + φ₁yₜ₋₁ + ... + φₚyₜ₋ₚ + εₜ + θ₁εₜ₋₁ + ... + θqεₜ₋q\n\nFields\n\ny::Vector{T}: Original data\np::Int: AR order\nq::Int: MA order\nc::T: Intercept\nphi::Vector{T}: AR coefficients [φ₁, ..., φₚ]\ntheta::Vector{T}: MA coefficients [θ₁, ..., θq]\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method (:css, :mle, :css_mle)\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARIMAModel","page":"Types","title":"MacroEconometricModels.ARIMAModel","text":"ARIMAModel{T} <: AbstractARIMAModel\n\nAutoregressive integrated moving average ARIMA(p,d,q) model. The model is fit to the d-times differenced series as ARMA(p,q).\n\nFields\n\ny::Vector{T}: Original (undifferenced) data\ny_diff::Vector{T}: Differenced series\np::Int: AR order\nd::Int: Integration order (number of differences)\nq::Int: MA order\nc::T: Intercept (on differenced series)\nphi::Vector{T}: AR coefficients\ntheta::Vector{T}: MA coefficients\nsigma2::T: Innovation variance\nresiduals::Vector{T}: Estimated residuals\nfitted::Vector{T}: Fitted values (on differenced series)\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARIMAForecast","page":"Types","title":"MacroEconometricModels.ARIMAForecast","text":"ARIMAForecast{T}\n\nForecast result from an ARIMA-class model.\n\nFields\n\nforecast::Vector{T}: Point forecasts\nci_lower::Vector{T}: Lower confidence interval bound\nci_upper::Vector{T}: Upper confidence interval bound\nse::Vector{T}: Standard errors of forecasts\nhorizon::Int: Forecast horizon\nconf_level::T: Confidence level (e.g., 0.95)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARIMAOrderSelection","page":"Types","title":"MacroEconometricModels.ARIMAOrderSelection","text":"ARIMAOrderSelection{T}\n\nResult from automatic ARIMA order selection.\n\nFields\n\nbest_p_aic::Int: Best AR order by AIC\nbest_q_aic::Int: Best MA order by AIC\nbest_p_bic::Int: Best AR order by BIC\nbest_q_bic::Int: Best MA order by BIC\naic_matrix::Matrix{T}: AIC values for all (p,q) combinations\nbic_matrix::Matrix{T}: BIC values for all (p,q) combinations\nbest_model_aic::AbstractARIMAModel: Fitted model with best AIC\nbest_model_bic::AbstractARIMAModel: Fitted model with best BIC\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VARModel","page":"Types","title":"MacroEconometricModels.VARModel","text":"VARModel{T} <: AbstractVARModel\n\nVAR model estimated via OLS.\n\nFields: Y (data), p (lags), B (coefficients), U (residuals), Sigma (covariance), aic, bic, hqic, varnames.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractVARModel","page":"Types","title":"MacroEconometricModels.AbstractVARModel","text":"Abstract supertype for Vector Autoregression models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VECMModel","page":"Types","title":"MacroEconometricModels.VECMModel","text":"VECMModel{T} <: AbstractVARModel\n\nVector Error Correction Model estimated via Johansen MLE or Engle-Granger two-step.\n\nThe VECM representation:     ΔYₜ = αβ'Yₜ₋₁ + Γ₁ΔYₜ₋₁ + ... + Γₚ₋₁ΔYₜ₋ₚ₊₁ + μ + uₜ\n\nwhere Π = αβ' is the long-run matrix (n × n, rank r), α (n × r) are adjustment speeds, and β (n × r) are cointegrating vectors.\n\nFields\n\nY: Original data in levels (T_obs × n)\np: Underlying VAR order (VECM has p-1 lagged differences)\nrank: Cointegrating rank r\nalpha: Adjustment coefficients (n × r)\nbeta: Cointegrating vectors (n × r), Phillips-normalized\nPi: Long-run matrix αβ' (n × n)\nGamma: Short-run dynamics [Γ₁, ..., Γₚ₋₁]\nmu: Intercept (n)\nU: Residuals (T_eff × n)\nSigma: Residual covariance (n × n)\naic, bic, hqic: Information criteria\nloglik: Log-likelihood\ndeterministic: :none, :constant, or :trend\nmethod: :johansen or :engle_granger\njohansen_result: Johansen test result (if applicable)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VECMForecast","page":"Types","title":"MacroEconometricModels.VECMForecast","text":"VECMForecast{T}\n\nForecast result from a VECM, preserving cointegrating relationships.\n\nFields\n\nlevels: Forecasts in levels (h × n)\ndifferences: Forecasts in first differences (h × n)\nci_lower, ci_upper: Confidence interval bounds in levels (h × n)\nhorizon: Forecast horizon\nci_method: CI method used (:none, :bootstrap, :simulation)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VECMGrangerResult","page":"Types","title":"MacroEconometricModels.VECMGrangerResult","text":"VECMGrangerResult{T}\n\nVECM Granger causality test result with short-run, long-run, and strong (joint) tests.\n\nFields\n\nshort_run_stat, short_run_pvalue, short_run_df: Wald test on Γ coefficients\nlong_run_stat, long_run_pvalue, long_run_df: Wald test on α (error correction)\nstrong_stat, strong_pvalue, strong_df: Joint test\ncause_var, effect_var: Variable indices\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractAnalysisResult","page":"Types","title":"MacroEconometricModels.AbstractAnalysisResult","text":"AbstractAnalysisResult\n\nAbstract supertype for all innovation accounting and structural analysis results. Provides a unified interface for accessing results from various methods (IRF, FEVD, HD).\n\nSubtypes should implement:\n\npoint_estimate(result) - return point estimate\nhas_uncertainty(result) - return true if uncertainty bounds available\nuncertainty_bounds(result) - return (lower, upper) bounds if available\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractFrequentistResult","page":"Types","title":"MacroEconometricModels.AbstractFrequentistResult","text":"AbstractFrequentistResult <: AbstractAnalysisResult\n\nFrequentist analysis results with point estimates and optional confidence intervals.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractBayesianResult","page":"Types","title":"MacroEconometricModels.AbstractBayesianResult","text":"AbstractBayesianResult <: AbstractAnalysisResult\n\nBayesian analysis results with posterior quantiles and means.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ImpulseResponse","page":"Types","title":"MacroEconometricModels.ImpulseResponse","text":"ImpulseResponse{T} <: AbstractImpulseResponse\n\nIRF results with optional confidence intervals.\n\nFields: values (H×n×n), cilower, ciupper, horizon, variables, shocks, ci_type.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BayesianImpulseResponse","page":"Types","title":"MacroEconometricModels.BayesianImpulseResponse","text":"BayesianImpulseResponse{T} <: AbstractImpulseResponse\n\nBayesian IRF with posterior quantiles.\n\nFields: quantiles (H×n×n×q), mean (H×n×n), horizon, variables, shocks, quantile_levels.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractImpulseResponse","page":"Types","title":"MacroEconometricModels.AbstractImpulseResponse","text":"Abstract supertype for impulse response function results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.FEVD","page":"Types","title":"MacroEconometricModels.FEVD","text":"FEVD results: decomposition (n×n×H), proportions, variable/shock names.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BayesianFEVD","page":"Types","title":"MacroEconometricModels.BayesianFEVD","text":"Bayesian FEVD with posterior quantiles.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractFEVD","page":"Types","title":"MacroEconometricModels.AbstractFEVD","text":"Abstract supertype for forecast error variance decomposition results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.HistoricalDecomposition","page":"Types","title":"MacroEconometricModels.HistoricalDecomposition","text":"HistoricalDecomposition{T} <: AbstractHistoricalDecomposition\n\nFrequentist historical decomposition result.\n\nFields:\n\ncontributions: Shock contributions (Teff × nvars × n_shocks)\ninitial_conditions: Initial condition component (Teff × nvars)\nactual: Actual data values (Teff × nvars)\nshocks: Structural shocks (Teff × nshocks)\nT_eff: Effective number of time periods\nvariables: Variable names\nshock_names: Shock names\nmethod: Identification method used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BayesianHistoricalDecomposition","page":"Types","title":"MacroEconometricModels.BayesianHistoricalDecomposition","text":"BayesianHistoricalDecomposition{T} <: AbstractHistoricalDecomposition\n\nBayesian historical decomposition with posterior quantiles.\n\nFields:\n\nquantiles: Contribution quantiles (Teff × nvars × nshocks × nquantiles)\nmean: Mean contributions (Teff × nvars × n_shocks)\ninitial_quantiles: Initial condition quantiles (Teff × nvars × n_quantiles)\ninitial_mean: Mean initial conditions (Teff × nvars)\nshocks_mean: Mean structural shocks (Teff × nshocks)\nactual: Actual data values (Teff × nvars)\nT_eff: Effective number of time periods\nvariables: Variable names\nshock_names: Shock names\nquantile_levels: Quantile levels (e.g., [0.16, 0.5, 0.84])\nmethod: Identification method used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractHistoricalDecomposition","page":"Types","title":"MacroEconometricModels.AbstractHistoricalDecomposition","text":"Abstract supertype for historical decomposition results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.FactorModel","page":"Types","title":"MacroEconometricModels.FactorModel","text":"FactorModel{T} <: AbstractFactorModel\n\nStatic factor model via PCA: Xₜ = Λ Fₜ + eₜ.\n\nFields: X, factors, loadings, eigenvalues, explainedvariance, cumulativevariance, r, standardized.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.DynamicFactorModel","page":"Types","title":"MacroEconometricModels.DynamicFactorModel","text":"DynamicFactorModel{T} <: AbstractFactorModel\n\nDynamic factor model: Xₜ = Λ Fₜ + eₜ, Fₜ = Σᵢ Aᵢ Fₜ₋ᵢ + ηₜ.\n\nFields: X, factors, loadings, A (VAR coefficients), factorresiduals, Sigmaeta, Sigmae, eigenvalues, explainedvariance, cumulative_variance, r, p, method, standardized, converged, iterations, loglik.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GeneralizedDynamicFactorModel","page":"Types","title":"MacroEconometricModels.GeneralizedDynamicFactorModel","text":"GeneralizedDynamicFactorModel{T} <: AbstractFactorModel\n\nGDFM with frequency-dependent loadings: Xₜ = χₜ + ξₜ.\n\nFields: X, factors, commoncomponent, idiosyncratic, loadingsspectral, spectraldensityX, spectraldensitychi, eigenvaluesspectral, frequencies, q (dynamic factors), r (static factors), bandwidth, kernel, standardized, varianceexplained.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.FactorForecast","page":"Types","title":"MacroEconometricModels.FactorForecast","text":"FactorForecast{T<:AbstractFloat}\n\nResult of factor model forecasting with optional confidence intervals.\n\nFields: factors, observables, factorslower, factorsupper, observableslower, observablesupper, factorsse, observablesse, horizon, conflevel, cimethod.\n\nWhen ci_method == :none, CI and SE fields are zero matrices.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractFactorModel","page":"Types","title":"MacroEconometricModels.AbstractFactorModel","text":"Abstract supertype for factor models (static and dynamic).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractLPImpulseResponse","page":"Types","title":"MacroEconometricModels.AbstractLPImpulseResponse","text":"Abstract supertype for LP impulse response results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractLPModel","page":"Types","title":"MacroEconometricModels.AbstractLPModel","text":"Abstract supertype for Local Projection models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPFEVD","page":"Types","title":"MacroEconometricModels.LPFEVD","text":"LPFEVD{T} <: AbstractFEVD\n\nLP-based Forecast Error Variance Decomposition (Gorodnichenko & Lee 2019).\n\nUses R²-based estimator: regress estimated LP forecast errors on identified structural shocks to measure the share of forecast error variance attributable to each shock. Includes VAR-based bootstrap bias correction and CIs.\n\nFields\n\nproportions: Raw FEVD estimates (n × n × H), [i,j,h] = share of variable i's h-step forecast error variance due to shock j\nbias_corrected: Bias-corrected FEVD (n × n × H)\nse: Bootstrap standard errors (n × n × H)\nci_lower: Lower CI bounds (n × n × H)\nci_upper: Upper CI bounds (n × n × H)\nmethod: Estimator (:r2, :lpa, :lpb)\nhorizon: Maximum FEVD horizon\nn_boot: Number of bootstrap replications used\nconf_level: Confidence level for CIs\nbias_correction: Whether bias correction was applied\n\nReference\n\nGorodnichenko, Y. & Lee, B. (2019). \"Forecast Error Variance Decompositions with Local Projections.\" JBES, 38(4), 921–933.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPForecast","page":"Types","title":"MacroEconometricModels.LPForecast","text":"LPForecast{T}\n\nDirect multi-step LP forecast result.\n\nEach horizon h uses its own regression coefficients directly (no recursion), producing ŷ{T+h} = αh + βh·shockh + Γh·controlsT.\n\nFields:\n\nforecasts: Point forecasts (H × n_response)\nci_lower: Lower CI bounds (H × n_response)\nci_upper: Upper CI bounds (H × n_response)\nse: Standard errors (H × n_response)\nhorizon: Maximum forecast horizon\nresponse_vars: Response variable indices\nshock_var: Shock variable index\nshock_path: Assumed shock trajectory\nconf_level: Confidence level\nci_method: CI method (:analytical, :bootstrap, :none)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPImpulseResponse","page":"Types","title":"MacroEconometricModels.LPImpulseResponse","text":"LPImpulseResponse{T} <: AbstractLPImpulseResponse\n\nLP-based impulse response function with confidence intervals from robust standard errors.\n\nFields:\n\nvalues: Point estimates (H+1 × n_response)\nci_lower: Lower CI bounds\nci_upper: Upper CI bounds\nse: Standard errors\nhorizon: Maximum horizon\nresponse_vars: Names of response variables\nshock_var: Name of shock variable\ncov_type: Covariance estimator type\nconf_level: Confidence level used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LPModel","page":"Types","title":"MacroEconometricModels.LPModel","text":"LPModel{T} <: AbstractLPModel\n\nLocal Projection model estimated via OLS with robust standard errors (Jordà 2005).\n\nThe LP regression for horizon h:     y{t+h} = αh + βh * shockt + Γh * controlst + ε_{t+h}\n\nFields:\n\nY: Response data matrix (Tobs × nvars)\nshock_var: Index of shock variable in Y\nresponse_vars: Indices of response variables (default: all)\nhorizon: Maximum IRF horizon H\nlags: Number of control lags included\nB: Vector of coefficient matrices, one per horizon h=0,...,H\nresiduals: Vector of residual matrices per horizon\nvcov: Vector of robust covariance matrices per horizon\nT_eff: Effective sample sizes per horizon\ncov_estimator: Covariance estimator used\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.StructuralLP","page":"Types","title":"MacroEconometricModels.StructuralLP","text":"StructuralLP{T} <: AbstractFrequentistResult\n\nStructural Local Projection result combining VAR-based identification with LP estimation.\n\nEstimates multi-shock IRFs by computing orthogonalized structural shocks from a VAR model and using them as regressors in LP regressions (Plagborg-Møller & Wolf 2021).\n\nFields:\n\nirf: 3D impulse responses (H × n × n) — reuses ImpulseResponse{T}\nstructural_shocks: Structural shocks (T_eff × n)\nvar_model: Underlying VAR model used for identification\nQ: Rotation/identification matrix\nmethod: Identification method used (:cholesky, :sign, :long_run, :fastica, etc.)\nlags: Number of LP control lags\ncov_type: HAC estimator type\nse: Standard errors (H × n × n)\nlp_models: Individual LP model per shock\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PVARModel","page":"Types","title":"MacroEconometricModels.PVARModel","text":"PVARModel{T} <: StatsAPI.RegressionModel\n\nPanel VAR model estimated via GMM or FE-OLS.\n\nStores coefficient matrices, robust standard errors, GMM internals for specification tests, and panel descriptors. GMM internals (instruments, weighting matrix, residuals) are retained to support Hansen J-test, bootstrap, and Andrews-Lu MMSC without re-estimation.\n\nFields\n\nPhi::Matrix{T} — m × K coefficient matrix (rows = equations, K = m*p + npredet + nexog [+ 1])\nSigma::Matrix{T} — m × m residual covariance (from level residuals)\nse::Matrix{T} — same shape as Phi (robust SEs, Windmeijer-corrected for 2-step)\npvalues::Matrix{T} — same shape as Phi\nm::Int — number of endogenous variables\np::Int — number of lags\nn_predet::Int — number of predetermined variables\nn_exog::Int — number of strictly exogenous variables\nvarnames::Vector{String} — endogenous variable names\npredet_names::Vector{String} — predetermined variable names\nexog_names::Vector{String} — exogenous variable names\nmethod::Symbol — :fdgmm, :systemgmm, :fe_ols\ntransformation::Symbol — :fd, :fod, :demean\nsteps::Symbol — :onestep, :twostep, :mstep\nsystem_constant::Bool — whether level equation includes a constant\nn_groups::Int — number of panel groups\nn_periods::Int — number of time periods (max)\nn_obs::Int — total effective observations\nobs_per_group::NamedTuple{(:min,:avg,:max), Tuple{Int,Float64,Int}}\ninstruments::Vector{Matrix{T}} — per-group instrument matrices\nresiduals_transformed::Vector{Matrix{T}} — per-group transformed residuals\nweighting_matrix::Matrix{T} — final weighting matrix W\nn_instruments::Int — number of moment conditions\ndata::PanelData{T} — original panel data\n\nReferences\n\nHoltz-Eakin, Newey & Rosen (1988), Econometrica 56(6), 1371-1395.\nArellano & Bond (1991), Review of Economic Studies 58(2), 277-297.\nBlundell & Bond (1998), Journal of Econometrics 87(1), 115-143.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PVARStability","page":"Types","title":"MacroEconometricModels.PVARStability","text":"PVARStability{T} <: Any\n\nEigenvalue stability analysis for a PVARModel.\n\nFields\n\neigenvalues::Vector{Complex{T}} — eigenvalues of companion matrix\nmoduli::Vector{T} — moduli |λ|\nis_stable::Bool — true if all |λ| < 1\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PVARTestResult","page":"Types","title":"MacroEconometricModels.PVARTestResult","text":"PVARTestResult{T} <: StatsAPI.HypothesisTest\n\nSpecification test result for Panel VAR (Hansen J-test, etc.).\n\nFields\n\ntest_name::String — e.g. \"Hansen J-test\"\nstatistic::T — test statistic value\npvalue::T — p-value\ndf::Int — degrees of freedom\nn_instruments::Int — number of instruments (moment conditions)\nn_params::Int — number of estimated parameters\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractGMMModel","page":"Types","title":"MacroEconometricModels.AbstractGMMModel","text":"Abstract supertype for GMM models.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GMMModel","page":"Types","title":"MacroEconometricModels.GMMModel","text":"GMMModel{T} <: AbstractGMMModel\n\nGeneralized Method of Moments estimator.\n\nMinimizes: g(θ)'W g(θ) where g(θ) = (1/n) Σᵢ gᵢ(θ)\n\nFields:\n\ntheta: Parameter estimates\nvcov: Asymptotic covariance matrix\nn_moments: Number of moment conditions\nn_params: Number of parameters\nn_obs: Number of observations\nweighting: Weighting specification\nW: Final weighting matrix\ng_bar: Sample moment vector at solution\nJ_stat: Hansen's J-test statistic\nJ_pvalue: p-value for J-test\nconverged: Convergence flag\niterations: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GMMWeighting","page":"Types","title":"MacroEconometricModels.GMMWeighting","text":"GMMWeighting{T} <: Any\n\nGMM weighting matrix specification.\n\nFields:\n\nmethod: Weighting method (:identity, :optimal, :two_step, :iterated)\nmax_iter: Maximum iterations for iterated GMM\ntol: Convergence tolerance\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.MinnesotaHyperparameters","page":"Types","title":"MacroEconometricModels.MinnesotaHyperparameters","text":"MinnesotaHyperparameters{T} <: AbstractPrior\n\nMinnesota prior hyperparameters: tau (tightness), decay, lambda (sum-of-coef), mu (co-persistence), omega (covariance).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractPrior","page":"Types","title":"MacroEconometricModels.AbstractPrior","text":"Abstract supertype for Bayesian prior specifications.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.BVARPosterior","page":"Types","title":"MacroEconometricModels.BVARPosterior","text":"BVARPosterior{T} <: Any\n\nPosterior draws from Bayesian VAR estimation.\n\nReplaces MCMCChains.Chains — stores i.i.d. or Gibbs draws from the Normal-Inverse-Wishart posterior directly.\n\nFields\n\nB_draws::Array{T,3}: Coefficient draws (n_draws × k × n)\nSigma_draws::Array{T,3}: Covariance draws (n_draws × n × n)\nn_draws::Int: Number of posterior draws\np::Int: Number of VAR lags\nn::Int: Number of variables\ndata::Matrix{T}: Original Y matrix (for residual computation downstream)\nprior::Symbol: Prior used (:normal or :minnesota)\nsampler::Symbol: Sampler used (:direct or :gibbs)\nvarnames::Vector{String}: Variable names\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractCovarianceEstimator","page":"Types","title":"MacroEconometricModels.AbstractCovarianceEstimator","text":"Abstract supertype for covariance estimators.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NeweyWestEstimator","page":"Types","title":"MacroEconometricModels.NeweyWestEstimator","text":"NeweyWestEstimator{T} <: AbstractCovarianceEstimator\n\nNewey-West HAC covariance estimator configuration.\n\nFields:\n\nbandwidth: Truncation lag (0 = automatic via Newey-West 1994 formula)\nkernel: Kernel function (:bartlett, :parzen, :quadraticspectral, :tukeyhanning)\nprewhiten: Use AR(1) prewhitening\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.WhiteEstimator","page":"Types","title":"MacroEconometricModels.WhiteEstimator","text":"WhiteEstimator <: AbstractCovarianceEstimator\n\nWhite heteroscedasticity-robust covariance estimator (HC0). Does not correct for serial correlation.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.DriscollKraayEstimator","page":"Types","title":"MacroEconometricModels.DriscollKraayEstimator","text":"DriscollKraayEstimator{T} <: AbstractCovarianceEstimator\n\nDriscoll-Kraay standard errors for panel data with cross-sectional dependence.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractUnitRootTest","page":"Types","title":"MacroEconometricModels.AbstractUnitRootTest","text":"Abstract supertype for all unit root test results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ADFResult","page":"Types","title":"MacroEconometricModels.ADFResult","text":"ADFResult{T} <: AbstractUnitRootTest\n\nAugmented Dickey-Fuller test result.\n\nFields:\n\nstatistic: ADF test statistic (t-ratio on γ)\npvalue: Approximate p-value (MacKinnon 1994, 2010)\nlags: Number of augmenting lags used\nregression: Regression specification (:none, :constant, :trend)\ncritical_values: Critical values at 1%, 5%, 10% levels\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.KPSSResult","page":"Types","title":"MacroEconometricModels.KPSSResult","text":"KPSSResult{T} <: AbstractUnitRootTest\n\nKPSS stationarity test result.\n\nFields:\n\nstatistic: KPSS test statistic\npvalue: Approximate p-value\nregression: Regression specification (:constant, :trend)\ncritical_values: Critical values at 1%, 5%, 10% levels\nbandwidth: Bartlett kernel bandwidth used\nnobs: Number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.PPResult","page":"Types","title":"MacroEconometricModels.PPResult","text":"PPResult{T} <: AbstractUnitRootTest\n\nPhillips-Perron test result.\n\nFields:\n\nstatistic: PP test statistic (Zt or Zα)\npvalue: Approximate p-value\nregression: Regression specification (:none, :constant, :trend)\ncritical_values: Critical values at 1%, 5%, 10% levels\nbandwidth: Newey-West bandwidth used\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ZAResult","page":"Types","title":"MacroEconometricModels.ZAResult","text":"ZAResult{T} <: AbstractUnitRootTest\n\nZivot-Andrews structural break unit root test result.\n\nFields:\n\nstatistic: Minimum t-statistic across all break points\npvalue: Approximate p-value\nbreak_index: Index of estimated structural break\nbreak_fraction: Break point as fraction of sample\nregression: Break specification (:constant, :trend, :both)\ncritical_values: Critical values at 1%, 5%, 10% levels\nlags: Number of augmenting lags\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NgPerronResult","page":"Types","title":"MacroEconometricModels.NgPerronResult","text":"NgPerronResult{T} <: AbstractUnitRootTest\n\nNg-Perron unit root test result (MZα, MZt, MSB, MPT).\n\nFields:\n\nMZa: Modified Zα statistic\nMZt: Modified Zt statistic\nMSB: Modified Sargan-Bhargava statistic\nMPT: Modified Point-optimal statistic\nregression: Regression specification (:constant, :trend)\ncritical_values: Dict mapping statistic name to critical values\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.JohansenResult","page":"Types","title":"MacroEconometricModels.JohansenResult","text":"JohansenResult{T} <: AbstractUnitRootTest\n\nJohansen cointegration test result.\n\nFields:\n\ntrace_stats: Trace test statistics for each rank\ntrace_pvalues: P-values for trace tests\nmax_eigen_stats: Maximum eigenvalue test statistics\nmax_eigen_pvalues: P-values for max eigenvalue tests\nrank: Estimated cointegration rank (at 5% level)\neigenvectors: Cointegrating vectors (β), columns are vectors\nadjustment: Adjustment coefficients (α)\neigenvalues: Eigenvalues from reduced rank regression\ncritical_values_trace: Critical values for trace test (rows: ranks, cols: 10%, 5%, 1%)\ncritical_values_max: Critical values for max eigenvalue test\ndeterministic: Deterministic specification\nlags: Number of lags in VECM\nnobs: Effective number of observations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VARStationarityResult","page":"Types","title":"MacroEconometricModels.VARStationarityResult","text":"VARStationarityResult{T}\n\nVAR model stationarity check result.\n\nFields:\n\nis_stationary: true if all eigenvalues have modulus < 1\neigenvalues: Eigenvalues of companion matrix (may be real or complex)\nmax_modulus: Maximum eigenvalue modulus\ncompanion_matrix: The companion form matrix F\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LRTestResult","page":"Types","title":"MacroEconometricModels.LRTestResult","text":"LRTestResult{T} <: StatsAPI.HypothesisTest\n\nResult from a likelihood ratio test comparing nested models.\n\nFields\n\nstatistic::T: LR statistic = -2(ℓR - ℓU)\npvalue::T: p-value from χ²(df) distribution\ndf::Int: Degrees of freedom (dofU - dofR)\nloglik_restricted::T: Log-likelihood of restricted model\nloglik_unrestricted::T: Log-likelihood of unrestricted model\ndof_restricted::Int: Parameters in restricted model\ndof_unrestricted::Int: Parameters in unrestricted model\nnobs_restricted::Int: Observations in restricted model\nnobs_unrestricted::Int: Observations in unrestricted model\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.LMTestResult","page":"Types","title":"MacroEconometricModels.LMTestResult","text":"LMTestResult{T} <: StatsAPI.HypothesisTest\n\nResult from a Lagrange multiplier (score) test comparing nested models.\n\nFields\n\nstatistic::T: LM statistic = s'(-H)⁻¹s\npvalue::T: p-value from χ²(df) distribution\ndf::Int: Degrees of freedom (dofU - dofR)\nnobs::Int: Number of observations\nscore_norm::T: ‖s‖₂ diagnostic (Euclidean norm of score vector)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GrangerCausalityResult","page":"Types","title":"MacroEconometricModels.GrangerCausalityResult","text":"GrangerCausalityResult{T} <: StatsAPI.HypothesisTest\n\nResult from a Granger causality test in a VAR model.\n\nFields\n\nstatistic::T: Wald χ² statistic\npvalue::T: p-value from χ²(df) distribution\ndf::Int: Degrees of freedom (number of restrictions)\ncause::Vector{Int}: Indices of causing variable(s)\neffect::Int: Index of effect variable\nn::Int: Number of variables in VAR\np::Int: Lag order\nnobs::Int: Effective number of observations\ntest_type::Symbol: :pairwise or :block\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractNowcastModel","page":"Types","title":"MacroEconometricModels.AbstractNowcastModel","text":"Abstract supertype for nowcasting models (DFM, BVAR, Bridge).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NowcastDFM","page":"Types","title":"MacroEconometricModels.NowcastDFM","text":"NowcastDFM{T<:AbstractFloat} <: AbstractNowcastModel\n\nDynamic factor model nowcasting result (Bańbura & Modugno 2014).\n\nEstimates factors from mixed-frequency data with arbitrary missing patterns using EM algorithm + Kalman smoother.\n\nFields\n\nX_sm::Matrix{T} — smoothed data (NaN filled)\nF::Matrix{T} — smoothed factors (Tobs × statedim)\nC::Matrix{T} — observation loadings\nA::Matrix{T} — state transition matrix\nQ::Matrix{T} — state innovation covariance\nR::Matrix{T} — observation noise covariance (diagonal)\nMx::Vector{T} — data column means (for standardization)\nWx::Vector{T} — data column stds\nZ_0::Vector{T} — initial state mean\nV_0::Matrix{T} — initial state covariance\nr::Int — number of factors\np::Int — VAR lags in factor dynamics\nblocks::Matrix{Int} — block structure (N × n_blocks)\nloglik::T — log-likelihood at convergence\nn_iter::Int — EM iterations used\nnM::Int — number of monthly variables\nnQ::Int — number of quarterly variables\nidio::Symbol — idiosyncratic spec (:ar1 or :iid)\ndata::Matrix{T} — original data with NaN\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NowcastBVAR","page":"Types","title":"MacroEconometricModels.NowcastBVAR","text":"NowcastBVAR{T<:AbstractFloat} <: AbstractNowcastModel\n\nLarge Bayesian VAR nowcasting result (Cimadomo et al. 2022).\n\nUses GLP-style normal-inverse-Wishart prior with hyperparameter optimization via marginal likelihood maximization.\n\nFields\n\nX_sm::Matrix{T} — smoothed data (NaN filled)\nbeta::Matrix{T} — posterior mode VAR coefficients\nsigma::Matrix{T} — posterior mode error covariance\nlambda::T — overall shrinkage\ntheta::T — cross-variable shrinkage\nmiu::T — sum-of-coefficients prior weight\nalpha::T — co-persistence prior weight\nlags::Int — number of lags\nloglik::T — marginal log-likelihood\nnM::Int — number of monthly variables\nnQ::Int — number of quarterly variables\ndata::Matrix{T} — original data with NaN\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NowcastBridge","page":"Types","title":"MacroEconometricModels.NowcastBridge","text":"NowcastBridge{T<:AbstractFloat} <: AbstractNowcastModel\n\nBridge equation combination nowcasting result (Bańbura et al. 2023).\n\nCombines multiple OLS bridge regressions (each using a pair of monthly indicators) via median combination.\n\nFields\n\nX_sm::Matrix{T} — smoothed data (NaN filled by interpolation)\nY_nowcast::Vector{T} — combined nowcast for target variable (per quarter)\nY_individual::Matrix{T} — individual equation nowcasts (nquarters × nequations)\nn_equations::Int — number of bridge equations\ncoefficients::Vector{Vector{T}} — OLS coefficients per equation\nnM::Int — number of monthly variables\nnQ::Int — number of quarterly variables\nlagM::Int — monthly indicator lags\nlagQ::Int — quarterly indicator lags\nlagY::Int — autoregressive lags\ndata::Matrix{T} — original data with NaN\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NowcastResult","page":"Types","title":"MacroEconometricModels.NowcastResult","text":"NowcastResult{T<:AbstractFloat}\n\nUnified nowcast result wrapping any AbstractNowcastModel.\n\nFields\n\nmodel::AbstractNowcastModel — underlying model\nX_sm::Matrix{T} — smoothed/nowcasted data\ntarget_index::Int — column index of target variable\nnowcast::T — current-quarter nowcast value\nforecast::T — next-quarter forecast value\nmethod::Symbol — :dfm, :bvar, or :bridge\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NowcastNews","page":"Types","title":"MacroEconometricModels.NowcastNews","text":"NowcastNews{T<:AbstractFloat}\n\nNews decomposition result (Bańbura & Modugno 2014).\n\nDecomposes nowcast revision into contributions from new data releases (news), data revisions, and parameter re-estimation.\n\nFields\n\nold_nowcast::T — previous nowcast value\nnew_nowcast::T — updated nowcast value\nimpact_news::Vector{T} — per-release news impact\nimpact_revision::T — data revision impact\nimpact_reestimation::T — parameter re-estimation impact (residual)\ngroup_impacts::Vector{T} — news aggregated by variable group\nvariable_names::Vector{String} — names for each news release\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ZeroRestriction","page":"Types","title":"MacroEconometricModels.ZeroRestriction","text":"Zero restriction: variable doesn't respond to shock at horizon.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SignRestriction","page":"Types","title":"MacroEconometricModels.SignRestriction","text":"Sign restriction: variable response to shock has required sign at horizon.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SVARRestrictions","page":"Types","title":"MacroEconometricModels.SVARRestrictions","text":"Container for SVAR restrictions.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AriasSVARResult","page":"Types","title":"MacroEconometricModels.AriasSVARResult","text":"Result from Arias et al. (2018) identification.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractVolatilityModel","page":"Types","title":"MacroEconometricModels.AbstractVolatilityModel","text":"Abstract supertype for univariate volatility models (ARCH/GARCH/SV).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ARCHModel","page":"Types","title":"MacroEconometricModels.ARCHModel","text":"ARCHModel{T} <: AbstractVolatilityModel\n\nARCH(q) model (Engle 1982): εₜ = σₜ zₜ, σ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q\n\nFields\n\ny::Vector{T}: Original data\nq::Int: ARCH order\nmu::T: Mean (intercept)\nomega::T: Variance intercept (ω > 0)\nalpha::Vector{T}: ARCH coefficients [α₁, ..., αq]\nconditional_variance::Vector{T}: Estimated conditional variances σ²ₜ\nstandardized_residuals::Vector{T}: Standardized residuals zₜ = εₜ/σₜ\nresiduals::Vector{T}: Raw residuals εₜ = yₜ - μ\nfitted::Vector{T}: Fitted values (mean)\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GARCHModel","page":"Types","title":"MacroEconometricModels.GARCHModel","text":"GARCHModel{T} <: AbstractVolatilityModel\n\nGARCH(p,q) model (Bollerslev 1986): σ²ₜ = ω + α₁ε²ₜ₋₁ + ... + αqε²ₜ₋q + β₁σ²ₜ₋₁ + ... + βpσ²ₜ₋p\n\nFields\n\ny::Vector{T}: Original data\np::Int: GARCH order (lagged variances)\nq::Int: ARCH order (lagged squared residuals)\nmu::T: Mean (intercept)\nomega::T: Variance intercept (ω > 0)\nalpha::Vector{T}: ARCH coefficients [α₁, ..., αq]\nbeta::Vector{T}: GARCH coefficients [β₁, ..., βp]\nconditional_variance::Vector{T}: Estimated conditional variances σ²ₜ\nstandardized_residuals::Vector{T}: Standardized residuals zₜ = εₜ/σₜ\nresiduals::Vector{T}: Raw residuals εₜ = yₜ - μ\nfitted::Vector{T}: Fitted values (mean)\nloglik::T: Log-likelihood\naic::T: Akaike Information Criterion\nbic::T: Bayesian Information Criterion\nmethod::Symbol: Estimation method\nconverged::Bool: Whether optimization converged\niterations::Int: Number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.EGARCHModel","page":"Types","title":"MacroEconometricModels.EGARCHModel","text":"EGARCHModel{T} <: AbstractVolatilityModel\n\nEGARCH(p,q) model (Nelson 1991): log(σ²ₜ) = ω + Σαᵢ(|zₜ₋ᵢ| - E|zₜ₋ᵢ|) + Σγᵢzₜ₋ᵢ + Σβⱼlog(σ²ₜ₋ⱼ)\n\nThe log specification ensures σ² > 0 without parameter constraints, and γᵢ captures leverage effects (typically γ < 0).\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GJRGARCHModel","page":"Types","title":"MacroEconometricModels.GJRGARCHModel","text":"GJRGARCHModel{T} <: AbstractVolatilityModel\n\nGJR-GARCH(p,q) model (Glosten, Jagannathan & Runkle 1993): σ²ₜ = ω + Σ(αᵢ + γᵢI(εₜ₋ᵢ < 0))ε²ₜ₋ᵢ + Σβⱼσ²ₜ₋ⱼ\n\nγᵢ > 0 means negative shocks increase variance more than positive shocks.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SVModel","page":"Types","title":"MacroEconometricModels.SVModel","text":"SVModel{T} <: AbstractVolatilityModel\n\nStochastic Volatility model (Taylor 1986), estimated via Kim-Shephard-Chib (1998) Gibbs sampler:     yₜ = exp(hₜ/2) εₜ,       εₜ ~ N(0,1)     hₜ = μ + φ(hₜ₋₁ - μ) + σ_η ηₜ,  ηₜ ~ N(0,1)\n\nFields\n\ny::Vector{T}: Original data\nh_draws::Matrix{T}: Posterior draws of latent log-volatilities (nsamples × nobs)\nmu_post::Vector{T}: Posterior draws of μ (log-variance level)\nphi_post::Vector{T}: Posterior draws of φ (persistence)\nsigma_eta_post::Vector{T}: Posterior draws of σ_η (volatility of volatility)\nvolatility_mean::Vector{T}: Posterior mean of exp(hₜ) at each time t\nvolatility_quantiles::Matrix{T}: Quantiles of exp(hₜ) (T × n_quantiles)\nquantile_levels::Vector{T}: Quantile levels (e.g., [0.025, 0.5, 0.975])\ndist::Symbol: Error distribution (:normal or :studentt)\nleverage::Bool: Whether leverage effect was estimated\nn_samples::Int: Number of posterior samples\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.VolatilityForecast","page":"Types","title":"MacroEconometricModels.VolatilityForecast","text":"VolatilityForecast{T}\n\nForecast result from a volatility model.\n\nFields\n\nforecast::Vector{T}: Point forecasts of conditional variance\nci_lower::Vector{T}: Lower confidence interval bound\nci_upper::Vector{T}: Upper confidence interval bound\nse::Vector{T}: Standard errors of forecasts\nhorizon::Int: Forecast horizon\nconf_level::T: Confidence level (e.g., 0.95)\nmodel_type::Symbol: Source model type (:arch, :garch, :egarch, :gjr_garch, :sv)\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractNormalityTest","page":"Types","title":"MacroEconometricModels.AbstractNormalityTest","text":"Abstract supertype for multivariate normality test results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.AbstractNonGaussianSVAR","page":"Types","title":"MacroEconometricModels.AbstractNonGaussianSVAR","text":"Abstract supertype for non-Gaussian SVAR identification results.\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NormalityTestResult","page":"Types","title":"MacroEconometricModels.NormalityTestResult","text":"NormalityTestResult{T} <: AbstractNormalityTest\n\nResult of a multivariate normality test.\n\nFields:\n\ntest_name::Symbol — :jarque_bera, :mardia_skewness, :mardia_kurtosis, :doornik_hansen, :henze_zirkler\nstatistic::T — test statistic\npvalue::T — p-value\ndf::Int — degrees of freedom (for chi-squared tests)\nn_vars::Int — number of variables\nn_obs::Int — number of observations\ncomponents::Union{Nothing, Vector{T}} — per-component statistics (for component-wise tests)\ncomponent_pvalues::Union{Nothing, Vector{T}} — per-component p-values\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NormalityTestSuite","page":"Types","title":"MacroEconometricModels.NormalityTestSuite","text":"NormalityTestSuite{T}\n\nCollection of normality test results from normality_test_suite.\n\nFields:\n\nresults::Vector{NormalityTestResult{T}} — individual test results\nresiduals::Matrix{T} — the residual matrix tested\nn_vars::Int\nn_obs::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ICASVARResult","page":"Types","title":"MacroEconometricModels.ICASVARResult","text":"ICASVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from ICA-based SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix (n × n): ut = B₀ εt\nW::Matrix{T} — unmixing matrix (n × n): εt = W ut\nQ::Matrix{T} — rotation matrix for compute_Q integration\nshocks::Matrix{T} — recovered structural shocks (T_eff × n)\nmethod::Symbol — :fastica, :jade, :sobi, :dcov, :hsic\nconverged::Bool\niterations::Int\nobjective::T — final objective value\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.NonGaussianMLResult","page":"Types","title":"MacroEconometricModels.NonGaussianMLResult","text":"NonGaussianMLResult{T} <: AbstractNonGaussianSVAR\n\nResult from non-Gaussian maximum likelihood SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix (n × n)\nQ::Matrix{T} — rotation matrix\nshocks::Matrix{T} — structural shocks (T_eff × n)\ndistribution::Symbol — :student_t, :mixture_normal, :pml, :skew_normal\nloglik::T — log-likelihood at MLE\nloglik_gaussian::T — Gaussian log-likelihood (for LR test)\ndist_params::Dict{Symbol, Any} — distribution parameters\nvcov::Matrix{T} — asymptotic covariance of B₀ elements\nse::Matrix{T} — standard errors for B₀\nconverged::Bool\niterations::Int\naic::T\nbic::T\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.MarkovSwitchingSVARResult","page":"Types","title":"MacroEconometricModels.MarkovSwitchingSVARResult","text":"MarkovSwitchingSVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from Markov-switching heteroskedasticity SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\nSigma_regimes::Vector{Matrix{T}} — covariance per regime\nLambda::Vector{Vector{T}} — relative variances per regime\nregime_probs::Matrix{T} — smoothed regime probabilities (T × K)\ntransition_matrix::Matrix{T} — Markov transition probabilities (K × K)\nloglik::T\nconverged::Bool\niterations::Int\nn_regimes::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.GARCHSVARResult","page":"Types","title":"MacroEconometricModels.GARCHSVARResult","text":"GARCHSVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from GARCH-based SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\ngarch_params::Matrix{T} — (n × 3): [ω, α, β] per shock\ncond_var::Matrix{T} — (T_eff × n) conditional variances\nshocks::Matrix{T} — structural shocks\nloglik::T\nconverged::Bool\niterations::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.SmoothTransitionSVARResult","page":"Types","title":"MacroEconometricModels.SmoothTransitionSVARResult","text":"SmoothTransitionSVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from smooth-transition heteroskedasticity SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\nSigma_regimes::Vector{Matrix{T}} — covariance matrices for extreme regimes\nLambda::Vector{Vector{T}} — relative variances per regime\ngamma::T — transition speed parameter\nthreshold::T — transition location parameter\ntransition_var::Vector{T} — transition variable values\nG_values::Vector{T} — transition function G(s_t) values\nloglik::T\nconverged::Bool\niterations::Int\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.ExternalVolatilitySVARResult","page":"Types","title":"MacroEconometricModels.ExternalVolatilitySVARResult","text":"ExternalVolatilitySVARResult{T} <: AbstractNonGaussianSVAR\n\nResult from external volatility instrument SVAR identification.\n\nFields:\n\nB0::Matrix{T} — structural impact matrix\nQ::Matrix{T} — rotation matrix\nSigma_regimes::Vector{Matrix{T}} — covariance per regime\nLambda::Vector{Vector{T}} — relative variances per regime\nregime_indices::Vector{Vector{Int}} — observation indices per regime\nloglik::T\n\n\n\n\n\n","category":"type"},{"location":"api_types/#MacroEconometricModels.IdentifiabilityTestResult","page":"Types","title":"MacroEconometricModels.IdentifiabilityTestResult","text":"IdentifiabilityTestResult{T}\n\nResult from an identifiability or specification test.\n\nFields:\n\ntest_name::Symbol — test identifier\nstatistic::T — test statistic\npvalue::T — p-value\nidentified::Bool — whether identification appears to hold\ndetails::Dict{Symbol, Any} — method-specific details\n\n\n\n\n\n","category":"type"},{"location":"nongaussian/#Statistical-Identification-via-Higher-Moments","page":"Statistical Identification","title":"Statistical Identification via Higher Moments","text":"This page covers identification of structural VAR models using statistical properties beyond second moments: heteroskedasticity (time-varying variances) and non-Gaussianity (higher-order moments). These methods provide identification without requiring the recursive ordering of Cholesky or the a priori sign/zero restrictions of traditional SVAR.\n\nThe classification follows Lewis (2025), which provides the definitive taxonomy of statistical identification in macroeconometrics. The key insight is that the standard reduced-form covariance Sigma = B_0 B_0 provides only n(n+1)2 equations for n^2 unknowns in B_0. The two strands of statistical identification resolve this in complementary ways:\n\nHeteroskedasticity (Section 2): exploits multiple covariance matrices from different volatility regimes\nNon-Gaussianity (Section 3): exploits higher-moment conditions (coskewness, cokurtosis) from non-Gaussian shocks","category":"section"},{"location":"nongaussian/#Quick-Start","page":"Statistical Identification","title":"Quick Start","text":"using MacroEconometricModels\n\n# Multivariate normality tests (diagnostics)\nsuite = normality_test_suite(model)                # Run all 7 tests\njb = jarque_bera_test(model)                       # Multivariate Jarque-Bera\n\n# Non-Gaussianity: ICA-based identification\nica = identify_fastica(model)                      # FastICA (Hyvärinen 1999)\njade = identify_jade(model)                        # JADE (Cardoso 1993)\n\n# Non-Gaussianity: ML identification\nml = identify_student_t(model)                     # Student-t shocks\nml = identify_nongaussian_ml(model; distribution=:mixture_normal)\n\n# Heteroskedasticity: regime-based identification\nms = identify_markov_switching(model; n_regimes=2) # Markov-switching (Lanne & Lütkepohl 2008)\nev = identify_external_volatility(model, regime)   # Known volatility regimes (Rigobon 2003)\n\n# Identifiability tests\ntest_shock_gaussianity(ica)                        # Are shocks non-Gaussian?\ntest_gaussian_vs_nongaussian(model)                # LR test: Gaussian vs non-Gaussian\ntest_shock_independence(ica)                       # Are shocks independent?\n\n# Integration with existing IRF pipeline\nirfs = irf(model, 20; method=:fastica)             # Works automatically via compute_Q\n\n","category":"section"},{"location":"nongaussian/#The-SVAR-Setting","page":"Statistical Identification","title":"The SVAR Setting","text":"The structural VAR has the decomposition:\n\nu_t = B_0 varepsilon_t quad Sigma = B_0 B_0\n\nwhere\n\nu_t is the n times 1 vector of reduced-form residuals\nvarepsilon_t is the n times 1 vector of structural shocks (unit variance, mutually independent)\nB_0 is the n times n structural impact matrix\n\nThe reduced-form covariance Sigma = B_0 B_0 provides n(n+1)2 equations for n^2 unknowns in B_0, leaving n(n-1)2 free parameters. Traditional approaches (Cholesky, sign/zero restrictions) resolve this by imposing economic constraints. Statistical identification takes a different path:\n\nHeteroskedasticity: if shock variances change across regimes, each regime provides a separate covariance equation Sigma_k = B_0 Lambda_k B_0, generating enough equations to identify B_0\nNon-Gaussianity: if shocks are non-Gaussian, independence imposes conditions beyond uncorrelatedness — coskewness, cokurtosis, and higher moments pin down B_0\n\nnote: Technical Note\nLewis (2025) shows that identification via non-Gaussianity can be thought of as a special case of identification based on heteroskedasticity (p. 674). The Darmois-Skitovich theorem (Comon 1994) establishes that if at most one component is Gaussian and shocks are independent, B_0 is unique up to column permutation and sign.\n\n","category":"section"},{"location":"nongaussian/#Identification-via-Heteroskedasticity","page":"Statistical Identification","title":"Identification via Heteroskedasticity","text":"\"If variances of structural shocks change through time, then there is not just a single reduced-form covariance matrix to exploit.\" — Lewis (2025, Section 3)\n\nWhen the structural shock variances change across K regimes while B_0 remains constant, we have:\n\nSigma_k = B_0 Lambda_k B_0 quad k = 1 ldots K\n\nwhere Lambda_k = textdiag(lambda_1k ldots lambda_nk) are regime-specific variance matrices. With K geq 2 regimes, the eigendecomposition of Sigma_1^-1 Sigma_2 identifies B_0 up to column permutation and sign, provided eigenvalues are distinct (Rigobon 2003).","category":"section"},{"location":"nongaussian/#Eigendecomposition-Identification","page":"Statistical Identification","title":"Eigendecomposition Identification","text":"The core idea (Rigobon 2003): given two regime covariance matrices Sigma_1 and Sigma_2, the eigendecomposition of Sigma_1^-1Sigma_2 yields:\n\nSigma_1^-1Sigma_2 = V D V^-1\n\nwhere\n\nV contains the eigenvectors\nD = textdiag(lambda_1 ldots lambda_n) contains the relative variance ratios\nB_0 = Sigma_1^12 V (with normalization)\n\nIdentification condition: The eigenvalues lambda_j must be distinct.","category":"section"},{"location":"nongaussian/#Markov-Switching-Volatility","page":"Statistical Identification","title":"Markov-Switching Volatility","text":"Estimates regime-specific covariance matrices via the Hamilton (1989) filter with EM algorithm:\n\nusing MacroEconometricModels\n\n# Load FRED-MD monetary policy model\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\n\nms = identify_markov_switching(model; n_regimes=2)\nprintln(\"Transition matrix:\")\nprintln(round.(ms.transition_matrix, digits=3))\nprintln(\"Regime probabilities (first 5 obs):\")\nprintln(round.(ms.regime_probs[1:5, :], digits=3))\n\nThe EM algorithm iterates:\n\nE-step: Hamilton filter (forward) + Kim smoother (backward) → regime probabilities\nM-step: Update regime covariances and transition matrix given probabilities\n\nReference: Lanne & Lütkepohl (2008)","category":"section"},{"location":"nongaussian/#GARCH-Based-Identification","page":"Statistical Identification","title":"GARCH-Based Identification","text":"Uses GARCH(1,1) conditional heteroskedasticity in the structural shocks for identification:\n\nh_jt = omega_j + alpha_j varepsilon_jt-1^2 + beta_j h_jt-1\n\ngarch = identify_garch(model)\nprintln(\"GARCH parameters (ω, α, β):\")\nfor j in 1:size(garch.garch_params, 1)\n    println(\"  Shock $j: \", round.(garch.garch_params[j, :], digits=4))\nend\n\nReference: Normandin & Phaneuf (2004)","category":"section"},{"location":"nongaussian/#Smooth-Transition","page":"Statistical Identification","title":"Smooth Transition","text":"The covariance varies smoothly between two regimes via a logistic transition function:\n\nSigma_t = B_0 I + G(s_t)(Lambda - I) B_0\n\nwhere G(s_t) = 1(1 + exp(-gamma(s_t - c))) is the logistic transition function.\n\n# Use a lagged variable as the transition variable\ns = Y[2:end, 1]  # first variable, lagged\nst = identify_smooth_transition(model, s)\nprintln(\"Transition speed γ = $(round(st.gamma, digits=3))\")\nprintln(\"Threshold c = $(round(st.threshold, digits=3))\")\n\nReference: Lütkepohl & Netšunajev (2017)","category":"section"},{"location":"nongaussian/#External-Volatility-Instruments","page":"Statistical Identification","title":"External Volatility Instruments","text":"When volatility regimes are known a priori (e.g., NBER recession dates, financial crisis indicators):\n\n# Binary regime indicator\nregime = vcat(fill(1, 100), fill(2, 100))  # first half = regime 1\nev = identify_external_volatility(model, regime)\n\nThis is the simplest heteroskedasticity method — it just splits the sample and applies eigendecomposition identification.\n\nReference: Rigobon (2003)","category":"section"},{"location":"nongaussian/#Heteroskedasticity-Result-Fields","page":"Statistical Identification","title":"Heteroskedasticity Result Fields","text":"Markov-Switching (MarkovSwitchingSVARResult):\n\nField Type Description\nB0 Matrix{T} Structural impact matrix\nQ Matrix{T} Rotation matrix\nSigma_regimes Vector{Matrix{T}} Covariance per regime\nLambda Vector{Vector{T}} Relative variances per regime\nregime_probs Matrix{T} Smoothed regime probabilities (T × K)\ntransition_matrix Matrix{T} Markov transition probabilities (K × K)\nloglik T Log-likelihood\nconverged Bool Convergence status\nn_regimes Int Number of regimes\n\nGARCH (GARCHSVARResult):\n\nField Type Description\nB0 Matrix{T} Structural impact matrix\nQ Matrix{T} Rotation matrix\ngarch_params Matrix{T} (n × 3): [ω, α, β] per shock\ncond_var Matrix{T} (T × n) conditional variances\nshocks Matrix{T} Structural shocks\nloglik T Log-likelihood\n\nSmooth Transition (SmoothTransitionSVARResult):\n\nField Type Description\nB0 Matrix{T} Structural impact matrix\ngamma T Transition speed parameter\nthreshold T Transition location parameter\nG_values Vector{T} Transition function values\n\nExternal Volatility (ExternalVolatilitySVARResult):\n\nField Type Description\nB0 Matrix{T} Structural impact matrix\nSigma_regimes Vector{Matrix{T}} Covariance per regime\nLambda Vector{Vector{T}} Relative variances per regime\nregime_indices Vector{Vector{Int}} Observation indices per regime\n\n","category":"section"},{"location":"nongaussian/#Identification-via-Non-Gaussianity","page":"Statistical Identification","title":"Identification via Non-Gaussianity","text":"\"Identification via non-Gaussianity can be thought of as a special case of identification based on heteroskedasticity.\" — Lewis (2025, p. 674)\n\nThe Darmois-Skitovich theorem establishes that if varepsilon_t has independent components and at most one is Gaussian, then the mixing matrix B_0 is unique up to column permutation and sign (Comon 1994). This provides identification from a single sample without requiring volatility changes.","category":"section"},{"location":"nongaussian/#ICA-Based-Methods-(Nonparametric)","page":"Statistical Identification","title":"ICA-Based Methods (Nonparametric)","text":"Independent Component Analysis (ICA) identifies B_0 by finding the rotation Q that makes the recovered shocks varepsilon_t = (B_0)^-1 u_t maximally independent and non-Gaussian.","category":"section"},{"location":"nongaussian/#Model-Specification","page":"Statistical Identification","title":"Model Specification","text":"u_t = B_0 varepsilon_t quad B_0 = L Q\n\nwhere L = textchol(Sigma) and Q is orthogonal. ICA searches over orthogonal Q to maximize a measure of non-Gaussianity or independence.\n\nIdentification condition: At most one structural shock may be Gaussian (Lanne, Meitz & Saikkonen 2017). If all shocks are non-Gaussian, B_0 is unique up to column permutation and sign.","category":"section"},{"location":"nongaussian/#FastICA","page":"Statistical Identification","title":"FastICA","text":"FastICA (Hyvärinen 1999) finds the unmixing matrix by maximizing a measure of non-Gaussianity (negentropy) via a fixed-point algorithm.\n\n# Default: logcosh contrast, deflation approach\nica = identify_fastica(model)\n\n# Symmetric approach with exponential contrast\nica = identify_fastica(model; approach=:symmetric, contrast=:exp)\n\nThree contrast functions are available:\n\n:logcosh (default) — robust, good general-purpose choice: G(u) = logcosh(u)\n:exp — better for super-Gaussian sources: G(u) = -exp(-u^22)\n:kurtosis — classical kurtosis-based: G(u) = u^44\n\nTwo extraction approaches:\n\n:deflation — extracts components one at a time (deflation approach)\n:symmetric — extracts all components simultaneously\n\nReference: Hyvärinen (1999)","category":"section"},{"location":"nongaussian/#JADE","page":"Statistical Identification","title":"JADE","text":"JADE (Joint Approximate Diagonalization of Eigenmatrices) uses fourth-order cumulant matrices and joint diagonalization via Jacobi rotations.\n\njade = identify_jade(model)\n\nJADE computes the fourth-order cumulant matrices C_ijkl = textcum(z_k z_l z_i z_j) and finds the orthogonal matrix that simultaneously diagonalizes all of them.\n\nReference: Cardoso & Souloumiac (1993)","category":"section"},{"location":"nongaussian/#SOBI","page":"Statistical Identification","title":"SOBI","text":"SOBI (Second-Order Blind Identification) exploits temporal structure via autocovariance matrices at multiple lags.\n\nsobi = identify_sobi(model; lags=1:12)\n\nUnlike FastICA and JADE which use higher-order statistics, SOBI only uses second-order statistics (autocovariances), making it suitable when temporal dependence is the main source of identifiability.\n\nReference: Belouchrani et al. (1997)","category":"section"},{"location":"nongaussian/#Distance-Covariance","page":"Statistical Identification","title":"Distance Covariance","text":"Minimizes the sum of pairwise distance covariances between recovered shocks. Distance covariance (Székely et al. 2007) is zero if and only if variables are independent.\n\ndcov = identify_dcov(model)\n\nReference: Matteson & Tsay (2017)","category":"section"},{"location":"nongaussian/#HSIC","page":"Statistical Identification","title":"HSIC","text":"Minimizes the Hilbert-Schmidt Independence Criterion using a Gaussian kernel. Like distance covariance, HSIC with a characteristic kernel is zero iff variables are independent.\n\nhsic = identify_hsic(model; sigma=1.0)\n\nThe bandwidth parameter sigma defaults to the median pairwise distance heuristic.\n\nReference: Gretton et al. (2005)","category":"section"},{"location":"nongaussian/#Maximum-Likelihood-Methods-(Parametric)","page":"Statistical Identification","title":"Maximum Likelihood Methods (Parametric)","text":"Instead of the two-step ICA approach, ML methods estimate B_0 and the shock distribution parameters jointly by maximizing the log-likelihood:\n\nell(theta) = sum_t=1^T left logdet(B_0^-1) + sum_j=1^n log f_j(varepsilon_jt theta_j) right\n\nwhere f_j(cdot theta_j) is the marginal density of shock j and theta_j are distribution-specific parameters.","category":"section"},{"location":"nongaussian/#Student-t-Shocks","page":"Statistical Identification","title":"Student-t Shocks","text":"Assumes each shock follows a (standardized) Student-t distribution with shock-specific degrees of freedom nu_j:\n\nml = identify_student_t(model)\nprintln(\"Degrees of freedom: \", ml.dist_params[:nu])\n\nLow nu indicates heavy tails. When nu to infty, the shock approaches Gaussianity. Identification requires that at most one shock has nu = infty.\n\nReference: Lanne, Meitz & Saikkonen (2017)","category":"section"},{"location":"nongaussian/#Mixture-of-Normals","page":"Statistical Identification","title":"Mixture of Normals","text":"Each shock follows a mixture of two normals: varepsilon_j sim p_j N(0 sigma_1j^2) + (1-p_j) N(0 sigma_2j^2) with the unit variance constraint p_j sigma_1j^2 + (1-p_j) sigma_2j^2 = 1.\n\nml = identify_mixture_normal(model)\nprintln(\"Mixing probabilities: \", ml.dist_params[:p_mix])\n\nReference: Lanne & Lütkepohl (2010)","category":"section"},{"location":"nongaussian/#Pseudo-Maximum-Likelihood-(PML)","page":"Statistical Identification","title":"Pseudo Maximum Likelihood (PML)","text":"Uses Pearson Type IV distributions, allowing both skewness and excess kurtosis.\n\nml = identify_pml(model)\n\nReference: Herwartz (2018)","category":"section"},{"location":"nongaussian/#Skew-Normal-Shocks","page":"Statistical Identification","title":"Skew-Normal Shocks","text":"Each shock follows a skew-normal distribution with pdf f(x) = 2phi(x)Phi(alpha_j x).\n\nml = identify_skew_normal(model)\nprintln(\"Skewness parameters: \", ml.dist_params[:alpha])\n\nReference: Azzalini (1985)","category":"section"},{"location":"nongaussian/#Unified-Dispatcher","page":"Statistical Identification","title":"Unified Dispatcher","text":"Use identify_nongaussian_ml to select the distribution at runtime:\n\nfor dist in [:student_t, :mixture_normal, :pml, :skew_normal]\n    ml = identify_nongaussian_ml(model; distribution=dist)\n    println(\"$dist: logL=$(round(ml.loglik, digits=2)), AIC=$(round(ml.aic, digits=2))\")\nend\n\nCompare AIC/BIC across distributions to select the best-fitting specification.","category":"section"},{"location":"nongaussian/#Moment-Based-Approaches","page":"Statistical Identification","title":"Moment-Based Approaches","text":"note: Emerging Direction\nMoment-based GMM estimators (Keweloh 2021, Lanne & Luoto 2021) exploit coskewness and cokurtosis conditions directly, without specifying a parametric distribution. These use conditions like:Evarepsilon_i^2 varepsilon_j = 0 (coskewness)\nEvarepsilon_i^2 varepsilon_j^2 - 1 = 0, Evarepsilon_i^3 varepsilon_j = 0 (cokurtosis)This is an important emerging direction in the literature. See Lewis (2025, Section 4.3) for a comprehensive discussion. Not yet implemented in this package.","category":"section"},{"location":"nongaussian/#ICA-/-ML-Result-Fields","page":"Statistical Identification","title":"ICA / ML Result Fields","text":"ICA (ICASVARResult):\n\nField Type Description\nB0 Matrix{T} Structural impact matrix (n times n)\nW Matrix{T} Unmixing matrix: varepsilon_t = W u_t\nQ Matrix{T} Rotation matrix: B_0 = L Q\nshocks Matrix{T} Recovered structural shocks (T times n)\nmethod Symbol Method used\nconverged Bool Whether the algorithm converged\niterations Int Number of iterations\nobjective T Final objective value\n\nML (NonGaussianMLResult):\n\nField Type Description\nB0 Matrix{T} Structural impact matrix\nQ Matrix{T} Rotation matrix\nshocks Matrix{T} Structural shocks\ndistribution Symbol Distribution used\nloglik T Log-likelihood at MLE\nloglik_gaussian T Gaussian log-likelihood (for LR test)\ndist_params Dict{Symbol,Any} Distribution parameters\nvcov Matrix{T} Asymptotic covariance of parameters\nse Matrix{T} Standard errors for B_0\nconverged Bool Convergence status\naic T Akaike information criterion\nbic T Bayesian information criterion\n\n","category":"section"},{"location":"nongaussian/#Multivariate-Normality-Tests","page":"Statistical Identification","title":"Multivariate Normality Tests","text":"Before applying non-Gaussian SVAR methods, it is essential to verify that the VAR residuals are indeed non-Gaussian. If residuals are Gaussian, non-Gaussian identification will not work (the problem is unidentified). These tests also serve as a prerequisite diagnostic for choosing between heteroskedasticity-based and non-Gaussianity-based approaches.","category":"section"},{"location":"nongaussian/#Multivariate-Jarque-Bera-Test","page":"Statistical Identification","title":"Multivariate Jarque-Bera Test","text":"The multivariate Jarque-Bera test extends the univariate JB test to vector residuals. Under the null hypothesis of multivariate normality, the test statistic is:\n\nJB = T cdot fracb_1k6 + T cdot frac(b_2k - k(k+2))^224k\n\nwhere b_1k is the multivariate skewness measure and b_2k is the multivariate kurtosis measure (Lütkepohl 2005, §4.5).\n\nusing MacroEconometricModels\n\n# Load FRED-MD monetary policy model\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\n\n# Joint test\njb = jarque_bera_test(model)\nprintln(\"Statistic: $(round(jb.statistic, digits=4)), p-value: $(round(jb.pvalue, digits=4))\")\n\n# Component-wise test on standardized residuals\njb_comp = jarque_bera_test(model; method=:component)\nprintln(\"Component p-values: \", round.(jb_comp.component_pvalues, digits=4))\n\nWith macroeconomic data, non-normality is common — rejecting the null supports using non-Gaussian SVAR methods below.","category":"section"},{"location":"nongaussian/#Mardia's-Tests","page":"Statistical Identification","title":"Mardia's Tests","text":"Mardia (1970) proposed separate tests for multivariate skewness and kurtosis:\n\nb_1k = frac1T^2 sum_ij (u_i Sigma^-1 u_j)^3 quad text(skewness)\n\nb_2k = frac1T sum_i (u_i Sigma^-1 u_i)^2 quad text(kurtosis)\n\nUnder H₀: T cdot b_1k6 sim chi^2(k(k+1)(k+2)6) and (b_2k - k(k+2))  sqrt8k(k+2)T sim N(01).\n\nskew_test = mardia_test(model; type=:skewness)\nkurt_test = mardia_test(model; type=:kurtosis)\nboth_test = mardia_test(model; type=:both)\n\nThe :both option combines both tests into a single chi-squared statistic.\n\nReference: Mardia (1970)","category":"section"},{"location":"nongaussian/#Doornik-Hansen-Test","page":"Statistical Identification","title":"Doornik-Hansen Test","text":"The Doornik-Hansen (2008) omnibus test applies the Bowman-Shenton transformation to each component's skewness and kurtosis, producing approximately standard normal transforms z_1 and z_2. The test statistic is:\n\nDH = sum_j=1^k (z_1j^2 + z_2j^2) sim chi^2(2k)\n\ndh = doornik_hansen_test(model)","category":"section"},{"location":"nongaussian/#Henze-Zirkler-Test","page":"Statistical Identification","title":"Henze-Zirkler Test","text":"The Henze-Zirkler (1990) test is based on the empirical characteristic function and is consistent against all alternatives. The test statistic uses a smoothing parameter beta that depends on the sample size and dimension.\n\nhz = henze_zirkler_test(model)","category":"section"},{"location":"nongaussian/#Normality-Test-Suite","page":"Statistical Identification","title":"Normality Test Suite","text":"Run all tests at once with normality_test_suite:\n\nsuite = normality_test_suite(model)\nprintln(suite)\n\nThis runs 7 tests: multivariate JB, component-wise JB, Mardia skewness, Mardia kurtosis, Mardia combined, Doornik-Hansen, and Henze-Zirkler.","category":"section"},{"location":"nongaussian/#Return-Values","page":"Statistical Identification","title":"Return Values","text":"Field Type Description\ntest_name Symbol Test identifier\nstatistic T Test statistic value\npvalue T p-value\ndf Int Degrees of freedom\nn_vars Int Number of variables\nn_obs Int Number of observations\ncomponents Vector{T} or nothing Per-component statistics\ncomponent_pvalues Vector{T} or nothing Per-component p-values\n\n","category":"section"},{"location":"nongaussian/#Identifiability-and-Specification-Tests","page":"Statistical Identification","title":"Identifiability and Specification Tests","text":"","category":"section"},{"location":"nongaussian/#Shock-Gaussianity-Test","page":"Statistical Identification","title":"Shock Gaussianity Test","text":"Tests whether recovered structural shocks are non-Gaussian using univariate Jarque-Bera tests on each shock. Non-Gaussian identification requires at most one Gaussian shock.\n\nica = identify_fastica(model)\nresult = test_shock_gaussianity(ica)\nprintln(\"Number of Gaussian shocks: \", result.details[:n_gaussian])\nprintln(\"Identified: \", result.identified)","category":"section"},{"location":"nongaussian/#Gaussian-vs-Non-Gaussian-LR-Test","page":"Statistical Identification","title":"Gaussian vs Non-Gaussian LR Test","text":"Likelihood ratio test: H_0: Gaussian shocks vs H_1: non-Gaussian shocks.\n\nLR = 2(ell_1 - ell_0) sim chi^2(p)\n\nwhere p is the number of extra distribution parameters.\n\nlr = test_gaussian_vs_nongaussian(model; distribution=:student_t)\nprintln(\"LR statistic: $(round(lr.statistic, digits=4))\")\nprintln(\"p-value: $(round(lr.pvalue, digits=4))\")\n\nRejecting H_0 supports the use of non-Gaussian identification.","category":"section"},{"location":"nongaussian/#Shock-Independence-Test","page":"Statistical Identification","title":"Shock Independence Test","text":"Tests whether recovered shocks are mutually independent using both cross-correlation (portmanteau) and distance covariance tests, combined via Fisher's method.\n\nresult = test_shock_independence(ica; max_lag=10)\nprintln(\"Independent: \", result.identified)  # fail-to-reject = independent","category":"section"},{"location":"nongaussian/#Identification-Strength","page":"Statistical Identification","title":"Identification Strength","text":"Bootstrap test of identification robustness: resamples residuals and measures the stability of the estimated B_0.\n\nresult = test_identification_strength(model; method=:fastica, n_bootstrap=499)\nprintln(\"Median Procrustes distance: $(round(result.statistic, digits=4))\")\n\nSmall distances indicate strong identification.","category":"section"},{"location":"nongaussian/#Overidentification-Test","page":"Statistical Identification","title":"Overidentification Test","text":"Tests consistency of additional restrictions beyond non-Gaussianity.\n\nresult = test_overidentification(model, ica; n_bootstrap=499)\nprintln(\"p-value: $(round(result.pvalue, digits=4))\")","category":"section"},{"location":"nongaussian/#Weak-Identification","page":"Statistical Identification","title":"Weak Identification","text":"warning: Weak Identification\nLewis (2022) shows that weak identification is likely in many empirical applications. When variances change little across regimes, or deviations from Gaussianity are small, the identifying information may be weak. In such cases:Standard Wald tests may have poor size properties\nConfidence intervals may be unreliable\nPoint estimates may be sensitive to specification choicesDiagnostic checks (identification strength test, shock gaussianity test) are essential. See Lewis (2022) for robust inference procedures.\n\n","category":"section"},{"location":"nongaussian/#Integration-with-IRF-Pipeline","page":"Statistical Identification","title":"Integration with IRF Pipeline","text":"All ICA, ML, and heteroskedasticity methods integrate seamlessly with the existing irf, fevd, and historical_decomposition functions via compute_Q:\n\n# Any statistical identification method works as an irf method\nirfs_ica = irf(model, 20; method=:fastica)\nirfs_ml  = irf(model, 20; method=:student_t)\nirfs_ms  = irf(model, 20; method=:markov_switching)\n\n# FEVD and HD also work\ndecomp = fevd(model, 20; method=:fastica)\n\nSupported method symbols: :fastica, :jade, :sobi, :dcov, :hsic, :student_t, :mixture_normal, :pml, :skew_normal, :markov_switching, :garch.\n\n","category":"section"},{"location":"nongaussian/#The-Labeling-Problem","page":"Statistical Identification","title":"The Labeling Problem","text":"Statistical identification (both heteroskedasticity and non-Gaussianity) identifies B_0 only up to column permutation and sign. The columns of B_0 represent structural shocks, but the data alone cannot tell us which column corresponds to which economic shock.\n\nThis means:\n\nEconomic information is still needed to label shocks (e.g., \"this is the monetary policy shock\")\nOur convention: positive diagonal of B_0 (sign normalization)\nColumn ordering may differ across bootstrap replications — the Procrustes distance in test_identification_strength accounts for this\n\nSee Lewis (2025, Section 6.4) for a thorough discussion of the labeling problem.\n\n","category":"section"},{"location":"nongaussian/#Complete-Example","page":"Statistical Identification","title":"Complete Example","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy model\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\n\n# Step 1: Test for non-Gaussianity of VAR residuals\nsuite = normality_test_suite(model)\nprintln(suite)\n\n# Step 2: Try ICA identification (non-Gaussianity approach)\nica = identify_fastica(model)\nprintln(\"\\nFastICA result:\")\nprintln(\"  Converged: \", ica.converged)\nprintln(\"  Q orthogonal: \", round(norm(ica.Q' * ica.Q - I), digits=8))\n\n# Step 3: Verify identification\ngauss = test_shock_gaussianity(ica)\nprintln(\"\\nShock Gaussianity Test:\")\nprintln(\"  Number of Gaussian shocks: \", gauss.details[:n_gaussian])\nprintln(\"  JB p-values: \", round.(gauss.details[:jb_pvals], digits=4))\n\nindep = test_shock_independence(ica; max_lag=5)\nprintln(\"\\nShock Independence Test:\")\nprintln(\"  Independent: \", indep.identified)\nprintln(\"  Fisher p-value: \", round(indep.pvalue, digits=4))\n\n# Step 4: Compare with ML approach (non-Gaussianity, parametric)\nml = identify_student_t(model)\nprintln(\"\\nStudent-t ML:\")\nprintln(\"  ν = \", round.(ml.dist_params[:nu], digits=2))\nprintln(\"  AIC = $(round(ml.aic, digits=2)), BIC = $(round(ml.bic, digits=2))\")\n\nlr = test_gaussian_vs_nongaussian(model)\nprintln(\"\\nGaussian vs Non-Gaussian LR test:\")\nprintln(\"  LR = $(round(lr.statistic, digits=4)), p = $(round(lr.pvalue, digits=4))\")\n\n# Step 5: Try heteroskedasticity approach\nms = identify_markov_switching(model; n_regimes=2)\nprintln(\"\\nMarkov-switching identification:\")\nprintln(\"  Converged: \", ms.converged)\nprintln(\"  Log-likelihood: \", round(ms.loglik, digits=2))\n\n# Step 6: Compute IRFs using preferred method\nirfs = irf(model, 20; method=:fastica)\nprintln(\"\\nIRF size: \", size(irfs.values))\n\n","category":"section"},{"location":"nongaussian/#See-Also","page":"Statistical Identification","title":"See Also","text":"VAR Estimation – Reduced-form VAR and traditional identification methods\nHypothesis Tests – Normality tests for residual diagnostics\nInnovation Accounting – IRF, FEVD, and historical decomposition\nAPI Reference – Complete function signatures","category":"section"},{"location":"nongaussian/#References","page":"Statistical Identification","title":"References","text":"","category":"section"},{"location":"nongaussian/#Survey","page":"Statistical Identification","title":"Survey","text":"Lewis, Daniel J. 2025. \"Identification Based on Higher Moments in Macroeconometrics.\" Annual Review of Economics 17: 665–693. https://doi.org/10.1146/annurev-economics-070124-051419","category":"section"},{"location":"nongaussian/#Heteroskedasticity-Based-Identification","page":"Statistical Identification","title":"Heteroskedasticity-Based Identification","text":"Rigobon, Roberto. 2003. \"Identification through Heteroskedasticity.\" Review of Economics and Statistics 85 (4): 777–792. https://doi.org/10.1162/003465303772815727\nSentana, Enrique, and Gabriele Fiorentini. 2001. \"Identification, Estimation and Testing of Conditionally Heteroskedastic Factor Models.\" Journal of Econometrics 102 (2): 143–164. https://doi.org/10.1016/S0304-4076(01)00051-3\nLanne, Markku, and Helmut Lütkepohl. 2008. \"Identifying Monetary Policy Shocks via Changes in Volatility.\" Journal of Money, Credit and Banking 40 (6): 1131–1149. https://doi.org/10.1111/j.1538-4616.2008.00151.x\nNormandin, Michel, and Louis Phaneuf. 2004. \"Monetary Policy Shocks: Testing Identification Conditions under Time-Varying Conditional Volatility.\" Journal of Monetary Economics 51 (6): 1217–1243. https://doi.org/10.1016/j.jmoneco.2003.11.002\nLütkepohl, Helmut, and Aleksei Netšunajev. 2017. \"Structural Vector Autoregressions with Smooth Transition in Variances.\" Journal of Economic Dynamics and Control 84: 43–57. https://doi.org/10.1016/j.jedc.2017.09.001\nLewis, Daniel J. 2021. \"Identifying Shocks via Time-Varying Volatility.\" Review of Economic Studies 88 (6): 3086–3124. https://doi.org/10.1093/restud/rdab009","category":"section"},{"location":"nongaussian/#Non-Gaussianity-—-ICA-(Nonparametric)","page":"Statistical Identification","title":"Non-Gaussianity — ICA (Nonparametric)","text":"Hyvärinen, Aapo. 1999. \"Fast and Robust Fixed-Point Algorithms for Independent Component Analysis.\" IEEE Transactions on Neural Networks 10 (3): 626–634. https://doi.org/10.1109/72.761722\nCardoso, Jean-François, and Antoine Souloumiac. 1993. \"Blind Beamforming for Non-Gaussian Signals.\" IEE Proceedings-F 140 (6): 362–370. https://doi.org/10.1049/ip-f-2.1993.0054\nBelouchrani, Adel, Karim Abed-Meraim, Jean-François Cardoso, and Eric Moulines. 1997. \"A Blind Source Separation Technique Using Second-Order Statistics.\" IEEE Transactions on Signal Processing 45 (2): 434–444. https://doi.org/10.1109/78.554307\nComon, Pierre. 1994. \"Independent Component Analysis, A New Concept?\" Signal Processing 36 (3): 287–314. https://doi.org/10.1016/0165-1684(94)90029-9","category":"section"},{"location":"nongaussian/#Non-Gaussianity-—-ML-(Parametric)","page":"Statistical Identification","title":"Non-Gaussianity — ML (Parametric)","text":"Lanne, Markku, Mika Meitz, and Pentti Saikkonen. 2017. \"Identification and Estimation of Non-Gaussian Structural Vector Autoregressions.\" Journal of Econometrics 196 (2): 288–304. https://doi.org/10.1016/j.jeconom.2016.06.002\nGourieroux, Christian, Alain Monfort, and Jean-Paul Renne. 2017. \"Statistical Inference for Independent Component Analysis: Application to Structural VAR Models.\" Journal of Econometrics 196 (1): 111–126. https://doi.org/10.1016/j.jeconom.2016.09.007\nLanne, Markku, and Helmut Lütkepohl. 2010. \"Structural Vector Autoregressions with Nonnormal Residuals.\" Journal of Business & Economic Statistics 28 (1): 159–168. https://doi.org/10.1198/jbes.2009.06003\nHerwartz, Helmut. 2018. \"Hodges-Lehmann Detection of Structural Shocks: An Analysis of Macroeconomic Dynamics in the Euro Area.\" Oxford Bulletin of Economics and Statistics 80 (4): 736–754. https://doi.org/10.1111/obes.12234\nAzzalini, Adelchi. 1985. \"A Class of Distributions Which Includes the Normal Ones.\" Scandinavian Journal of Statistics 12 (2): 171–178. https://www.jstor.org/stable/4615982","category":"section"},{"location":"nongaussian/#Non-Gaussianity-—-Moments-(GMM)","page":"Statistical Identification","title":"Non-Gaussianity — Moments (GMM)","text":"Keweloh, Sascha A. 2021. \"A Generalized Method of Moments Estimator for Structural Vector Autoregressions Based on Higher Moments.\" Journal of Business & Economic Statistics 39 (3): 772–882. https://doi.org/10.1080/07350015.2020.1730858\nLanne, Markku, and Jani Luoto. 2021. \"GMM Estimation of Non-Gaussian Structural Vector Autoregression.\" Journal of Business & Economic Statistics 39 (1): 69–81. https://doi.org/10.1080/07350015.2019.1629940","category":"section"},{"location":"nongaussian/#Diagnostics-and-Weak-Identification","page":"Statistical Identification","title":"Diagnostics and Weak Identification","text":"Lewis, Daniel J. 2022. \"Robust Inference in Models Identified via Heteroskedasticity.\" Review of Economics and Statistics 104 (3): 510–524. https://doi.org/10.1162/resta00977","category":"section"},{"location":"nongaussian/#Multivariate-Normality-Tests-2","page":"Statistical Identification","title":"Multivariate Normality Tests","text":"Jarque, Carlos M., and Anil K. Bera. 1980. \"Efficient Tests for Normality, Homoscedasticity and Serial Independence of Regression Residuals.\" Economics Letters 6 (3): 255–259. https://doi.org/10.1016/0165-1765(80)90024-5\nMardia, Kanti V. 1970. \"Measures of Multivariate Skewness and Kurtosis with Applications.\" Biometrika 57 (3): 519–530. https://doi.org/10.1093/biomet/57.3.519\nDoornik, Jurgen A., and Henrik Hansen. 2008. \"An Omnibus Test for Univariate and Multivariate Normality.\" Oxford Bulletin of Economics and Statistics 70: 927–939. https://doi.org/10.1111/j.1468-0084.2008.00537.x\nHenze, Norbert, and Bernhard Zirkler. 1990. \"A Class of Invariant Consistent Tests for Multivariate Normality.\" Communications in Statistics - Theory and Methods 19 (10): 3595–3617. https://doi.org/10.1080/03610929008830400\nLütkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.","category":"section"},{"location":"nongaussian/#Independence-Measures","page":"Statistical Identification","title":"Independence Measures","text":"Székely, Gábor J., Maria L. Rizzo, and Nail K. Bakirov. 2007. \"Measuring and Testing Dependence by Correlation of Distances.\" Annals of Statistics 35 (6): 2769–2794. https://doi.org/10.1214/009053607000000505\nGretton, Arthur, Olivier Bousquet, Alex Smola, and Bernhard Schölkopf. 2005. \"Measuring Statistical Dependence with Hilbert-Schmidt Norms.\" In Algorithmic Learning Theory, edited by Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita, 63–77. Berlin: Springer. https://doi.org/10.1007/11564089_7\nMatteson, David S., and Ruey S. Tsay. 2017. \"Independent Component Analysis via Distance Covariance.\" Journal of the American Statistical Association 112 (518): 623–637. https://doi.org/10.1080/01621459.2016.1150851","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This chapter provides comprehensive worked examples demonstrating the main functionality of MacroEconometricModels.jl. Each example includes complete code, economic interpretation, and best practices. The examples follow the natural empirical workflow: load and prepare data, test properties, estimate univariate and multivariate models, and report results. All examples use built-in datasets (FRED-MD, FRED-QD, Penn World Table) –- no simulated data.","category":"section"},{"location":"examples/#Quick-Reference","page":"Examples","title":"Quick Reference","text":"# Example Key Functions Description\n1 FRED-MD Data Pipeline load_example, apply_tcode, diagnose, estimate_var Real data workflow from FRED-MD to VAR estimation\n2 Hypothesis Tests adf_test, kpss_test, johansen_test, granger_test Unit root, cointegration, and Granger causality on FRED data\n3 Time Series Filters hp_filter, hamilton_filter, beveridge_nelson, baxter_king, boosted_hp All 5 filters compared on FRED-MD Industrial Production\n4 ARIMA Models estimate_ar, estimate_arma, auto_arima, forecast ARIMA estimation and forecasting on INDPRO growth\n5 Volatility Models estimate_garch, estimate_egarch, estimate_sv, news_impact_curve GARCH/SV estimation on S&P 500 returns\n6 Three-Variable VAR estimate_var, irf, fevd, identify_arias Monetary policy VAR with Cholesky, sign, long-run, and Arias (2018) identification\n7 Bayesian VAR with Minnesota Prior estimate_bvar, optimize_hyperparameters Minnesota prior, conjugate posterior estimation, credible intervals\n8 VECM Analysis estimate_vecm, johansen_test, to_var, forecast Cointegration in FRED-QD macro aggregates\n9 Local Projections estimate_lp, estimate_lp_iv, structural_lp, lp_fevd Standard, IV, smooth, state-dependent, structural LP, and LP-FEVD\n10 Factor Model for Large Panels estimate_factors, ic_criteria, forecast Large panel factor extraction from FRED-MD\n11 Panel VAR Analysis estimate_pvar, pvar_oirf, pvar_fevd, pvar_bootstrap_irf Cross-country dynamics from Penn World Table\n12 GMM Estimation estimate_gmm, j_test IV regression via GMM with FRED-MD instruments\n13 Non-Gaussian Identification identify_fastica, normality_test_suite, test_shock_gaussianity ICA, ML, heteroskedastic identification on monetary policy VAR\n14 Complete Workflow Multiple FRED-MD pipeline: unit roots → lag selection → VAR → BVAR → LP\n15 Table Output (LaTeX & HTML) set_display_backend, print_table, table Export tables for papers, slides, and web\n16 Bibliographic References refs Multi-format references for models and methods\n17 Nowcasting nowcast_dfm, nowcast_bvar, nowcast_bridge, nowcast_news DFM, BVAR, bridge equation nowcasting with FRED-MD data\n\n","category":"section"},{"location":"examples/#Example-1:-FRED-MD-Data-Pipeline","page":"Examples","title":"Example 1: FRED-MD Data Pipeline","text":"This example demonstrates a complete empirical workflow using the built-in FRED-MD dataset: loading data, applying transformations, data cleaning, and VAR estimation. See Data Management for data container details and Examples for additional workflows.\n\nusing MacroEconometricModels, Random\n\nRandom.seed!(42)\n\n# --- Step 1: Load the FRED-MD dataset ---\nmd = load_example(:fred_md)\ndesc(md)                                # Dataset description and vintage info","category":"section"},{"location":"examples/#Explore-the-Dataset","page":"Examples","title":"Explore the Dataset","text":"# Variable descriptions\nvardesc(md, \"INDPRO\")                   # \"IP Index\"\nvardesc(md, \"UNRATE\")                   # \"Civilian Unemployment Rate\"\nvardesc(md, \"CPIAUCSL\")                 # \"CPI: All Items\"\n\n# Bibliographic reference\nrefs(md)                                # McCracken & Ng (2016)","category":"section"},{"location":"examples/#Transform-and-Clean","page":"Examples","title":"Transform and Clean","text":"# Apply recommended FRED transformation codes (log-diff, diff, etc.)\nmd_transformed = apply_tcode(md, md.tcode)\n\n# Diagnose data issues (NaN from differencing, constant columns, etc.)\ndiag = diagnose(md_transformed)\n\n# Clean: remove rows with missing values\nclean = fix(md_transformed; method=:listwise)\n\nInterpretation. The FRED transformation codes ensure stationarity: code 1 = no transform, 2 = first difference, 4 = log, 5 = log first difference, etc. Differencing introduces NaN in the first row(s), which fix removes. Always diagnose before estimation to catch remaining data issues.","category":"section"},{"location":"examples/#Subset-and-Estimate","page":"Examples","title":"Subset and Estimate","text":"# Select 4 key macroeconomic variables\nsubset = clean[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\", \"FEDFUNDS\"]]\n\n# Summary statistics\ndescribe_data(subset)\n\n# Estimate VAR(4) — quarterly lag structure for monthly data\nmodel = estimate_var(subset, 4)\n\n# Structural analysis\nirfs = irf(model, 24; method=:cholesky)     # 24-month horizon\nfvd = fevd(model, 24)\n\nInterpretation. The four-variable system captures industrial production, unemployment, inflation, and monetary policy –- the core variables for monetary VAR analysis. With monthly data, 4 lags covers one quarter of dynamics. The Cholesky ordering places slow-moving real variables first and the policy rate last, consistent with the standard recursive identification in monetary economics. The impulse responses trace the transmission of a monetary policy shock through output, unemployment, and prices.","category":"section"},{"location":"examples/#Example-2:-Hypothesis-Tests","page":"Examples","title":"Example 2: Hypothesis Tests","text":"This example demonstrates comprehensive pre-estimation testing using real macroeconomic data. Pre-estimation analysis is the first step in any empirical macro workflow. See Hypothesis Tests for theoretical background.","category":"section"},{"location":"examples/#Individual-Unit-Root-Tests","page":"Examples","title":"Individual Unit Root Tests","text":"using MacroEconometricModels\nusing Statistics\n\n# Load FRED-MD data\nfred = load_example(:fred_md)\n\n# CPI level — should be I(1) (non-stationary price level)\ncpi_level = fred[:, \"CPIAUCSL\"]\n\n# CPI inflation (log first difference) — should be I(0) (stationary)\ncpi_infl = filter(isfinite, apply_tcode(fred[:, \"CPIAUCSL\"], 5))\n\n# === ADF Test ===\nprintln(\"=\"^60)\nprintln(\"ADF Test (H₀: unit root)\")\nprintln(\"=\"^60)\n\nadf_infl = adf_test(cpi_infl; lags=:aic, regression=:constant)\nprintln(\"\\nCPI Inflation (log Δ):\")\nprintln(\"  Statistic: \", round(adf_infl.statistic, digits=3))\nprintln(\"  P-value: \", round(adf_infl.pvalue, digits=4))\nprintln(\"  Lags: \", adf_infl.lags)\n\nadf_level = adf_test(cpi_level; lags=:aic, regression=:constant)\nprintln(\"\\nCPI Level:\")\nprintln(\"  Statistic: \", round(adf_level.statistic, digits=3))\nprintln(\"  P-value: \", round(adf_level.pvalue, digits=4))\n\nThe ADF test statistic is compared to non-standard critical values (Dickey-Fuller distribution, not Student-t). For CPI inflation, the large negative test statistic yields a small p-value, rejecting the unit root null. For the CPI level, the test fails to reject, consistent with a unit root in the price level. The number of augmenting lags selected by AIC controls for residual serial correlation.","category":"section"},{"location":"examples/#KPSS-Complementary-Test","page":"Examples","title":"KPSS Complementary Test","text":"# === KPSS Test ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"KPSS Test (H₀: stationarity)\")\nprintln(\"=\"^60)\n\nkpss_infl = kpss_test(cpi_infl; regression=:constant)\nprintln(\"\\nCPI Inflation:\")\nprintln(\"  Statistic: \", round(kpss_infl.statistic, digits=4))\nprintln(\"  P-value: \", kpss_infl.pvalue > 0.10 ? \">0.10\" : round(kpss_infl.pvalue, digits=4))\nprintln(\"  Bandwidth: \", kpss_infl.bandwidth)\n\nkpss_level = kpss_test(cpi_level; regression=:constant)\nprintln(\"\\nCPI Level:\")\nprintln(\"  Statistic: \", round(kpss_level.statistic, digits=4))\nprintln(\"  P-value: \", kpss_level.pvalue < 0.01 ? \"<0.01\" : round(kpss_level.pvalue, digits=4))","category":"section"},{"location":"examples/#Combining-ADF-and-KPSS-for-Robust-Inference","page":"Examples","title":"Combining ADF and KPSS for Robust Inference","text":"# === Combined Analysis ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Combined ADF + KPSS Analysis\")\nprintln(\"=\"^60)\n\nfunction unit_root_decision(y; name=\"Series\")\n    adf = adf_test(y; lags=:aic)\n    kpss = kpss_test(y)\n\n    adf_reject = adf.pvalue < 0.05  # Reject unit root\n    kpss_reject = kpss.pvalue < 0.05  # Reject stationarity\n\n    decision = if adf_reject && !kpss_reject\n        \"I(0) - Stationary\"\n    elseif !adf_reject && kpss_reject\n        \"I(1) - Unit root\"\n    elseif adf_reject && kpss_reject\n        \"Conflicting (possible structural break)\"\n    else\n        \"Inconclusive\"\n    end\n\n    println(\"\\n$name:\")\n    println(\"  ADF p-value: \", round(adf.pvalue, digits=4))\n    println(\"  KPSS p-value: \", round(kpss.pvalue, digits=4))\n    println(\"  Decision: $decision\")\n\n    return decision\nend\n\n# Test key FRED-MD variables\nindpro = fred[:, \"INDPRO\"]\nfedfunds = fred[:, \"FEDFUNDS\"]\n\nunit_root_decision(cpi_level; name=\"CPI Level\")\nunit_root_decision(cpi_infl; name=\"CPI Inflation\")\nunit_root_decision(indpro; name=\"Industrial Production (level)\")\nunit_root_decision(fedfunds; name=\"Federal Funds Rate\")","category":"section"},{"location":"examples/#Testing-for-Structural-Breaks","page":"Examples","title":"Testing for Structural Breaks","text":"# === Zivot-Andrews Test ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Zivot-Andrews Test (H₀: unit root without break)\")\nprintln(\"=\"^60)\n\n# Industrial production may exhibit structural breaks (recessions)\nza_result = za_test(indpro; regression=:constant, trim=0.15)\nprintln(\"\\nIndustrial Production:\")\nprintln(\"  Minimum t-stat: \", round(za_result.statistic, digits=3))\nprintln(\"  P-value: \", round(za_result.pvalue, digits=4))\nprintln(\"  Break index: \", za_result.break_index)\nprintln(\"  Break at: \", round(za_result.break_fraction * 100, digits=1), \"% of sample\")\n\n# Compare with standard ADF\nadf_indpro = adf_test(indpro)\nprintln(\"\\n  ADF (ignoring break): p=\", round(adf_indpro.pvalue, digits=4))\nprintln(\"  ZA (allowing break): p=\", round(za_result.pvalue, digits=4))","category":"section"},{"location":"examples/#Ng-Perron-Tests-for-Size-Control","page":"Examples","title":"Ng-Perron Tests for Size Control","text":"# === Ng-Perron Tests ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Ng-Perron Tests (improved size properties)\")\nprintln(\"=\"^60)\n\nnp_result = ngperron_test(fedfunds; regression=:constant)\n\nprintln(\"\\nFederal Funds Rate:\")\nprintln(\"  MZα: \", round(np_result.MZa, digits=3),\n        \" (5% CV: \", np_result.critical_values[:MZa][5], \")\")\nprintln(\"  MZt: \", round(np_result.MZt, digits=3),\n        \" (5% CV: \", np_result.critical_values[:MZt][5], \")\")\nprintln(\"  MSB: \", round(np_result.MSB, digits=4),\n        \" (5% CV: \", np_result.critical_values[:MSB][5], \")\")\nprintln(\"  MPT: \", round(np_result.MPT, digits=3),\n        \" (5% CV: \", np_result.critical_values[:MPT][5], \")\")","category":"section"},{"location":"examples/#Johansen-Cointegration-Test","page":"Examples","title":"Johansen Cointegration Test","text":"# === Johansen Cointegration Test ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Johansen Cointegration Test\")\nprintln(\"=\"^60)\n\n# Load FRED-QD: log GDP, Consumption, Investment are I(1) and potentially cointegrated\nqd = load_example(:fred_qd)\nY_coint = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY_coint = Y_coint[all.(isfinite, eachrow(Y_coint)), :]\n\njohansen = johansen_test(Y_coint, 2; deterministic=:constant)\n\nprintln(\"\\nCointegrated system (GDP, Consumption, Investment):\")\nprintln(\"  Estimated rank: \", johansen.rank)\nprintln(\"\\n  Trace test:\")\nfor r in 0:2\n    stat = round(johansen.trace_stats[r+1], digits=2)\n    cv = round(johansen.critical_values_trace[r+1, 2], digits=2)\n    reject = stat > cv ? \"Reject\" : \"Fail to reject\"\n    println(\"    H₀: r ≤ $r: stat=$stat, 5% CV=$cv → $reject\")\nend\n\nprintln(\"\\n  Eigenvalues: \", round.(johansen.eigenvalues, digits=4))\n\nif johansen.rank > 0\n    println(\"\\n  Cointegrating vector(s):\")\n    for i in 1:johansen.rank\n        println(\"    β$i: \", round.(johansen.eigenvectors[:, i], digits=3))\n    end\nend\n\nThe Johansen trace test sequentially tests hypotheses about the cointegration rank. When the trace statistic exceeds the critical value, we reject the null and move to the next rank. The estimated cointegrating vectors beta represent long-run equilibrium relationships among real GDP, personal consumption, and gross private domestic investment: deviations from beta y_t are stationary even though the individual series are I(1). The adjustment coefficients alpha govern how quickly variables correct back toward equilibrium.","category":"section"},{"location":"examples/#Granger-Causality","page":"Examples","title":"Granger Causality","text":"# === Granger Causality Tests ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Granger Causality Tests\")\nprintln(\"=\"^60)\n\n# Monetary policy VAR: INDPRO, CPIAUCSL, FEDFUNDS (transformed to stationarity)\nY_gc = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY_gc = Y_gc[all.(isfinite, eachrow(Y_gc)), :]\n\nm = estimate_var(Y_gc, 4)\n\n# Does inflation Granger-cause the fed funds rate?\ng = granger_test(m, 2, 3)\nprintln(\"\\nInflation → Fed Funds: Wald = \", round(g.statistic, digits=2),\n        \", p = \", round(g.pvalue, digits=4))\n\n# Does the fed funds rate Granger-cause output?\ng2 = granger_test(m, 3, 1)\nprintln(\"Fed Funds → Output: Wald = \", round(g2.statistic, digits=2),\n        \", p = \", round(g2.pvalue, digits=4))\n\n# Block test: do inflation and fed funds jointly Granger-cause output?\ng_block = granger_test(m, [2, 3], 1)\nprintln(\"\\nBlock [Inflation, Rate] → Output: Wald = \", round(g_block.statistic, digits=2),\n        \", p = \", round(g_block.pvalue, digits=4))\n\n# Full causality table (n × n matrix, nothing on diagonal)\nresults = granger_test_all(m)\n\nThe Granger causality test examines whether lagged values of one variable help predict another. A significant result (low p-value) indicates that past values of the cause variable contain predictive information for the effect variable beyond what is captured by the effect variable's own lags and those of other variables. The block test extends this to groups of variables, testing their joint predictive power.","category":"section"},{"location":"examples/#Testing-All-Variables-Before-VAR","page":"Examples","title":"Testing All Variables Before VAR","text":"# === Multi-Variable Pre-VAR Analysis ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Pre-VAR Unit Root Analysis\")\nprintln(\"=\"^60)\n\n# Select key macro variables (in levels) for unit root testing\nvar_list = [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\", \"UNRATE\"]\nY_macro = to_matrix(fred[:, var_list])\nY_macro = Y_macro[all.(isfinite, eachrow(Y_macro)), :]\n\n# Test all variables\nresults = test_all_variables(Y_macro; test=:adf)\n\nprintln(\"\\nUnit root test results:\")\nprintln(\"-\"^50)\nn_i1 = 0\nfor (i, r) in enumerate(results)\n    status = r.pvalue > 0.05 ? \"I(1)\" : \"I(0)\"\n    n_i1 += r.pvalue > 0.05\n    println(\"  $(var_list[i]): p=$(round(r.pvalue, digits=3)) → $status\")\nend\n\nprintln(\"\\nSummary: $n_i1 of $(size(Y_macro, 2)) variables appear I(1)\")\n\n# Recommendation\nif n_i1 == size(Y_macro, 2)\n    println(\"\\nRecommendation: All variables I(1)\")\n    println(\"  → Test for cointegration\")\n    println(\"  → If cointegrated: use VECM\")\n    println(\"  → If not: use VAR in first differences\")\nelseif n_i1 == 0\n    println(\"\\nRecommendation: All variables I(0)\")\n    println(\"  → Use VAR in levels\")\nelse\n    println(\"\\nRecommendation: Mixed I(0)/I(1)\")\n    println(\"  → Consider ARDL bounds test\")\n    println(\"  → Or difference I(1) variables\")\nend","category":"section"},{"location":"examples/#Complete-Pre-Estimation-Workflow","page":"Examples","title":"Complete Pre-Estimation Workflow","text":"# === Complete Workflow ===\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Complete Pre-Estimation Workflow\")\nprintln(\"=\"^60)\n\nfunction pre_estimation_analysis(Y; var_names=nothing, α=0.05)\n    T, n = size(Y)\n    var_names = isnothing(var_names) ? [\"Var$i\" for i in 1:n] : var_names\n\n    println(\"\\n1. Individual Unit Root Tests\")\n    println(\"-\"^40)\n\n    integration_orders = zeros(Int, n)\n    for i in 1:n\n        adf = adf_test(Y[:, i]; lags=:aic)\n        kpss = kpss_test(Y[:, i])\n\n        if adf.pvalue < α && kpss.pvalue > α\n            integration_orders[i] = 0\n            status = \"I(0)\"\n        elseif adf.pvalue > α && kpss.pvalue < α\n            integration_orders[i] = 1\n            status = \"I(1)\"\n        else\n            integration_orders[i] = -1  # Inconclusive\n            status = \"Inconclusive\"\n        end\n        println(\"  $(var_names[i]): $status (ADF p=$(round(adf.pvalue, digits=3)), KPSS p=$(round(kpss.pvalue, digits=3)))\")\n    end\n\n    n_i1 = sum(integration_orders .== 1)\n    n_i0 = sum(integration_orders .== 0)\n\n    println(\"\\n2. Summary\")\n    println(\"-\"^40)\n    println(\"  I(0) variables: $n_i0\")\n    println(\"  I(1) variables: $n_i1\")\n    println(\"  Inconclusive: $(n - n_i0 - n_i1)\")\n\n    # Cointegration test if all I(1)\n    if n_i1 >= 2\n        println(\"\\n3. Cointegration Test\")\n        println(\"-\"^40)\n        joh = johansen_test(Y, 2)\n        println(\"  Estimated cointegration rank: \", joh.rank)\n\n        if joh.rank > 0\n            println(\"  → Cointegration detected\")\n            println(\"  → Recommendation: VECM with rank=$(joh.rank)\")\n        else\n            println(\"  → No cointegration\")\n            println(\"  → Recommendation: VAR in first differences\")\n        end\n    elseif n_i0 == n\n        println(\"\\n3. Recommendation\")\n        println(\"-\"^40)\n        println(\"  All series stationary → VAR in levels\")\n    end\n\n    return (integration_orders=integration_orders, n_i0=n_i0, n_i1=n_i1)\nend\n\n# Run complete analysis on FRED-MD macro variables (in levels)\nresult = pre_estimation_analysis(Y_macro; var_names=var_list)\n\n","category":"section"},{"location":"examples/#Example-3:-Time-Series-Filters","page":"Examples","title":"Example 3: Time Series Filters","text":"This example compares all five trend-cycle decomposition filters on real U.S. industrial production. See Time Series Filters for theory, return values, and individual filter options.\n\nusing MacroEconometricModels, Statistics\n\n# Load FRED-MD and extract log of Industrial Production\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\nprintln(\"Industrial Production (log): T=$(length(y)) months\")\nprintln(\"  Mean growth: \", round(mean(diff(y)), digits=5))\n\n# === Apply all five filters ===\n\n# Hodrick-Prescott (lambda=129600 for monthly data)\nhp  = hp_filter(y; lambda=129600.0)\n\n# Hamilton regression filter (h=24 months, p=12)\nham = hamilton_filter(y; h=24, p=12)\n\n# Beveridge-Nelson decomposition via ARIMA\nbn  = beveridge_nelson(y; p=2, q=0)\n\n# Baxter-King symmetric band-pass (18-96 months, K=12 lead/lag)\nbk  = baxter_king(y; pl=18, pu=96, K=12)\n\n# Boosted HP (iterated HP with BIC stopping)\nbhp = boosted_hp(y; stopping=:BIC)\n\n# === Compare cycle standard deviations ===\nprintln(\"\\nCycle standard deviations:\")\nprintln(\"  HP:       \", round(std(cycle(hp)), digits=4))\nprintln(\"  Hamilton: \", round(std(cycle(ham)), digits=4))\nprintln(\"  BN:       \", round(std(cycle(bn)), digits=4))\nprintln(\"  BK:       \", round(std(cycle(bk)), digits=4))\nprintln(\"  bHP:      \", round(std(cycle(bhp)), digits=4))\n\n# === Trend comparison at selected dates ===\nt_mid = length(y) ÷ 2\nprintln(\"\\nTrend values at t=$t_mid:\")\nprintln(\"  HP:       \", round(trend(hp)[t_mid], digits=4))\nprintln(\"  Hamilton: \", round(trend(ham)[t_mid], digits=4))\nprintln(\"  BN:       \", round(trend(bn)[t_mid], digits=4))\nprintln(\"  bHP:      \", round(trend(bhp)[t_mid], digits=4))\n\nThe HP filter is the most common choice in macroeconomics, but Hamilton (2018) argues it can induce spurious dynamics. The Hamilton filter uses a regression-based approach that avoids end-of-sample bias. Beveridge-Nelson decomposes the series using its ARIMA representation, while Baxter-King isolates business-cycle frequencies (18–96 months for monthly data) via a symmetric moving average. The boosted HP iteratively re-applies the HP filter, improving trend estimation for series with structural changes. Comparing cycle standard deviations reveals how aggressively each filter extracts the cyclical component. Note that filter parameters are set for monthly data: lambda = 129600 (HP), h = 24 months ahead (Hamilton), and plpu = 1896 months (BK business-cycle band).\n\n","category":"section"},{"location":"examples/#Example-4:-ARIMA-Models","page":"Examples","title":"Example 4: ARIMA Models","text":"This example demonstrates univariate time series modeling with ARIMA models on U.S. industrial production growth: estimation, order selection, diagnostics, and forecasting.\n\nusing MacroEconometricModels\nusing Statistics\n\n# Load FRED-MD and compute INDPRO growth (log first difference, tcode=5)\nfred = load_example(:fred_md)\ny = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))\n\nprintln(\"INDPRO growth: T=$(length(y)) observations\")\nprintln(\"  Mean: \", round(mean(y), digits=5))\nprintln(\"  Std:  \", round(std(y), digits=5))\n\n# === AR(2) via OLS ===\nar = estimate_ar(y, 2)\nprintln(\"\\nAR(2) Estimation\")\nprintln(\"  Coefficients: \", round.(coef(ar), digits=4))\nprintln(\"  AIC: \", round(aic(ar), digits=2))\nprintln(\"  BIC: \", round(bic(ar), digits=2))\n\n# === ARMA(1,1) via CSS-MLE ===\narma = estimate_arma(y, 1, 1)\nprintln(\"\\nARMA(1,1) Estimation\")\nprintln(\"  AR coef: \", round(arma.ar_coefs[1], digits=4))\nprintln(\"  MA coef: \", round(arma.ma_coefs[1], digits=4))\nprintln(\"  AIC: \", round(aic(arma), digits=2))\n\n# === Automatic order selection ===\nbest = auto_arima(y; max_d=0)\nprintln(\"\\nauto_arima selection:\")\nprintln(\"  Best model: ARIMA($(best.p),$(best.d),$(best.q))\")\nprintln(\"  AIC: \", round(aic(best), digits=2))\n\n# === Information criteria table ===\nict = ic_table(y, 4, 4)\nprintln(\"\\nIC table (top 5 by AIC):\")\nfor i in 1:min(5, size(ict, 1))\n    println(\"  p=$(Int(ict[i,1])), q=$(Int(ict[i,2])): AIC=$(round(ict[i,3], digits=1)), BIC=$(round(ict[i,4], digits=1))\")\nend\n\n# === Forecast ===\nfc = forecast(arma, 12; conf_level=0.95)\nprintln(\"\\nARMA(1,1) Forecasts:\")\nfor h in [1, 4, 8, 12]\n    println(\"  h=$h: $(round(fc.forecast[h], digits=5)) [$(round(fc.ci_lower[h], digits=5)), $(round(fc.ci_upper[h], digits=5))]\")\nend\n\nThe auto_arima function performs a grid search over (p,d,q) combinations, selecting the model that minimizes AIC. We pass max_d=0 since INDPRO growth (log first difference) is already stationary. The CSS-MLE estimation method initializes parameters via conditional sum of squares (CSS), then refines via exact maximum likelihood using the Kalman filter. Forecast confidence intervals widen with the horizon, reflecting accumulating prediction uncertainty.\n\n","category":"section"},{"location":"examples/#Example-5:-Volatility-Models","page":"Examples","title":"Example 5: Volatility Models","text":"This example estimates ARCH, GARCH, EGARCH, and GJR-GARCH models on S&P 500 returns from FRED-MD, compares their news impact curves, runs diagnostics, and forecasts volatility. See also Volatility Models for theory and return value tables.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\n# Load S&P 500 returns from FRED-MD (log first difference, tcode=5)\nfred = load_example(:fred_md)\nsp_idx = findfirst(v -> occursin(\"S&P\", v) && occursin(\"500\", v), varnames(fred))\ny = filter(isfinite, apply_tcode(fred[:, varnames(fred)[sp_idx]], 5))\n\nprintln(\"S&P 500 monthly returns: T=$(length(y)) observations\")\nprintln(\"  Mean: \", round(mean(y), digits=5))\nprintln(\"  Std:  \", round(std(y), digits=5))\n\n# === Step 1: Test for ARCH effects ===\nstat, pval, q = arch_lm_test(y, 5)\nprintln(\"\\nARCH-LM test (q=5): stat=$(round(stat, digits=2)), p=$(round(pval, digits=6))\")\n\nstat2, pval2, K = ljung_box_squared(y, 10)\nprintln(\"Ljung-Box squared (K=10): stat=$(round(stat2, digits=2)), p=$(round(pval2, digits=6))\")\n\n# === Step 2: Estimate competing models ===\ngarch   = estimate_garch(y, 1, 1)\negarch  = estimate_egarch(y, 1, 1)\ngjr     = estimate_gjr_garch(y, 1, 1)\n\nprintln(\"\\n\" * \"=\"^60)\nprintln(\"Model Comparison\")\nprintln(\"=\"^60)\nprintln(\"              AIC         BIC     Persistence\")\nprintln(\"  GARCH:   \", round(aic(garch), digits=1),\n        \"    \", round(bic(garch), digits=1),\n        \"    \", round(persistence(garch), digits=4))\nprintln(\"  EGARCH:  \", round(aic(egarch), digits=1),\n        \"    \", round(bic(egarch), digits=1),\n        \"    \", round(persistence(egarch), digits=4))\nprintln(\"  GJR:     \", round(aic(gjr), digits=1),\n        \"    \", round(bic(gjr), digits=1),\n        \"    \", round(persistence(gjr), digits=4))\n\n# === Step 3: News impact curves ===\nnic_g  = news_impact_curve(garch)\nnic_e  = news_impact_curve(egarch)\nnic_j  = news_impact_curve(gjr)\n\nprintln(\"\\nNews Impact at epsilon = -2 vs epsilon = +2:\")\nidx_neg = findfirst(x -> x >= -2.0, nic_g.shocks)\nidx_pos = findfirst(x -> x >= 2.0, nic_g.shocks)\n\nprintln(\"  GARCH:  var(-2) = \", round(nic_g.variance[idx_neg], digits=4),\n        \"   var(+2) = \", round(nic_g.variance[idx_pos], digits=4))\nprintln(\"  EGARCH: var(-2) = \", round(nic_e.variance[idx_neg], digits=4),\n        \"   var(+2) = \", round(nic_e.variance[idx_pos], digits=4))\nprintln(\"  GJR:    var(-2) = \", round(nic_j.variance[idx_neg], digits=4),\n        \"   var(+2) = \", round(nic_j.variance[idx_pos], digits=4))\n\n# === Step 4: Residual diagnostics ===\nprintln(\"\\nResidual ARCH-LM test (q=5):\")\nfor (name, m) in [(\"GARCH\", garch), (\"EGARCH\", egarch), (\"GJR\", gjr)]\n    _, p, _ = arch_lm_test(m, 5)\n    status = p > 0.05 ? \"Pass\" : \"FAIL\"\n    println(\"  $name: p=$(round(p, digits=4))  $status\")\nend\n\n# === Step 5: Volatility forecasts ===\nH = 20\nfc_g = forecast(garch, H)\nfc_e = forecast(egarch, H)\nfc_j = forecast(gjr, H)\n\nprintln(\"\\nVolatility forecasts (conditional variance):\")\nprintln(\"  h    GARCH    EGARCH   GJR      Uncond\")\nfor h_idx in [1, 5, 10, 20]\n    println(\"  $h_idx    \",\n            round(fc_g.forecast[h_idx], digits=4), \"  \",\n            round(fc_e.forecast[h_idx], digits=4), \"  \",\n            round(fc_j.forecast[h_idx], digits=4), \"  \",\n            round(unconditional_variance(garch), digits=4))\nend\n\n# === Step 6: Stochastic Volatility ===\nRandom.seed!(42)\nprintln(\"\\nEstimating SV model via KSC Gibbs sampler...\")\nsv = estimate_sv(y; n_samples=2000, burnin=1000)\n\nprintln(\"SV posterior summary:\")\nprintln(\"  mu:      \", round(mean(sv.mu_post), digits=3))\nprintln(\"  phi:     \", round(mean(sv.phi_post), digits=3))\nprintln(\"  sigma_eta: \", round(mean(sv.sigma_eta_post), digits=3))\n\nfc_sv = forecast(sv, H)\nprintln(\"\\nSV forecast at h=1:  \", round(fc_sv.forecast[1], digits=4))\nprintln(\"SV forecast at h=20: \", round(fc_sv.forecast[end], digits=4))\n\nEquity returns typically exhibit volatility clustering and a leverage effect (negative returns increase volatility more than positive returns of the same magnitude). The EGARCH and GJR-GARCH models capture this asymmetry, while the symmetric GARCH treats positive and negative shocks equally. The news impact curves reveal this: for EGARCH and GJR-GARCH, the variance response to varepsilon = -2 exceeds that for varepsilon = +2; for symmetric GARCH, they are equal. All models' standardized residuals should pass the ARCH-LM test after fitting, confirming that the variance dynamics are adequately captured.\n\n","category":"section"},{"location":"examples/#Example-6:-Three-Variable-VAR-Analysis","page":"Examples","title":"Example 6: Three-Variable VAR Analysis","text":"This example walks through a complete analysis of a monetary policy VAR using FRED-MD data: industrial production growth, CPI inflation, and the federal funds rate.","category":"section"},{"location":"examples/#Setup-and-Data-Preparation","page":"Examples","title":"Setup and Data Preparation","text":"using MacroEconometricModels\nusing Random\nusing LinearAlgebra\nusing Statistics\n\n# Load FRED-MD and construct the 3-variable monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nvar_names = [\"Output Growth\", \"Inflation\", \"Fed Funds Rate\"]\nT, n = size(Y)\np = 4  # Monthly data: 4 lags = one quarter\n\nprintln(\"Data: T=$T observations, n=$n variables\")\nprintln(\"  Variables: \", join(var_names, \", \"))","category":"section"},{"location":"examples/#Frequentist-VAR-Estimation","page":"Examples","title":"Frequentist VAR Estimation","text":"# Estimate VAR(4) model via OLS\nmodel = estimate_var(Y, p)\n\n# Model diagnostics\nprintln(\"Log-likelihood: \", loglikelihood(model))\nprintln(\"AIC: \", aic(model))\nprintln(\"BIC: \", bic(model))\n\n# Check stability (eigenvalues inside unit circle)\nF = companion_matrix(model.B, n, p)\neigenvalues = eigvals(F)\nprintln(\"Max eigenvalue modulus: \", maximum(abs.(eigenvalues)))\nprintln(\"Stable: \", maximum(abs.(eigenvalues)) < 1)\n\nThe AIC and BIC values measure the trade-off between fit and parsimony. Lower values indicate a better model. The maximum eigenvalue modulus should be strictly less than 1 for the VAR to be stationary; values close to 1 indicate high persistence, while values near 0 suggest rapid mean-reversion.","category":"section"},{"location":"examples/#Cholesky-Identified-IRF","page":"Examples","title":"Cholesky-Identified IRF","text":"# Compute 20-period IRF with Cholesky identification\n# Ordering: Output → Inflation → Rate (contemporaneous causality)\nH = 20\nirfs = irf(model, H; method=:cholesky)\n\n# Display impact responses (horizon 0)\nprintln(\"\\nImpact responses (B₀):\")\nprintln(\"  Output shock → Output: \", round(irfs.irf[1, 1, 1], digits=3))\nprintln(\"  Output shock → Inflation: \", round(irfs.irf[1, 2, 1], digits=3))\nprintln(\"  Output shock → Rate: \", round(irfs.irf[1, 3, 1], digits=3))\n\n# Long-run responses (horizon H)\nprintln(\"\\nLong-run responses (h=$H):\")\nprintln(\"  Output shock → Output: \", round(irfs.irf[H+1, 1, 1], digits=3))","category":"section"},{"location":"examples/#Sign-Restriction-Identification","page":"Examples","title":"Sign Restriction Identification","text":"Random.seed!(42)\n\n# Sign restrictions: Demand shock raises output and inflation on impact\nfunction check_demand_shock(irf_array)\n    # irf_array is (H+1) × n × n\n    # Check: Shock 1 → Variable 1 (Output) positive\n    #        Shock 1 → Variable 2 (Inflation) positive\n    return irf_array[1, 1, 1] > 0 && irf_array[1, 2, 1] > 0\nend\n\n# Estimate with sign restrictions\nirfs_sign = irf(model, H; method=:sign, check_func=check_demand_shock, n_draws=1000)\n\nprintln(\"\\nSign-identified demand shock:\")\nprintln(\"  Output response: \", round(irfs_sign.irf[1, 1, 1], digits=3))\nprintln(\"  Inflation response: \", round(irfs_sign.irf[1, 2, 1], digits=3))\n\nThe Cholesky identification assumes a recursive causal ordering (Output → Inflation → Rate), meaning output responds only to its own shocks contemporaneously. Sign restrictions provide a theory-based alternative: requiring both output and inflation to rise on impact identifies a \"demand shock\" without imposing a specific causal ordering. If sign restrictions accept many draws, the set-identified IRFs will show wider bands than point-identified Cholesky responses.","category":"section"},{"location":"examples/#Forecast-Error-Variance-Decomposition","page":"Examples","title":"Forecast Error Variance Decomposition","text":"# Compute FEVD\nfevd_result = fevd(model, H; method=:cholesky)\n\n# Variance decomposition at horizon 1, 4, and 20\nfor h in [1, 4, 20]\n    println(\"\\nFEVD at horizon $h:\")\n    for i in 1:n\n        println(\"  $(var_names[i]):\")\n        for j in 1:n\n            pct = round(fevd_result.fevd[h, i, j] * 100, digits=1)\n            println(\"    Shock $j: $pct%\")\n        end\n    end\nend\n\nThe FEVD shows the proportion of each variable's forecast error variance attributable to each structural shock. At short horizons, own shocks typically dominate. As the horizon increases, cross-variable transmission becomes more important, and the FEVD converges to the unconditional variance decomposition. If monetary policy shocks explain a large share of output variance at long horizons, it suggests that interest rate movements are a primary driver of real activity fluctuations.","category":"section"},{"location":"examples/#Long-Run-(Blanchard-Quah)-Identification","page":"Examples","title":"Long-Run (Blanchard-Quah) Identification","text":"Long-run restrictions identify shocks by constraining their cumulative effects. The classic application distinguishes supply and demand shocks where demand shocks have no long-run effect on output.\n\n# Blanchard-Quah long-run identification\nirfs_lr = irf(model, 20; method=:long_run)\n\nInterpretation. The long-run restriction forces the cumulative IRF of shock 1 on variable 2 to zero at the infinite horizon. This is implemented via the Blanchard-Quah decomposition of the long-run multiplier matrix. The method is particularly useful for bivariate supply-demand identification.","category":"section"},{"location":"examples/#Arias-et-al.-(2018)-Zero-and-Sign-Restrictions","page":"Examples","title":"Arias et al. (2018) Zero and Sign Restrictions","text":"The Arias et al. (2018) algorithm provides a unified framework for imposing both zero and sign restrictions on impulse responses. It draws orthogonal rotation matrices uniformly conditional on the restrictions.\n\nRandom.seed!(42)\n\n# Define restrictions: shock 1 has positive effect on var 1, zero on var 3\nrestrictions = SVARRestrictions(3)\nadd_sign_restriction!(restrictions, 1, 1, :positive, 0)   # shock 1 → var 1 positive at h=0\nadd_zero_restriction!(restrictions, 1, 3, 0)               # shock 1 → var 3 zero at h=0\n\n# Arias identification (draws uniform rotations conditional on restrictions)\nresult = identify_arias(model, restrictions, 20; n_draws=500)\nirfs_arias = irf(model, 20; method=:arias, restrictions=restrictions, n_draws=500)\n\nInterpretation. The Arias algorithm guarantees draws from the correct posterior over the identified set, unlike accept-reject approaches. The zero restriction imposes exact equality while sign restrictions constrain the sign at specified horizons. Report the median and pointwise credible bands from the accepted draws.\n\n","category":"section"},{"location":"examples/#Example-7:-Bayesian-VAR-with-Minnesota-Prior","page":"Examples","title":"Example 7: Bayesian VAR with Minnesota Prior","text":"This example demonstrates Bayesian estimation with automatic hyperparameter optimization using the same monetary policy dataset.","category":"section"},{"location":"examples/#Hyperparameter-Optimization","page":"Examples","title":"Hyperparameter Optimization","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\np = 4\n\n# Find optimal shrinkage using marginal likelihood (Giannone et al. 2015)\nprintln(\"Optimizing hyperparameters...\")\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=20)\n\nprintln(\"Optimal hyperparameters:\")\nprintln(\"  τ (overall tightness): \", round(best_hyper.tau, digits=4))\nprintln(\"  d (lag decay): \", best_hyper.d)\n\nThe optimal tau value reflects the degree of shrinkage that maximizes the marginal likelihood. A small tau (e.g., 0.05) means strong shrinkage toward the random walk prior, appropriate for large systems or short samples. A larger tau (e.g., 0.5-1.0) allows the data more influence, appropriate when the sample is informative relative to the model complexity.","category":"section"},{"location":"examples/#BVAR-Estimation","page":"Examples","title":"BVAR Estimation","text":"# Estimate BVAR with optimized Minnesota prior\nprintln(\"\\nEstimating BVAR with conjugate NIW sampler...\")\npost = estimate_bvar(Y, p;\n    n_draws = 1000,\n    prior = :minnesota,\n    hyper = best_hyper,\n    varnames = [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]\n)\n\n# Posterior summary (coefficients from first equation)\nprintln(\"\\nPosterior summary for Output equation:\")\n# Access posterior draws: post.B_draws, post.Sigma_draws","category":"section"},{"location":"examples/#Bayesian-IRF-with-Credible-Intervals","page":"Examples","title":"Bayesian IRF with Credible Intervals","text":"# Bayesian IRF with Cholesky identification\nH = 20\nbirf_chol = irf(post, H; method=:cholesky)\n\n# Extract median and 68% credible intervals\n# birf_chol.quantiles is (H+1) × n × n × 3 array\n# [:, :, :, 1] = 16th percentile\n# [:, :, :, 2] = median\n# [:, :, :, 3] = 84th percentile\n\nprintln(\"\\nBayesian IRF of Output to own shock:\")\nfor h in [0, 4, 8, 12, 20]\n    med = round(birf_chol.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf_chol.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf_chol.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend","category":"section"},{"location":"examples/#Bayesian-Sign-Restrictions","page":"Examples","title":"Bayesian Sign Restrictions","text":"# Bayesian IRF with sign restrictions\nfunction check_demand_shock(irf_array)\n    return irf_array[1, 1, 1] > 0 && irf_array[1, 2, 1] > 0\nend\n\nbirf_sign = irf(post, H;\n    method = :sign,\n    check_func = check_demand_shock\n)\n\nprintln(\"\\nBayesian sign-restricted demand shock → Output:\")\nfor h in [0, 4, 8, 12]\n    med = round(birf_sign.quantiles[h+1, 1, 1, 2], digits=3)\n    lo = round(birf_sign.quantiles[h+1, 1, 1, 1], digits=3)\n    hi = round(birf_sign.quantiles[h+1, 1, 1, 3], digits=3)\n    println(\"  h=$h: $med [$lo, $hi]\")\nend","category":"section"},{"location":"examples/#Large-BVAR-with-Many-Variables","page":"Examples","title":"Large BVAR with Many Variables","text":"# Estimate a larger BVAR using 20 FRED-MD variables\nfred_sub = apply_tcode(fred[:, varnames(fred)[1:20]])\nX_large = to_matrix(fred_sub)\nX_large = X_large[all.(isfinite, eachrow(X_large)), :]\n\nprintln(\"\\nLarge BVAR: T=$(size(X_large,1)), n=$(size(X_large,2))\")\nhyper_large = optimize_hyperparameters(X_large, 4; grid_size=10)\nprintln(\"Optimal τ for large system: \", round(hyper_large.tau, digits=4))\n\npost_large = estimate_bvar(X_large, 4;\n    n_draws = 500,\n    prior = :minnesota,\n    hyper = hyper_large\n)\n\nInterpretation. Large BVARs with Minnesota priors shrink coefficients toward zero (or a random walk), preventing overfitting in high-dimensional systems. The marginal likelihood-based hyperparameter optimization (Giannone et al. 2015) automatically selects the degree of shrinkage. For a 20-variable system, optimal tau is typically much smaller than for the 3-variable system, reflecting the greater need for regularization.\n\n","category":"section"},{"location":"examples/#Example-8:-VECM-Analysis","page":"Examples","title":"Example 8: VECM Analysis","text":"This example demonstrates Vector Error Correction Model (VECM) estimation for cointegrated macroeconomic aggregates using FRED-QD data: testing for cointegration rank, estimating the VECM, computing impulse responses via VAR conversion, forecasting, and Granger causality decomposition. See VECM for theory and return value tables.\n\nusing MacroEconometricModels, Random\n\n# Load FRED-QD: log GDP, Consumption, Investment (I(1) series)\nqd = load_example(:fred_qd)\nY = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nvar_names = [\"GDP\", \"Consumption\", \"Investment\"]\nprintln(\"Data: T=$(size(Y,1)) quarters, n=$(size(Y,2)) ($(join(var_names, \", \")))\")\n\n# === Step 1: Test for cointegration ===\njoh = johansen_test(Y, 2)\nprintln(\"\\nJohansen test:\")\nprintln(\"  Estimated cointegration rank: \", joh.rank)\nfor r in 0:2\n    stat = round(joh.trace_stats[r+1], digits=2)\n    cv = round(joh.critical_values_trace[r+1, 2], digits=2)\n    reject = stat > cv ? \"Reject\" : \"Fail to reject\"\n    println(\"  H₀: r ≤ $r: stat=$stat, 5% CV=$cv → $reject\")\nend\n\n# === Step 2: Estimate VECM with automatic rank ===\nvecm = estimate_vecm(Y, 2)\nreport(vecm)\n\n# === Step 3: Examine cointegrating vectors and adjustment speeds ===\nprintln(\"\\nβ (cointegrating vectors):\")\nprintln(vecm.beta)\nprintln(\"α (adjustment speeds):\")\nprintln(vecm.alpha)\n\n# === Step 4: Impulse responses via VAR conversion ===\nirfs = irf(vecm, 20; method=:cholesky)\n\nprintln(\"\\nIRF of GDP shock → GDP:\")\nfor h in [0, 4, 8, 12, 20]\n    println(\"  h=$h: \", round(irfs.irf[h+1, 1, 1], digits=3))\nend\n\n# === Step 5: Forecast with bootstrap CIs ===\nRandom.seed!(42)\nfc = forecast(vecm, 10; ci_method=:bootstrap, reps=200)\n\nprintln(\"\\nVECM forecast (GDP):\")\nfor h in [1, 5, 10]\n    println(\"  h=$h: \", round(fc.forecast[h, 1], digits=3))\nend\n\n# === Step 6: Granger causality (short-run, long-run, strong) ===\nprintln(\"\\nGranger causality (VECM decomposition):\")\nfor i in 1:3, j in 1:3\n    i == j && continue\n    g = granger_causality_vecm(vecm, i, j)\n    println(\"  $(var_names[i]) → $(var_names[j]): p=$(round(g.strong_pvalue, digits=4))\")\nend\n\n# === Step 7: Convert to VAR for FEVD ===\nvar_model = to_var(vecm)\ndecomp = fevd(var_model, 20)\n\nInterpretation. The cointegrating vector beta identifies the long-run equilibrium relationship among real GDP, consumption, and investment. If beta approx 1 -a -b, this implies a linear combination of the three variables is stationary –- they share common stochastic trends. The adjustment coefficients alpha show how each variable responds when the system deviates from equilibrium. A significant alpha_i indicates that variable i adjusts to restore the long-run relationship. The VECM-specific Granger causality decomposes predictive power into short-run (lagged differences) and long-run (error correction) channels.\n\n","category":"section"},{"location":"examples/#Example-9:-Local-Projections","page":"Examples","title":"Example 9: Local Projections","text":"This example demonstrates various LP methods for estimating impulse responses using the FRED-MD monetary policy dataset.","category":"section"},{"location":"examples/#Standard-Local-Projection","page":"Examples","title":"Standard Local Projection","text":"using MacroEconometricModels\nusing Random\nusing Statistics\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nT = size(Y, 1)\n\n# Estimate LP-IRF with Newey-West standard errors\nH = 20\nshock_var = 3  # Fed funds rate as the shock variable\n\nlp_model = estimate_lp(Y, shock_var, H;\n    lags = 4,\n    cov_type = :newey_west,\n    bandwidth = 0  # Automatic bandwidth selection\n)\n\n# Extract IRF with confidence intervals\nlp_result = lp_irf(lp_model; conf_level = 0.95)\n\nprintln(\"LP-IRF of monetary policy shock → Output:\")\nfor h in 0:4:H\n    val = round(lp_result.values[h+1, 1], digits=3)\n    se = round(lp_result.se[h+1, 1], digits=3)\n    println(\"  h=$h: $val (SE: $se)\")\nend","category":"section"},{"location":"examples/#LP-with-Instrumental-Variables","page":"Examples","title":"LP with Instrumental Variables","text":"# Generate external instrument (e.g., monetary policy shock proxy)\nRandom.seed!(123)\nZ = 0.5 * Y[:, 3] + randn(T, 1)  # Correlated with rate but exogenous\n\n# Estimate LP-IV\nshock_var = 3  # Instrument for rate shock\nlpiv_model = estimate_lp_iv(Y, shock_var, Z, H;\n    lags = 4,\n    cov_type = :newey_west\n)\n\n# Check instrument strength\nweak_test = weak_instrument_test(lpiv_model; threshold = 10.0)\nprintln(\"\\nFirst-stage F-statistics by horizon:\")\nfor h in 0:4:H\n    F = round(weak_test.F_stats[h+1], digits=2)\n    status = F > 10 ? \"strong\" : \"weak\"\n    println(\"  h=$h: F=$F ($status)\")\nend\nprintln(\"All horizons pass F>10: \", weak_test.passes_threshold)\n\n# Extract IRF\nlpiv_result = lp_iv_irf(lpiv_model)","category":"section"},{"location":"examples/#Smooth-Local-Projection","page":"Examples","title":"Smooth Local Projection","text":"# Estimate smooth LP with B-splines\nsmooth_model = estimate_smooth_lp(Y, 1, H;\n    degree = 3,      # Cubic splines\n    n_knots = 4,     # Interior knots\n    lambda = 1.0,    # Smoothing parameter\n    lags = 4\n)\n\n# Cross-validate lambda\noptimal_lambda = cross_validate_lambda(Y, 1, H;\n    lambda_grid = 10.0 .^ (-4:0.5:2),\n    k_folds = 5\n)\nprintln(\"\\nOptimal smoothing parameter: \", round(optimal_lambda, digits=4))\n\n# Compare standard vs smooth LP\ncomparison = compare_smooth_lp(Y, 1, H; lambda = optimal_lambda)\nprintln(\"Variance reduction ratio: \", round(comparison.variance_reduction, digits=3))","category":"section"},{"location":"examples/#State-Dependent-Local-Projection","page":"Examples","title":"State-Dependent Local Projection","text":"# Construct state variable from output growth (moving average)\nstate_var = zeros(T)\nfor t in 4:T\n    state_var[t] = mean(Y[t-3:t, 1])\nend\nstate_var = (state_var .- mean(state_var[4:end])) ./ std(state_var[4:end])\n\n# Estimate state-dependent LP (Auerbach & Gorodnichenko 2013)\nstate_model = estimate_state_lp(Y, 1, state_var, H;\n    gamma = 1.5,           # Transition speed\n    threshold = :median,    # Threshold at median\n    lags = 4\n)\n\n# Extract regime-specific IRFs\nirf_both = state_irf(state_model; regime = :both)\n\nprintln(\"\\nState-dependent IRFs (output shock → output):\")\nprintln(\"Expansion vs Recession comparison:\")\nfor h in [0, 4, 8, 12]\n    exp_val = round(irf_both.expansion.values[h+1, 1], digits=3)\n    rec_val = round(irf_both.recession.values[h+1, 1], digits=3)\n    diff_val = round(exp_val - rec_val, digits=3)\n    println(\"  h=$h: Expansion=$exp_val, Recession=$rec_val, Diff=$diff_val\")\nend\n\n# Test for regime differences\ndiff_test = test_regime_difference(state_model)\nprintln(\"\\nJoint test for regime differences:\")\nprintln(\"  Average |t|: \", round(diff_test.joint_test.avg_t_stat, digits=2))\nprintln(\"  p-value: \", round(diff_test.joint_test.p_value, digits=4))","category":"section"},{"location":"examples/#Structural-LP-(Plagborg-Moller-and-Wolf-2021)","page":"Examples","title":"Structural LP (Plagborg-Moller & Wolf 2021)","text":"Structural LP extends standard LP to jointly identify multiple shocks, enabling direct comparison with VAR-based IRFs. The structural_lp function estimates impulse responses for all shocks simultaneously.\n\n# Structural LP with Cholesky identification\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\n\n# Multi-shock IRFs: response of inflation to output shock\nslp.irfs[1, 2, :]  # H+1 vector of responses\n\n# Historical decomposition from structural LP\nhd = historical_decomposition(slp)\n\nInterpretation. Structural LP produces IRFs that are asymptotically equivalent to VAR-based IRFs under correct specification (Plagborg-Moller and Wolf 2021). The key advantage is robustness to lag length misspecification. Use structural_lp when you want multi-shock identification without committing to a specific VAR lag order.","category":"section"},{"location":"examples/#LP-FEVD-(Gorodnichenko-and-Lee-2019)","page":"Examples","title":"LP-FEVD (Gorodnichenko & Lee 2019)","text":"LP-FEVD decomposes forecast error variance using local projection methods, providing a model-free alternative to VAR-based FEVD.\n\n# LP-FEVD with R² estimator\nlfevd = lp_fevd(slp; estimator=:r2)\n\n# Decomposition at horizon h: fraction of variable j's forecast error due to shock i\nlfevd.decomposition  # (H+1) × n_shocks × n_vars array\n\nInterpretation. The R² estimator measures the fraction of h-step-ahead forecast error variance attributable to each structural shock. Unlike VAR-FEVD, LP-FEVD does not require invertibility of the MA representation and is robust to lag truncation. The decomposition should sum to approximately 1 across shocks at each horizon. See Innovation Accounting for a detailed comparison of VAR-FEVD and LP-FEVD.\n\n","category":"section"},{"location":"examples/#Example-10:-Factor-Model-for-Large-Panels","page":"Examples","title":"Example 10: Factor Model for Large Panels","text":"This example demonstrates factor extraction and selection from a large macroeconomic panel using FRED-MD data.","category":"section"},{"location":"examples/#Load-and-Prepare-FRED-MD-Panel","page":"Examples","title":"Load and Prepare FRED-MD Panel","text":"using MacroEconometricModels\nusing Random\nusing Statistics\n\n# Load FRED-MD and select first 20 variables for a manageable panel\nfred = load_example(:fred_md)\nselected_vars = varnames(fred)[1:20]\n\n# Apply FRED transformation codes and clean\nfred_sub = apply_tcode(fred[:, selected_vars])\nX = to_matrix(fred_sub)\nX = X[all.(isfinite, eachrow(X)), :]\n\nT, N = size(X)\nprintln(\"FRED-MD panel: T=$T, N=$N\")\nprintln(\"  Variables: \", join(selected_vars[1:5], \", \"), \", ...\")","category":"section"},{"location":"examples/#Determine-Number-of-Factors","page":"Examples","title":"Determine Number of Factors","text":"# Bai-Ng information criteria\nr_max = 10\nic = ic_criteria(X, r_max)\n\nprintln(\"\\nBai-Ng information criteria:\")\nprintln(\"  IC1 selects: \", ic.r_IC1, \" factors\")\nprintln(\"  IC2 selects: \", ic.r_IC2, \" factors\")\nprintln(\"  IC3 selects: \", ic.r_IC3, \" factors\")\n\n# IC values for each r\nprintln(\"\\nIC values by number of factors:\")\nfor r in 1:r_max\n    println(\"  r=$r: IC1=$(round(ic.IC1[r], digits=4)), IC2=$(round(ic.IC2[r], digits=4))\")\nend","category":"section"},{"location":"examples/#Estimate-Factor-Model","page":"Examples","title":"Estimate Factor Model","text":"# Use IC2's recommendation\nr_opt = ic.r_IC2\n\n# Estimate factor model\nfm = estimate_factors(X, r_opt; standardize = true)\n\nprintln(\"\\nEstimated factor model:\")\nprintln(\"  Number of factors: \", fm.r)\nprintln(\"  Factors dimension: \", size(fm.factors))\nprintln(\"  Loadings dimension: \", size(fm.loadings))\n\n# Variance explained\nprintln(\"\\nVariance explained:\")\nfor j in 1:r_opt\n    pct = round(fm.explained_variance[j] * 100, digits=1)\n    cum = round(fm.cumulative_variance[j] * 100, digits=1)\n    println(\"  Factor $j: $pct% (cumulative: $cum%)\")\nend","category":"section"},{"location":"examples/#Model-Diagnostics","page":"Examples","title":"Model Diagnostics","text":"# R² for each variable\nr2_vals = r2(fm)\n\nprintln(\"\\nR² statistics:\")\nprintln(\"  Mean: \", round(mean(r2_vals), digits=3))\nprintln(\"  Median: \", round(median(r2_vals), digits=3))\nprintln(\"  Min: \", round(minimum(r2_vals), digits=3))\nprintln(\"  Max: \", round(maximum(r2_vals), digits=3))\n\n# Variables well-explained (R² > 0.5)\nwell_explained = sum(r2_vals .> 0.5)\nprintln(\"  Variables with R² > 0.5: $well_explained / $N\")\n\n# Top-loaded variables for each factor\nprintln(\"\\nTop-loaded variables per factor:\")\nfor j in 1:r_opt\n    loadings_j = abs.(fm.loadings[:, j])\n    top_idx = sortperm(loadings_j, rev=true)[1:min(5, N)]\n    println(\"  Factor $j: \", join([selected_vars[i] for i in top_idx], \", \"))\nend\n\nThe Bai-Ng information criteria select the number of factors by balancing fit against complexity. IC2 tends to perform best in simulations. The R² values show how well the common factors explain each variable; variables with low R² are primarily driven by idiosyncratic shocks and contribute less to the common component. Examining the top-loaded variables for each factor reveals the economic interpretation: the first factor typically corresponds to real activity, the second to price dynamics.","category":"section"},{"location":"examples/#Factor-Model-Forecasting","page":"Examples","title":"Factor Model Forecasting","text":"Random.seed!(42)\n\n# Forecast 12 steps ahead with theoretical (analytical) CIs\nfc = forecast(fm, 12; ci_method=:theoretical, conf_level=0.95)\n\nprintln(\"\\nFactor forecast with 95% CIs:\")\nprintln(\"  Factors: \", size(fc.factors))        # 12×r\nprintln(\"  Observables: \", size(fc.observables)) # 12×N\nprintln(\"  CI method: \", fc.ci_method)\n\n# SEs should increase with horizon (growing uncertainty)\nprintln(\"\\nFactor 1 SE by horizon:\")\nfor h in [1, 4, 8, 12]\n    println(\"  h=$h: SE=$(round(fc.factors_se[h, 1], digits=4))\")\nend\n\n# Bootstrap CIs (non-parametric, no Gaussian assumption)\nfc_boot = forecast(fm, 12; ci_method=:bootstrap, n_boot=500, conf_level=0.90)\n\nprintln(\"\\nBootstrap vs theoretical CI widths (Factor 1, h=12):\")\nwidth_theory = fc.factors_upper[12, 1] - fc.factors_lower[12, 1]\nwidth_boot = fc_boot.factors_upper[12, 1] - fc_boot.factors_lower[12, 1]\nprintln(\"  Theoretical: \", round(width_theory, digits=3))\nprintln(\"  Bootstrap: \", round(width_boot, digits=3))\n\nThe theoretical SEs grow monotonically with the forecast horizon for stationary factor dynamics, reflecting accumulating forecast uncertainty. Bootstrap CIs are useful when factor innovations may be non-Gaussian or exhibit conditional heteroskedasticity.","category":"section"},{"location":"examples/#Dynamic-Factor-Model-Forecasting","page":"Examples","title":"Dynamic Factor Model Forecasting","text":"Random.seed!(42)\n\n# Estimate DFM with VAR(2) factor dynamics\ndfm = estimate_dynamic_factors(X, r_opt, 2)\n\n# Forecast with all CI methods\nfc_none = forecast(dfm, 12)                                    # Point only\nfc_theo = forecast(dfm, 12; ci_method=:theoretical)            # Analytical CIs\nfc_boot = forecast(dfm, 12; ci_method=:bootstrap, n_boot=500)  # Bootstrap CIs\nfc_sim  = forecast(dfm, 12; ci_method=:simulation, n_boot=500) # Simulation CIs\n\nprintln(\"\\nDFM forecast comparison (Observable 1, h=12):\")\nprintln(\"  Point forecast: \", round(fc_none.observables[12, 1], digits=3))\nprintln(\"  Theoretical CI: [\", round(fc_theo.observables_lower[12, 1], digits=3),\n        \", \", round(fc_theo.observables_upper[12, 1], digits=3), \"]\")\nprintln(\"  Bootstrap CI:   [\", round(fc_boot.observables_lower[12, 1], digits=3),\n        \", \", round(fc_boot.observables_upper[12, 1], digits=3), \"]\")\n\nThe DFM supports four CI methods: :theoretical (fastest, assumes Gaussian innovations), :bootstrap (residual resampling), :simulation (full Monte Carlo draws), and the legacy ci=true interface which maps to :simulation.\n\n","category":"section"},{"location":"examples/#Example-11:-Panel-VAR-Analysis","page":"Examples","title":"Example 11: Panel VAR Analysis","text":"This example demonstrates the full Panel VAR workflow using the Penn World Table: data preparation, lag selection, GMM estimation, specification tests, structural analysis, and bootstrap confidence intervals. See Panel VAR for theory and method details.\n\nusing MacroEconometricModels, DataFrames, Random\n\n# Load Penn World Table (38 OECD countries, 74 years, 42 variables)\npwt = load_example(:pwt)\nprintln(\"PWT: \", length(unique(pwt.group_id)), \" countries, \",\n        length(unique(pwt.time_id)), \" years, \",\n        pwt.n_vars, \" variables\")\n\n# Use the full PanelData object; select dependent variables via kwarg\n# Key variables: rgdpna (real GDP), rkna (capital stock), hc (human capital)\ndep_vars = [\"rgdpna\", \"rkna\", \"hc\"]","category":"section"},{"location":"examples/#Lag-Selection","page":"Examples","title":"Lag Selection","text":"# Andrews-Lu MMSC-based lag selection (max 4 lags)\nlag_result = pvar_lag_selection(pwt, 4; dependent_vars=dep_vars)\n\nInterpretation. The lag selection procedure computes MMSC-AIC, MMSC-BIC, and MMSC-HQIC for each candidate lag length. Lower values indicate better fit-complexity trade-off. Choose the lag minimizing BIC for a conservative choice.","category":"section"},{"location":"examples/#Two-Step-GMM-Estimation","page":"Examples","title":"Two-Step GMM Estimation","text":"# Estimate PVAR with two-step GMM and Windmeijer correction\nmodel = estimate_pvar(pwt, 2; steps=:twostep, dependent_vars=dep_vars)","category":"section"},{"location":"examples/#Specification-Tests","page":"Examples","title":"Specification Tests","text":"# Hansen J-test for overidentifying restrictions\nj = pvar_hansen_j(model)\n\n# Andrews-Lu MMSC for moment selection\nmmsc = pvar_mmsc(model)\n\n# Stability check (all eigenvalues inside unit circle)\nstab = pvar_stability(model)\n\nInterpretation. A non-rejected Hansen J-test (large p-value) supports the validity of the instruments. The MMSC criteria help select among alternative moment conditions. All eigenvalues of the companion matrix should be inside the unit circle for the PVAR to be stable.","category":"section"},{"location":"examples/#Structural-Analysis","page":"Examples","title":"Structural Analysis","text":"# Orthogonalized IRF (Cholesky identification)\nirfs = pvar_oirf(model, 10)\n\n# Generalized IRF (order-invariant, Pesaran & Shin 1998)\ngirfs = pvar_girf(model, 10)\n\n# Forecast error variance decomposition\nfv = pvar_fevd(model, 10)","category":"section"},{"location":"examples/#Bootstrap-Confidence-Intervals","page":"Examples","title":"Bootstrap Confidence Intervals","text":"Random.seed!(42)\n\n# Group-level block bootstrap for IRF confidence intervals\nboot_irfs = pvar_bootstrap_irf(model, 10; n_draws=200)\n\nInterpretation. The bootstrap resamples entire cross-sectional units (countries) to preserve within-country dependence. With 38 OECD countries and 200 draws, the resulting confidence intervals account for both estimation uncertainty and cross-country heterogeneity. Report median IRFs with 90% bootstrap bands.","category":"section"},{"location":"examples/#Alternative-Estimators","page":"Examples","title":"Alternative Estimators","text":"# Fixed-Effects OLS (within estimator)\nfe_model = estimate_pvar_feols(pwt, 2; dependent_vars=dep_vars)\n\n# System GMM (Blundell & Bond 1998)\nsys_model = estimate_pvar(pwt, 2; steps=:twostep, system_instruments=true, dependent_vars=dep_vars)\n\nInterpretation. FE-OLS is consistent when T is large relative to N but suffers from Nickell bias in short panels. System GMM adds level equations with lagged differences as instruments, improving efficiency when the first-difference instruments are weak (near unit root). Compare coefficient estimates across estimators as a robustness check.\n\n","category":"section"},{"location":"examples/#Example-12:-GMM-Estimation","page":"Examples","title":"Example 12: GMM Estimation","text":"This example demonstrates GMM estimation of a simple IV regression using FRED-MD data.","category":"section"},{"location":"examples/#Define-Moment-Conditions","page":"Examples","title":"Define Moment Conditions","text":"using MacroEconometricModels\nusing Random\nusing LinearAlgebra\n\n# Load FRED-MD and construct variables for IV estimation\nfred = load_example(:fred_md)\nY_all = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\", \"UNRATE\"]]))\nY_all = Y_all[all.(isfinite, eachrow(Y_all)), :]\n\n# IV regression: Output growth on inflation, instrumented by fed funds and unemployment\n# Model: INDPRO_growth = β₁ + β₂ * CPIAUCSL_growth + ε\n# Instruments: FEDFUNDS, UNRATE (correlated with inflation, excluded from output eq.)\n\ny_dep = Y_all[:, 1]                               # Output growth\nX_endo = hcat(ones(size(Y_all, 1)), Y_all[:, 2])  # Constant + inflation\nZ_inst = hcat(ones(size(Y_all, 1)), Y_all[:, 3:4]) # Constant + rate + unemployment\n\nn_obs = length(y_dep)\nn_params = 2\n\n# Data bundle\ndata = (Y = y_dep, X = X_endo, Z = Z_inst)\n\n# Moment function: E[Z'(Y - Xβ)] = 0\nfunction moment_conditions(theta, data)\n    residuals = data.Y - data.X * theta\n    data.Z .* residuals  # n_obs × n_moments matrix\nend","category":"section"},{"location":"examples/#GMM-Estimation","page":"Examples","title":"GMM Estimation","text":"# Initial values\ntheta0 = zeros(n_params)\n\n# Two-step efficient GMM\ngmm_result = estimate_gmm(moment_conditions, theta0, data;\n    weighting = :two_step,\n    hac = true\n)\n\nprintln(\"GMM Estimation Results:\")\nprintln(\"  Estimated β: \", round.(gmm_result.theta, digits=4))\nprintln(\"  Converged: \", gmm_result.converged)\nprintln(\"  Iterations: \", gmm_result.iterations)\n\n# Standard errors\nse = sqrt.(diag(gmm_result.vcov))\nprintln(\"\\n  Standard errors: \", round.(se, digits=4))\n\n# Confidence intervals\nz = 1.96\nfor i in 1:n_params\n    lo = round(gmm_result.theta[i] - z * se[i], digits=4)\n    hi = round(gmm_result.theta[i] + z * se[i], digits=4)\n    println(\"  β[$i]: 95% CI = [$lo, $hi]\")\nend","category":"section"},{"location":"examples/#J-Test-for-Overidentification","page":"Examples","title":"J-Test for Overidentification","text":"# Test overidentifying restrictions\nj_result = j_test(gmm_result)\n\nprintln(\"\\nHansen J-test:\")\nprintln(\"  J-statistic: \", round(j_result.J_stat, digits=4))\nprintln(\"  Degrees of freedom: \", j_result.df)\nprintln(\"  p-value: \", round(j_result.p_value, digits=4))\nprintln(\"  Reject at 5%: \", j_result.reject_05)\n\nThe GMM estimate of the inflation coefficient captures the partial correlation of CPI inflation with output growth, after instrumenting with the federal funds rate and unemployment rate. The standard errors from two-step efficient GMM are asymptotically optimal. The Hansen J-test evaluates whether the moment conditions are jointly satisfied: a large p-value (failing to reject) indicates that the instruments are valid and the model is correctly specified. Rejection suggests either invalid instruments or model misspecification.\n\n","category":"section"},{"location":"examples/#Example-13:-Non-Gaussian-Identification","page":"Examples","title":"Example 13: Non-Gaussian Identification","text":"When structural shocks are non-Gaussian, statistical independence provides identification without imposing economic restrictions like recursive ordering or sign constraints. This example demonstrates the full non-Gaussian identification workflow on a monetary policy VAR: testing for non-Gaussianity, ICA-based and ML-based identification, and post-estimation specification tests.","category":"section"},{"location":"examples/#Setup:-Monetary-Policy-VAR","page":"Examples","title":"Setup: Monetary Policy VAR","text":"using MacroEconometricModels\nusing Random\nusing LinearAlgebra\nusing Statistics\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nT, n = size(Y)\nprintln(\"Data: T=$T, n=$n (INDPRO, CPIAUCSL, FEDFUNDS)\")","category":"section"},{"location":"examples/#Step-1:-Test-for-Non-Gaussianity","page":"Examples","title":"Step 1: Test for Non-Gaussianity","text":"Before using non-Gaussian identification, verify that residuals are indeed non-Gaussian:\n\n# Estimate VAR\nmodel = estimate_var(Y, 4)\n\n# Run the full normality test suite\nsuite = normality_test_suite(model)\n\nprintln(\"Multivariate Normality Tests (H₀: residuals are Gaussian)\")\nprintln(\"=\"^55)\nfor r in suite.results\n    stars = r.pvalue < 0.01 ? \"***\" : r.pvalue < 0.05 ? \"**\" : r.pvalue < 0.10 ? \"*\" : \"\"\n    println(\"  $(r.test_name): stat=$(round(r.statistic, digits=2)), p=$(round(r.pvalue, digits=4)) $stars\")\nend\n\nAll four tests (Jarque-Bera, Mardia, Doornik-Hansen, Henze-Zirkler) should reject normality for macroeconomic data, which typically exhibits heavy tails and skewness. If normality is not rejected, non-Gaussian identification may lack power and Cholesky or sign restrictions should be preferred.\n\nYou can also run individual tests:\n\n# Individual tests\njb = jarque_bera_test(model)\nmardia = mardia_test(model; type=:both)\ndh = doornik_hansen_test(model)\nhz = henze_zirkler_test(model)\n\nprintln(\"\\nDetailed Mardia test:\")\nprintln(\"  Skewness stat: \", round(mardia.statistic, digits=2))\nprintln(\"  P-value: \", round(mardia.pvalue, digits=4))","category":"section"},{"location":"examples/#Step-2:-ICA-Based-Identification","page":"Examples","title":"Step 2: ICA-Based Identification","text":"ICA (Independent Component Analysis) recovers structurally independent shocks by maximizing statistical independence:\n\n# FastICA identification (default: logcosh contrast)\nica_result = identify_fastica(model; contrast=:logcosh, approach=:deflation)\n\nprintln(\"\\nFastICA Identification\")\nprintln(\"=\"^40)\nprintln(\"  Converged: \", ica_result.converged)\nprintln(\"  Iterations: \", ica_result.iterations)\nprintln(\"  Objective: \", round(ica_result.objective, digits=6))\n\n# Structural impact matrix B₀\nprintln(\"\\nEstimated B₀ (structural impact matrix):\")\nfor i in 1:n\n    println(\"  \", [round(ica_result.B0[i, j], digits=3) for j in 1:n])\nend\n\nCompare different ICA algorithms:\n\n# JADE (Joint Approximate Diagonalization of Eigenmatrices)\njade_result = identify_jade(model)\n\n# SOBI (Second-Order Blind Identification — exploits temporal structure)\nsobi_result = identify_sobi(model; lags=1:12)\n\n# Distance-covariance ICA\ndcov_result = identify_dcov(model)\n\nprintln(\"\\nComparison of ICA methods:\")\nprintln(\"  FastICA converged: \", ica_result.converged, \" (iter: \", ica_result.iterations, \")\")\nprintln(\"  JADE converged:    \", jade_result.converged, \" (iter: \", jade_result.iterations, \")\")\nprintln(\"  SOBI converged:    \", sobi_result.converged, \" (iter: \", sobi_result.iterations, \")\")\nprintln(\"  dCov converged:    \", dcov_result.converged, \" (iter: \", dcov_result.iterations, \")\")\n\nFastICA is the fastest and most commonly used, but JADE is more robust when multiple shocks have similar kurtosis. SOBI exploits temporal dependence and works even with mildly non-Gaussian shocks.","category":"section"},{"location":"examples/#Step-3:-Compute-IRFs-with-ICA-Identification","page":"Examples","title":"Step 3: Compute IRFs with ICA Identification","text":"The rotation matrix Q from ICA integrates directly with the standard irf() and fevd() functions:\n\n# IRF using FastICA-identified structure\nirfs_ica = irf(model, 20; method=:fastica)\n\nprintln(\"\\nFastICA-identified IRF (shock 1 → all variables):\")\nfor h in [0, 4, 8, 12, 20]\n    vals = [round(irfs_ica.irf[h+1, v, 1], digits=3) for v in 1:n]\n    println(\"  h=$h: \", vals)\nend\n\n# FEVD using ICA identification\nfevd_ica = fevd(model, 20; method=:fastica)\n\nprintln(\"\\nFEVD at h=20 (ICA-identified):\")\nfor v in 1:n\n    shares = [round(fevd_ica.fevd[21, v, s] * 100, digits=1) for s in 1:n]\n    println(\"  Variable $v: \", shares, \"%\")\nend\n\nUnlike Cholesky identification, the ICA-based IRFs do not depend on variable ordering. The same data produces the same structural shocks regardless of how the columns of Y are arranged.","category":"section"},{"location":"examples/#Step-4:-ML-Based-Identification","page":"Examples","title":"Step 4: ML-Based Identification","text":"Maximum likelihood methods parameterize the shock distribution and jointly estimate B_0 and the distributional parameters:\n\n# Student-t ML identification\nml_t = identify_student_t(model)\n\nprintln(\"\\nStudent-t ML Identification\")\nprintln(\"=\"^40)\nprintln(\"  Converged: \", ml_t.converged)\nprintln(\"  Log-likelihood (non-Gaussian): \", round(ml_t.loglik, digits=2))\nprintln(\"  Log-likelihood (Gaussian):     \", round(ml_t.loglik_gaussian, digits=2))\nprintln(\"  AIC: \", round(ml_t.aic, digits=2))\nprintln(\"  BIC: \", round(ml_t.bic, digits=2))\n\n# Estimated degrees of freedom for each shock\nif haskey(ml_t.dist_params, :nu)\n    println(\"  Estimated ν (df): \", round.(ml_t.dist_params[:nu], digits=2))\nend\n\n# Standard errors for B₀ elements\nprintln(\"\\nB₀ standard errors:\")\nfor i in 1:n\n    println(\"  \", [round(ml_t.se[i, j], digits=4) for j in 1:n])\nend\n\nThe Student-t ML approach provides standard errors for B_0 elements, unlike ICA which only gives point estimates. Compare with other distributional assumptions:\n\n# Mixture of normals\nml_mix = identify_mixture_normal(model; n_components=2)\n\n# Pseudo-maximum likelihood (robust, no distributional assumption)\nml_pml = identify_pml(model)\n\n# Unified interface — select distribution via keyword\nml_auto = identify_nongaussian_ml(model; distribution=:student_t)\n\nprintln(\"\\nML method comparison (AIC):\")\nprintln(\"  Student-t:      AIC = \", round(ml_t.aic, digits=2))\nprintln(\"  Mixture normal: AIC = \", round(ml_mix.aic, digits=2))\nprintln(\"  PML:            AIC = \", round(ml_pml.aic, digits=2))\n\nLower AIC indicates a better distributional fit. The PML estimator is semiparametrically efficient and does not require specifying the shock distribution.","category":"section"},{"location":"examples/#Step-5:-Heteroskedasticity-Based-Identification","page":"Examples","title":"Step 5: Heteroskedasticity-Based Identification","text":"When shocks exhibit time-varying volatility, changes in the covariance structure can identify the structural model:\n\n# External volatility regimes (e.g., pre/post Great Moderation)\n# Split sample roughly in half\nT_half = T ÷ 2\nregime = vcat(ones(Int, T_half), 2 * ones(Int, T - T_half))\nvol_result = identify_external_volatility(model, regime; regimes=2)\n\nprintln(\"\\nExternal Volatility Identification\")\nprintln(\"=\"^40)\nprintln(\"  Regime 1 shock variances: \",\n        [round(vol_result.Lambda_vecs[1][j], digits=3) for j in 1:n])\nprintln(\"  Regime 2 shock variances: \",\n        [round(vol_result.Lambda_vecs[2][j], digits=3) for j in 1:n])","category":"section"},{"location":"examples/#Step-6:-Post-Estimation-Specification-Tests","page":"Examples","title":"Step 6: Post-Estimation Specification Tests","text":"Verify that the identification assumptions hold:\n\nRandom.seed!(42)\n\n# Test 1: Are recovered shocks non-Gaussian?\ngauss_test = test_shock_gaussianity(ica_result)\nprintln(\"\\nShock Gaussianity Test (H₀: shocks are Gaussian)\")\nprintln(\"  Statistic: \", round(gauss_test.statistic, digits=2))\nprintln(\"  P-value: \", round(gauss_test.pvalue, digits=4))\nprintln(\"  Non-Gaussian: \", gauss_test.identified)\n\n# Test 2: Are recovered shocks independent?\nindep_test = test_shock_independence(ica_result; max_lag=10)\nprintln(\"\\nShock Independence Test (H₀: shocks are independent)\")\nprintln(\"  Statistic: \", round(indep_test.statistic, digits=2))\nprintln(\"  P-value: \", round(indep_test.pvalue, digits=4))\nprintln(\"  Independent: \", indep_test.identified)\n\n# Test 3: Identification strength (bootstrap)\nstrength_test = test_identification_strength(model; method=:fastica, n_bootstrap=499)\nprintln(\"\\nIdentification Strength Test\")\nprintln(\"  Statistic: \", round(strength_test.statistic, digits=4))\nprintln(\"  P-value: \", round(strength_test.pvalue, digits=4))\nprintln(\"  Strongly identified: \", strength_test.identified)\n\n# Test 4: Gaussian vs non-Gaussian likelihood ratio\nlr_test = test_gaussian_vs_nongaussian(model; method=:fastica, n_bootstrap=499)\nprintln(\"\\nLR Test: Gaussian vs Non-Gaussian\")\nprintln(\"  LR statistic: \", round(lr_test.statistic, digits=2))\nprintln(\"  P-value: \", round(lr_test.pvalue, digits=4))\n\nA valid non-Gaussian SVAR requires: (1) rejection of shock Gaussianity (non-Gaussian shocks are needed for identification), (2) failure to reject shock independence (the identified shocks should be independent), and (3) strong identification (the structural parameters are precisely estimated). If the Gaussianity test fails to reject, the data may not contain enough non-Gaussianity to identify the model, and traditional Cholesky or sign restrictions should be used instead.","category":"section"},{"location":"examples/#Comparing-Cholesky-vs-ICA-Identification","page":"Examples","title":"Comparing Cholesky vs ICA Identification","text":"# Cholesky IRF (ordering-dependent)\nirfs_chol = irf(model, 20; method=:cholesky)\n\n# ICA IRF (ordering-independent)\nirfs_ica = irf(model, 20; method=:fastica)\n\nprintln(\"\\nCholesky vs FastICA IRF comparison (shock 1 → variable 1):\")\nprintln(\"  h   Cholesky   FastICA\")\nfor h in [0, 4, 8, 12, 20]\n    chol_val = round(irfs_chol.irf[h+1, 1, 1], digits=3)\n    ica_val = round(irfs_ica.irf[h+1, 1, 1], digits=3)\n    println(\"  $h    $chol_val      $ica_val\")\nend\n\nWhen the true structural ordering happens to be recursive, Cholesky and ICA should yield similar IRFs. Large discrepancies suggest that the recursive assumption may be misspecified, and the data-driven ICA identification should be preferred.\n\n","category":"section"},{"location":"examples/#Example-14:-Complete-Workflow","page":"Examples","title":"Example 14: Complete Workflow","text":"This example shows a complete empirical workflow using FRED-MD data, combining multiple techniques for a thorough monetary policy analysis.\n\nusing MacroEconometricModels\nusing Random\nusing Statistics\n\n# === Step 1: Data Preparation ===\nfred = load_example(:fred_md)\n\n# Core monetary policy variables\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\", \"M2SL\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nT, n = size(Y)\nvar_names = [\"Output\", \"Inflation\", \"Rate\", \"Money\"]\nprintln(\"Data: T=$T, n=$n ($(join(var_names, \", \")))\")\n\n# === Step 2: Lag Selection ===\nprintln(\"=\"^50)\nprintln(\"Step 1: Lag Selection\")\nprintln(\"=\"^50)\n\naics = Float64[]\nbics = Float64[]\nfor p in 1:13\n    m = estimate_var(Y, p)\n    push!(aics, aic(m))\n    push!(bics, bic(m))\nend\np_aic = argmin(aics)\np_bic = argmin(bics)\nprintln(\"AIC selects p=$p_aic, BIC selects p=$p_bic\")\np = p_bic  # Use BIC's conservative choice\n\n# === Step 3: VAR Estimation ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 2: VAR Estimation\")\nprintln(\"=\"^50)\n\nmodel = estimate_var(Y, p)\nprintln(\"Estimated VAR($p)\")\nprintln(\"Log-likelihood: \", round(loglikelihood(model), digits=2))\n\n# === Step 4: Frequentist IRF ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 3: Impulse Response Analysis\")\nprintln(\"=\"^50)\n\nH = 20\nirfs = irf(model, H; method=:cholesky)\nfevd_res = fevd(model, H; method=:cholesky)\n\n# === Step 5: Bayesian Estimation ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 4: Bayesian Analysis\")\nprintln(\"=\"^50)\n\n# Optimize priors\nbest_hyper = optimize_hyperparameters(Y, p; grid_size=15)\nprintln(\"Optimal τ: \", round(best_hyper.tau, digits=4))\n\n# BVAR with conjugate NIW sampler\npost = estimate_bvar(Y, p; n_draws=1000,\n                     prior=:minnesota, hyper=best_hyper)\n\n# Bayesian IRF\nbirf = irf(post, H; method=:cholesky)\n\n# === Step 6: Local Projections Comparison ===\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Step 5: LP vs VAR Comparison\")\nprintln(\"=\"^50)\n\nlp_model = estimate_lp(Y, 1, H; lags=p, cov_type=:newey_west)\nlp_result = lp_irf(lp_model)\n\nprintln(\"IRF(Output→Output) at h=0:\")\nprintln(\"  VAR: \", round(irfs.irf[1, 1, 1], digits=3))\nprintln(\"  LP: \", round(lp_result.values[1, 1], digits=3))\n\nprintln(\"\\nIRF(Output→Output) at h=8:\")\nprintln(\"  VAR: \", round(irfs.irf[9, 1, 1], digits=3))\nprintln(\"  LP: \", round(lp_result.values[9, 1], digits=3))\n\n# === Step 7: Robustness Check with Smooth LP ===\nsmooth_lp = estimate_smooth_lp(Y, 1, H; lambda=1.0, lags=p)\nsmooth_result = smooth_lp_irf(smooth_lp)\n\nprintln(\"\\nSmooth LP variance reduction: \",\n        round(mean(smooth_result.se.^2) / mean(lp_result.se.^2), digits=3))\n\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Analysis Complete!\")\nprintln(\"=\"^50)\n\nComparing VAR and LP impulse responses at the same horizon provides a robustness check. Under correct specification, both estimators are consistent for the same causal parameter (Plagborg-Moller & Wolf, 2021), but LP is less efficient. Large discrepancies suggest potential dynamic misspecification in the VAR. The smooth LP variance reduction ratio measures efficiency gains from B-spline regularization; values well below 1.0 indicate substantial noise reduction from imposing smoothness.\n\n","category":"section"},{"location":"examples/#Example-15:-Table-Output-—-Text,-LaTeX,-and-HTML","page":"Examples","title":"Example 15: Table Output — Text, LaTeX, and HTML","text":"All show, print_table, and Base.show methods in MacroEconometricModels route through a unified PrettyTables backend. Switching from terminal text to LaTeX or HTML output requires a single call to set_display_backend. This is useful for embedding results directly into papers (LaTeX), slides (HTML), or reports.","category":"section"},{"location":"examples/#Setup","page":"Examples","title":"Setup","text":"using MacroEconometricModels\nusing Random\n\n# Estimate a monetary policy VAR and compute IRFs + FEVD\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nmodel = estimate_var(Y, 4)\n\nRandom.seed!(42)\nirfs = irf(model, 12; method=:cholesky, ci_type=:bootstrap, n_boot=500)\nfevd_result = fevd(model, 12; method=:cholesky)","category":"section"},{"location":"examples/#Text-Output-(Default)","page":"Examples","title":"Text Output (Default)","text":"The default backend is :text, producing terminal-friendly borderless tables:\n\n# Confirm default backend\nget_display_backend()   # :text\n\n# Print IRF table for variable 1, shock 1\nprint_table(irfs, 1, 1)\n\nOutput:\n\n           IRF: Var 1 ← Shock 1\n  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n      h      IRF    CI_lo    CI_hi\n  ────────────────────────────────\n      1   1.0000   1.0000   1.0000\n      4   0.5765   0.3821   0.7542\n      8   0.2134   0.0512   0.3891\n     12   0.0712  -0.0203   0.1744\n\n# Print FEVD table for variable 2\nprint_table(fevd_result, 2)","category":"section"},{"location":"examples/#LaTeX-Output-for-Papers","page":"Examples","title":"LaTeX Output for Papers","text":"Switch to LaTeX to get tables ready for \\input{} in your .tex file:\n\n# Switch to LaTeX backend\nset_display_backend(:latex)\n\n# Print IRF table — output is now LaTeX\nprint_table(irfs, 1, 1)\n\nOutput:\n\n\\begin{table}\n  \\caption{IRF: Var 1 ← Shock 1}\n  \\begin{tabular}{rrrr}\n    \\hline\n    h & IRF & CI\\_lo & CI\\_hi \\\\\n    \\hline\n    1 & 1.0 & 1.0 & 1.0 \\\\\n    4 & 0.5765 & 0.3821 & 0.7542 \\\\\n    8 & 0.2134 & 0.0512 & 0.3891 \\\\\n    12 & 0.0712 & -0.0203 & 0.1744 \\\\\n    \\hline\n  \\end{tabular}\n\\end{table}\n\nTo save LaTeX output directly to a file:\n\nset_display_backend(:latex)\n\n# Write IRF table to file\nopen(\"tables/irf_table.tex\", \"w\") do io\n    print_table(io, irfs, 1, 1)\nend\n\n# Write FEVD table to file\nopen(\"tables/fevd_table.tex\", \"w\") do io\n    print_table(io, fevd_result, 2)\nend\n\nThen in your LaTeX document:\n\n\\begin{document}\nTable~\\ref{tab:irf} reports the impulse responses...\n\\input{tables/irf_table.tex}\n\\end{document}","category":"section"},{"location":"examples/#HTML-Output-for-Slides-and-Web","page":"Examples","title":"HTML Output for Slides and Web","text":"Switch to HTML for Jupyter notebooks, web dashboards, or HTML-based presentations:\n\n# Switch to HTML backend\nset_display_backend(:html)\n\n# Print IRF table — output is now an HTML <table>\nprint_table(irfs, 1, 1)\n\nOutput:\n\n<table>\n  <caption>IRF: Var 1 ← Shock 1</caption>\n  <tr><th>h</th><th>IRF</th><th>CI_lo</th><th>CI_hi</th></tr>\n  <tr><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr>\n  <tr><td>4</td><td>0.5765</td><td>0.3821</td><td>0.7542</td></tr>\n  ...\n</table>\n\nTo save HTML output to a file:\n\nset_display_backend(:html)\n\nopen(\"tables/irf_table.html\", \"w\") do io\n    print_table(io, irfs, 1, 1)\nend","category":"section"},{"location":"examples/#Switching-Backends-in-a-Workflow","page":"Examples","title":"Switching Backends in a Workflow","text":"You can switch backends freely within a session. A common pattern for a research workflow:\n\nusing MacroEconometricModels\nusing Random\n\n# Use the monetary policy VAR from Example 6\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nmodel = estimate_var(Y, 4)\nH = 20\n\nRandom.seed!(42)\nirfs = irf(model, H; method=:cholesky, ci_type=:bootstrap, n_boot=500)\nfevd_result = fevd(model, H; method=:cholesky)\nhd_result = historical_decomposition(model)\n\n# === Step 1: Inspect in terminal ===\nset_display_backend(:text)\nprint_table(irfs, 1, 1)       # Quick look at IRF\nprint_table(fevd_result, 1)    # Quick look at FEVD\n\n# === Step 2: Export LaTeX for the paper ===\nset_display_backend(:latex)\n\nopen(\"tables/irf_gdp.tex\", \"w\") do io\n    print_table(io, irfs, 1, 1; horizons=[1, 4, 8, 12, 20])\nend\n\nopen(\"tables/fevd_gdp.tex\", \"w\") do io\n    print_table(io, fevd_result, 1; horizons=[1, 4, 8, 12, 20])\nend\n\n# === Step 3: Export HTML for slides ===\nset_display_backend(:html)\n\nopen(\"slides/irf_gdp.html\", \"w\") do io\n    print_table(io, irfs, 1, 1)\nend\n\n# === Step 4: Reset to text for continued interactive work ===\nset_display_backend(:text)","category":"section"},{"location":"examples/#Using-table()-to-Extract-Raw-Data","page":"Examples","title":"Using table() to Extract Raw Data","text":"The table() function returns a plain Matrix that you can manipulate, pass to DataFrames, or export with CSV:\n\nusing DataFrames, CSV\n\n# Extract IRF as a matrix: columns are [h, IRF, CI_lo, CI_hi]\nirf_data = table(irfs, 1, 1; horizons=[1, 4, 8, 12, 20])\n\n# Convert to DataFrame\ndf = DataFrame(irf_data, [:h, :IRF, :CI_lo, :CI_hi])\n\n# Save as CSV\nCSV.write(\"tables/irf_data.csv\", df)\n\n# Extract FEVD as a matrix: columns are [h, Shock1, Shock2, ..., ShockN]\nfevd_data = table(fevd_result, 1; horizons=[1, 4, 8, 12, 20])","category":"section"},{"location":"examples/#Backend-Affects-show()-Too","page":"Examples","title":"Backend Affects show() Too","text":"The display backend also controls how objects render when printed in the REPL or displayed in Jupyter:\n\nset_display_backend(:latex)\n\n# REPL display is now LaTeX\nmodel    # VARModel show → LaTeX tables\nirfs     # ImpulseResponse show → LaTeX tables\n\nset_display_backend(:text)  # Reset\n\nThis means in a Jupyter notebook, you can set the backend to :html once at the top:\n\n# Top of Jupyter notebook\nusing MacroEconometricModels\nset_display_backend(:html)\n\n# All subsequent cells render as formatted HTML tables\nmodel = estimate_var(Y, 4)\nirfs = irf(model, 12; method=:cholesky)\nirfs   # Displays as an HTML table","category":"section"},{"location":"examples/#Summary-of-Output-Functions","page":"Examples","title":"Summary of Output Functions","text":"Function Returns Use Case\ntable(result, ...) Matrix Raw numeric data for custom processing, CSV export\nprint_table([io], result, ...) Nothing (prints) Formatted output via current backend (text/LaTeX/HTML)\nshow(io, result) Nothing (prints) REPL display, also respects backend\nset_display_backend(:text) Nothing Terminal output (default)\nset_display_backend(:latex) Nothing LaTeX \\begin{tabular} output\nset_display_backend(:html) Nothing HTML <table> output\nget_display_backend() Symbol Check current backend\n\n","category":"section"},{"location":"examples/#Example-16:-Bibliographic-References","page":"Examples","title":"Example 16: Bibliographic References","text":"The refs() function returns bibliographic references for any model, result type, or identification method. References are available in four formats: AEA text (default), BibTeX, LaTeX \\bibitem, and HTML with clickable DOI links.","category":"section"},{"location":"examples/#Basic-Usage","page":"Examples","title":"Basic Usage","text":"using MacroEconometricModels, Random\n\n# Estimate a model\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 4)\n\n# Get references for this model type (AEA text format)\nrefs(model)\n\nOutput:\n\nSims, Christopher A. 1980. \"Macroeconomics and Reality.\" Econometrica 48 (1): 1-48.\nLutkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer.","category":"section"},{"location":"examples/#Multiple-Output-Formats","page":"Examples","title":"Multiple Output Formats","text":"# BibTeX format — paste into your .bib file\nrefs(model; format=:bibtex)\n\n# LaTeX \\bibitem format\nrefs(model; format=:latex)\n\n# HTML with clickable DOI links\nrefs(model; format=:html)","category":"section"},{"location":"examples/#References-by-Method-Name","page":"Examples","title":"References by Method Name","text":"# References for identification methods\nrefs(:cholesky)       # Cholesky decomposition\nrefs(:fastica)        # FastICA for SVAR\nrefs(:sign)           # Sign restrictions\nrefs(:johansen)       # Johansen cointegration\nrefs(:garch)          # GARCH models\n\n# References for specific result types\nindpro_growth = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))\ngarch = estimate_garch(indpro_growth, 1, 1)\nrefs(garch)           # Bollerslev (1986)\n\nRandom.seed!(42)\nsv = estimate_sv(indpro_growth; n_samples=500, burnin=200)\nrefs(sv)              # Taylor (1986), Kim et al. (1998), Omori et al. (2007)","category":"section"},{"location":"examples/#Export-to-.bib-File","page":"Examples","title":"Export to .bib File","text":"# Write BibTeX entries for all models used in your analysis\nopen(\"references.bib\", \"w\") do io\n    refs(io, model; format=:bibtex)\n    println(io)\n    refs(io, :fastica; format=:bibtex)\n    println(io)\n    refs(io, :johansen; format=:bibtex)\nend\n\nThe refs() function covers all 45+ references in the package's database, including every estimation method, identification scheme, and test. This ensures correct citation of the methods used in your empirical analysis.\n\n","category":"section"},{"location":"examples/#Example-17:-Nowcasting","page":"Examples","title":"Example 17: Nowcasting","text":"This example demonstrates the full nowcasting workflow using FRED-MD data: constructing mixed-frequency data, estimating three nowcasting models, comparing results, and decomposing forecast revisions into data release contributions. See Nowcasting for model specifications and theory.","category":"section"},{"location":"examples/#Construct-Mixed-Frequency-Data","page":"Examples","title":"Construct Mixed-Frequency Data","text":"using MacroEconometricModels, Random\n\n# Load FRED-MD and select a mixed-frequency panel\nfred = load_example(:fred_md)\n\n# Monthly indicators (first 8 variables, transformed)\nmonthly_vars = varnames(fred)[1:8]\nfred_sub = apply_tcode(fred[:, monthly_vars])\nY_m = to_matrix(fred_sub)\n\n# Construct quarterly GDP proxy: aggregate 3 monthly INDPRO observations\n# In practice, quarterly variables come from FRED-QD; here we create a simple\n# mixed-frequency layout by treating every 3rd row as quarterly observations\nT_obs = size(Y_m, 1)\nnM = size(Y_m, 2)\nnQ = 2  # Two quarterly targets\n\n# Build mixed-frequency matrix: monthly indicators + quarterly targets\nY = hcat(Y_m, fill(NaN, T_obs, nQ))\n\n# Quarterly targets: observed every 3rd month only\nqd = load_example(:fred_qd)\ngdp_growth = diff(log.(qd[:, \"GDPC1\"]))\npce_growth = diff(log.(qd[:, \"PCECC96\"]))\n\n# Map quarterly observations to monthly grid (every 3rd month)\nn_quarters = min(T_obs ÷ 3, length(gdp_growth))\nfor q in 1:n_quarters\n    t_month = 3 * q\n    if t_month <= T_obs\n        Y[t_month, nM+1] = gdp_growth[min(q, length(gdp_growth))]\n        Y[t_month, nM+2] = pce_growth[min(q, length(pce_growth))]\n    end\nend\n\n# Clean: keep only rows where monthly data is available\nvalid_rows = all.(isfinite, eachrow(Y[:, 1:nM]))\nY = Y[valid_rows, :]\nT_obs = size(Y, 1)\n\n# Ragged edge: remove last observations for slow-release variables\nY[end, 5:nM] .= NaN\nY[end-1, 7:nM] .= NaN\n\nN = nM + nQ\nprintln(\"Data: T=$T_obs, nM=$nM monthly, nQ=$nQ quarterly\")\nprintln(\"Missing: \", sum(isnan.(Y)), \" / \", length(Y), \" entries\")\n\nInterpretation. The data matrix has the standard nowcasting layout: monthly indicators in the first columns, quarterly targets in the last columns. Quarterly values appear every 3rd row (months 3, 6, 9, ...) with NaN elsewhere. The ragged edge mimics real-world publication lags where some monthly series release earlier than others.","category":"section"},{"location":"examples/#DFM-Nowcasting","page":"Examples","title":"DFM Nowcasting","text":"# Estimate DFM with 2 factors and AR(1) idiosyncratic dynamics\ndfm = nowcast_dfm(Y, nM, nQ; r=2, p=1, idio=:ar1, max_iter=100)\n\nprintln(\"DFM estimation:\")\nprintln(\"  Factors: \", dfm.r)\nprintln(\"  EM iterations: \", dfm.n_iter)\nprintln(\"  Log-likelihood: \", round(dfm.loglik, digits=2))\n\n# Extract nowcast\nr_dfm = nowcast(dfm)\nprintln(\"  Nowcast: \", round(r_dfm.nowcast, digits=3))\nprintln(\"  Forecast: \", round(r_dfm.forecast, digits=3))","category":"section"},{"location":"examples/#BVAR-Nowcasting","page":"Examples","title":"BVAR Nowcasting","text":"# Estimate large BVAR with optimized hyperparameters\nbvar = nowcast_bvar(Y, nM, nQ; lags=5)\n\nprintln(\"\\nBVAR estimation:\")\nprintln(\"  Lags: \", bvar.lags)\nprintln(\"  Optimized λ: \", round(bvar.lambda, digits=4))\nprintln(\"  Optimized θ: \", round(bvar.theta, digits=4))\nprintln(\"  Marginal loglik: \", round(bvar.loglik, digits=2))\n\nr_bvar = nowcast(bvar)\nprintln(\"  Nowcast: \", round(r_bvar.nowcast, digits=3))","category":"section"},{"location":"examples/#Bridge-Equation-Nowcasting","page":"Examples","title":"Bridge Equation Nowcasting","text":"# Estimate bridge equations (all pairs of monthly indicators)\nbridge = nowcast_bridge(Y, nM, nQ; lagM=1, lagQ=1, lagY=1)\n\nprintln(\"\\nBridge estimation:\")\nprintln(\"  Equations: \", bridge.n_equations)\n\nr_bridge = nowcast(bridge)\nprintln(\"  Nowcast: \", round(r_bridge.nowcast, digits=3))","category":"section"},{"location":"examples/#Method-Comparison","page":"Examples","title":"Method Comparison","text":"println(\"\\n=== Nowcast Comparison ===\")\nprintln(\"  DFM:    \", round(r_dfm.nowcast, digits=3))\nprintln(\"  BVAR:   \", round(r_bvar.nowcast, digits=3))\nprintln(\"  Bridge: \", round(r_bridge.nowcast, digits=3))\n\nInterpretation. Comparing nowcasts across methods provides a robustness check. The DFM is best for large cross-sections (it summarizes information via factors), the BVAR captures direct cross-variable dynamics, and bridge equations give a transparent baseline. In practice, central banks often report a range or average across methods.","category":"section"},{"location":"examples/#Forecasting","page":"Examples","title":"Forecasting","text":"# Multi-step DFM forecast\nfc_dfm = forecast(dfm, 6; target_var=N)\nprintln(\"\\nDFM 6-step forecast:\")\nfor h in 1:6\n    println(\"  h=$h: \", round(fc_dfm[h], digits=3))\nend\n\n# Multi-step BVAR forecast\nfc_bvar = forecast(bvar, 6; target_var=N)\nprintln(\"\\nBVAR 6-step forecast:\")\nfor h in 1:6\n    println(\"  h=$h: \", round(fc_bvar[h], digits=3))\nend","category":"section"},{"location":"examples/#News-Decomposition","page":"Examples","title":"News Decomposition","text":"The news decomposition attributes nowcast revisions to individual data releases:\n\n# Create two data vintages\nX_old = copy(Y)\nX_new = copy(Y)\n# Simulate that variables 1-3 were just released for the last month\nX_old[end, 1:3] .= NaN\n\n# Compute news decomposition\nnews = nowcast_news(X_new, X_old, dfm, T_obs; target_var=N)\n\nprintln(\"\\nNews Decomposition:\")\nprintln(\"  Old nowcast: \", round(news.old_nowcast, digits=3))\nprintln(\"  New nowcast: \", round(news.new_nowcast, digits=3))\nprintln(\"  Revision:    \", round(news.new_nowcast - news.old_nowcast, digits=3))\n\nprintln(\"\\nRelease impacts (sorted by |impact|):\")\nsorted_idx = sortperm(abs.(news.impact_news), rev=true)\nfor k in 1:min(5, length(sorted_idx))\n    j = sorted_idx[k]\n    println(\"  \", news.variable_names[j], \": \",\n            round(news.impact_news[j], digits=4))\nend\n\nInterpretation. The news decomposition answers the key question for real-time forecasters: Why did the nowcast change? Each impact_news[j] shows how much release j contributed to the revision. A positive impact means the actual value was higher than expected, revising the nowcast upward. This is the standard communication tool used by central banks to explain forecast revisions.","category":"section"},{"location":"examples/#Balancing-a-Panel","page":"Examples","title":"Balancing a Panel","text":"# Fill NaN in a TimeSeriesData container using DFM\nall_varnames = vcat(monthly_vars, [\"GDP_growth\", \"PCE_growth\"])\nts = TimeSeriesData(Y; varnames=all_varnames, frequency=Monthly)\n\nts_balanced = balance_panel(ts; r=2, p=1)\nprintln(\"\\nBalanced panel: \", sum(isnan.(to_matrix(ts_balanced))), \" NaN remaining\")\n\n","category":"section"},{"location":"examples/#Best-Practices","page":"Examples","title":"Best Practices","text":"","category":"section"},{"location":"examples/#Data-Preparation","page":"Examples","title":"Data Preparation","text":"Stationarity: Test for unit roots using ADF and KPSS together\nBoth fail to reject → inconclusive, consider structural breaks\nADF rejects, KPSS doesn't → stationary (I(0))\nADF doesn't reject, KPSS rejects → unit root (I(1))\nStructural Breaks: Use Zivot-Andrews test if visual inspection suggests breaks\nCointegration: For I(1) variables, test for cointegration before differencing\nOutliers: Check for and handle outliers\nMissing data: Factor models can handle some missing data; VARs require complete data\nScaling: For factor models, standardize variables","category":"section"},{"location":"examples/#Model-Selection","page":"Examples","title":"Model Selection","text":"Lag length: Use information criteria (BIC is more conservative)\nNumber of factors: Use Bai-Ng criteria; prefer IC2 or IC3\nPrior tightness: Optimize via marginal likelihood for large models","category":"section"},{"location":"examples/#Identification","page":"Examples","title":"Identification","text":"Economic theory: Base restrictions on economic reasoning\nRobustness: Try multiple identification schemes\nNarrative: Use historical knowledge when available\nNon-Gaussian: Test residuals with normality_test_suite first; if non-Gaussian, ICA/ML methods provide ordering-free identification\nSpecification tests: Validate non-Gaussian identification with test_shock_gaussianity and test_shock_independence","category":"section"},{"location":"examples/#Inference","page":"Examples","title":"Inference","text":"HAC standard errors: Always use for LP at horizons > 0\nCredible intervals: Report 68% and 90% bands for Bayesian\nBootstrap: Use for frequentist VAR confidence intervals","category":"section"},{"location":"examples/#Reporting","page":"Examples","title":"Reporting","text":"Present both: VAR and LP estimates as robustness check\nHorizon selection: Focus on economically meaningful horizons\nFEVD: Report at multiple horizons (short, medium, long-run)\nLaTeX export: Use set_display_backend(:latex) then print_table(io, ...) for paper-ready tables\nHTML export: Use set_display_backend(:html) for Jupyter notebooks and web reports\nRaw data: Use table(result, ...) to extract matrices for custom formatting or CSV export\n\n","category":"section"},{"location":"arima/#Univariate-Time-Series-(ARIMA)","page":"ARIMA","title":"Univariate Time Series (ARIMA)","text":"This chapter covers univariate ARIMA-class models: AR, MA, ARMA, and ARIMA. These models are fundamental building blocks for time series analysis, forecasting, and as diagnostic tools for checking residual autocorrelation in multivariate models.","category":"section"},{"location":"arima/#Introduction","page":"ARIMA","title":"Introduction","text":"Univariate time series models capture temporal dependence through autoregressive (AR) and moving average (MA) components. The general ARIMA(p,d,q) model nests:\n\nAR(p): Autoregressive model — current value depends on past values\nMA(q): Moving average model — current value depends on past shocks\nARMA(p,q): Combined autoregressive–moving average\nARIMA(p,d,q): Integrated ARMA — models non-stationary series via differencing\n\nReferences: Box & Jenkins (1976), Hamilton (1994, Chapters 3–5), Brockwell & Davis (1991)","category":"section"},{"location":"arima/#Quick-Start","page":"ARIMA","title":"Quick Start","text":"using MacroEconometricModels\n\n# Industrial production growth (monthly, FRED-MD)\nfred = load_example(:fred_md)\ny = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))    # log first difference\n\nar = estimate_ar(y, 2)                                      # AR(2) via OLS\nma = estimate_ma(y, 1; method=:css_mle)                     # MA(1) via CSS-MLE\narma = estimate_arma(y, 1, 1)                                # ARMA(1,1)\nfc = forecast(arma, 12; conf_level=0.95)                     # Forecast IP growth 12 months ahead\nsel = select_arima_order(y, 4, 4)                            # Grid search for best (p,q)\n\n# For I(1) level modeling\ny_level = filter(isfinite, log.(fred[:, \"INDPRO\"]))\narima = estimate_arima(y_level, 1, 1, 0)                    # ARIMA(1,1,0) on log IP\n\n","category":"section"},{"location":"arima/#The-AR(p)-Model","page":"ARIMA","title":"The AR(p) Model","text":"","category":"section"},{"location":"arima/#Model-Specification","page":"ARIMA","title":"Model Specification","text":"An autoregressive model of order p is:\n\ny_t = c + phi_1 y_t-1 + phi_2 y_t-2 + cdots + phi_p y_t-p + varepsilon_t\n\nwhere varepsilon_t sim textWN(0 sigma^2) is white noise. Using the lag operator L:\n\nphi(L) y_t = c + varepsilon_t quad phi(L) = 1 - phi_1 L - phi_2 L^2 - cdots - phi_p L^p","category":"section"},{"location":"arima/#Stationarity","page":"ARIMA","title":"Stationarity","text":"The process is stationary if all roots of the characteristic polynomial phi(z) = 0 lie outside the unit circle, equivalently if all eigenvalues of the companion matrix\n\nF = beginbmatrix\nphi_1  phi_2  cdots  phi_p-1  phi_p \n1  0  cdots  0  0 \n0  1  cdots  0  0 \nvdots  vdots  ddots  vdots  vdots \n0  0  cdots  1  0\nendbmatrix_p times p\n\nhave modulus less than 1: lambda_i(F)  1 for all i.","category":"section"},{"location":"arima/#Estimation","page":"ARIMA","title":"Estimation","text":"AR models support two estimation methods:\n\nOLS (:ols, default): For AR(p), construct the regression:\n\ny_t = beta_0 + beta_1 y_t-1 + cdots + beta_p y_t-p + varepsilon_t\n\nand apply ordinary least squares. This is consistent and asymptotically efficient for stationary AR processes.\n\nMaximum Likelihood (:mle): Maximizes the exact Gaussian log-likelihood via the Kalman filter (see Exact MLE via Kalman Filter below).\n\nusing MacroEconometricModels\n\n# Industrial production growth (monthly, FRED-MD)\nfred = load_example(:fred_md)\ny = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))\n\n# OLS estimation (default)\nar_ols = estimate_ar(y, 2)\n\n# MLE estimation\nar_mle = estimate_ar(y, 2; method=:mle)\n\n# Access results\nar_ols.phi      # AR coefficients [φ₁, φ₂]\nar_ols.sigma2   # Innovation variance\nar_ols.aic      # Akaike Information Criterion\nar_ols.bic      # Bayesian Information Criterion","category":"section"},{"location":"arima/#ARModel-Return-Values","page":"ARIMA","title":"ARModel Return Values","text":"Field Type Description\ny Vector{T} Original time series\np Int AR order\nc T Intercept (constant term)\nphi Vector{T} AR coefficients phi_1 ldots phi_p\nsigma2 T Innovation variance hatsigma^2\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T Akaike Information Criterion\nbic T Bayesian Information Criterion\nmethod Symbol Estimation method (:ols or :mle)\nconverged Bool Convergence indicator\niterations Int Number of optimization iterations\n\nReference: Hamilton (1994, Section 5.2)\n\n","category":"section"},{"location":"arima/#The-MA(q)-Model","page":"ARIMA","title":"The MA(q) Model","text":"","category":"section"},{"location":"arima/#Model-Specification-2","page":"ARIMA","title":"Model Specification","text":"A moving average model of order q is:\n\ny_t = c + varepsilon_t + theta_1 varepsilon_t-1 + theta_2 varepsilon_t-2 + cdots + theta_q varepsilon_t-q\n\nor equivalently:\n\ny_t = c + theta(L) varepsilon_t quad theta(L) = 1 + theta_1 L + theta_2 L^2 + cdots + theta_q L^q","category":"section"},{"location":"arima/#Invertibility","page":"ARIMA","title":"Invertibility","text":"The MA process is invertible if all roots of theta(z) = 0 lie outside the unit circle. Invertibility ensures a unique MA representation and is checked via the companion matrix eigenvalue condition, identical in form to the AR stationarity check.","category":"section"},{"location":"arima/#Estimation-2","page":"ARIMA","title":"Estimation","text":"MA parameters cannot be estimated by OLS. Three methods are available:\n\nCSS (:css): Conditional Sum of Squares — fast, approximate\nMLE (:mle): Exact MLE via Kalman filter\nCSS-MLE (:css_mle, default): CSS initialization followed by MLE refinement\n\nma_model = estimate_ma(y, 1; method=:css_mle)\nma_model.theta   # MA coefficient [θ₁]","category":"section"},{"location":"arima/#MAModel-Return-Values","page":"ARIMA","title":"MAModel Return Values","text":"Field Type Description\ny Vector{T} Original time series\nq Int MA order\nc T Intercept\ntheta Vector{T} MA coefficients theta_1 ldots theta_q\nsigma2 T Innovation variance\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method (:css, :mle, :css_mle)\nconverged Bool Convergence indicator\niterations Int Number of optimization iterations\n\n","category":"section"},{"location":"arima/#The-ARMA(p,q)-Model","page":"ARIMA","title":"The ARMA(p,q) Model","text":"","category":"section"},{"location":"arima/#Model-Specification-3","page":"ARIMA","title":"Model Specification","text":"The ARMA(p,q) model combines autoregressive and moving average components:\n\ny_t = c + sum_i=1^p phi_i y_t-i + varepsilon_t + sum_j=1^q theta_j varepsilon_t-j\n\nor in lag-operator form:\n\nphi(L) y_t = c + theta(L) varepsilon_t\n\nThe process is stationary and invertible when all roots of phi(z) and theta(z) lie outside the unit circle, respectively.","category":"section"},{"location":"arima/#Estimation-3","page":"ARIMA","title":"Estimation","text":"The same three methods (:css, :mle, :css_mle) are available. The unified internal pipeline _estimate_arma_internal dispatches to the appropriate method:\n\nCSS: Minimizes the conditional sum of squared residuals using Nelder-Mead. Residuals are computed recursively: hatvarepsilon_t = y_t - c - sum_i phi_i y_t-i - sum_j theta_j hatvarepsilon_t-j\nMLE: Maximizes the exact Gaussian log-likelihood via the Kalman filter using L-BFGS optimization. The variance parameter is optimized on the log scale for unconstrained optimization.\nCSS-MLE (default): Uses CSS estimates to initialize MLE, combining the robustness of CSS with the efficiency of exact MLE.\n\narma_model = estimate_arma(y, 1, 1; method=:css_mle)\narma_model.phi     # AR coefficients\narma_model.theta   # MA coefficients\narma_model.loglik  # Log-likelihood","category":"section"},{"location":"arima/#ARMAModel-Return-Values","page":"ARIMA","title":"ARMAModel Return Values","text":"Field Type Description\ny Vector{T} Original time series\np Int AR order\nq Int MA order\nc T Intercept\nphi Vector{T} AR coefficients phi_1 ldots phi_p\ntheta Vector{T} MA coefficients theta_1 ldots theta_q\nsigma2 T Innovation variance\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence indicator\niterations Int Number of iterations\n\nnote: Technical Note\nCSS (Conditional Sum of Squares) is fast but approximate: it conditions on initial residuals being zero, which introduces bias in small samples. MLE via the Kalman filter provides exact inference by properly handling initialization but is computationally more expensive and can be sensitive to starting values. The default :css_mle combines both: CSS provides robust starting values, then MLE refines to the exact optimum. For pure AR models, OLS is equivalent to CSS and is preferred for speed.\n\nReference: Hamilton (1994, Chapter 5), Harvey (1993, Chapter 3)\n\n","category":"section"},{"location":"arima/#The-ARIMA(p,d,q)-Model","page":"ARIMA","title":"The ARIMA(p,d,q) Model","text":"","category":"section"},{"location":"arima/#Model-Specification-4","page":"ARIMA","title":"Model Specification","text":"The ARIMA(p,d,q) model applies d-fold differencing to produce a stationary series, then fits an ARMA(p,q):\n\nphi(L) (1-L)^d y_t = c + theta(L) varepsilon_t\n\nwhere (1-L)^d y_t denotes the d-th difference of y_t. Common cases:\n\nd = 1: Delta y_t = y_t - y_t-1 (first difference, for I(1) series)\nd = 2: Delta^2 y_t = Delta y_t - Delta y_t-1 (second difference, for I(2) series)","category":"section"},{"location":"arima/#Estimation-4","page":"ARIMA","title":"Estimation","text":"The implementation differences the series d times, then estimates ARMA(p,q) on the differenced series using the unified estimation pipeline.\n\n# Log industrial production — an I(1) series\nfred = load_example(:fred_md)\ny_level = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\n# Fit ARIMA(1,1,0) — differenced once, then AR(1)\nmodel = estimate_arima(y_level, 1, 1, 0)\nmodel.phi    # AR coefficients on differenced series\nmodel.d      # Integration order","category":"section"},{"location":"arima/#ARIMAModel-Return-Values","page":"ARIMA","title":"ARIMAModel Return Values","text":"Field Type Description\ny Vector{T} Original (undifferenced) time series\ny_diff Vector{T} d-fold differenced series\np Int AR order\nd Int Integration order\nq Int MA order\nc T Intercept (on differenced series)\nphi Vector{T} AR coefficients\ntheta Vector{T} MA coefficients\nsigma2 T Innovation variance\nresiduals Vector{T} Estimated residuals\nfitted Vector{T} Fitted values (on differenced scale)\nloglik T Log-likelihood\naic T AIC\nbic T BIC\nmethod Symbol Estimation method\nconverged Bool Convergence indicator\niterations Int Number of iterations\n\n","category":"section"},{"location":"arima/#kalman_mle","page":"ARIMA","title":"Exact MLE via Kalman Filter","text":"","category":"section"},{"location":"arima/#State-Space-Representation","page":"ARIMA","title":"State-Space Representation","text":"For exact maximum likelihood estimation, the ARMA(p,q) model is cast into Harvey's (1993) state-space form:\n\nObservation equation:\n\ny_t = c + Z alpha_t\n\nState equation:\n\nalpha_t+1 = T alpha_t + R eta_t quad eta_t sim N(0 Q)\n\nwhere the state vector alpha_t = a_t a_t-1 ldots a_t-r+1 has dimension r = max(p q+1), and:\n\nZ = 1 theta_1 ldots theta_r-1 is the observation vector\nT is the r times r companion matrix with AR coefficients in the first row\nR = 1 0 ldots 0 is the selection vector\nQ = sigma^2 is the innovation variance","category":"section"},{"location":"arima/#Kalman-Filter-Log-Likelihood","page":"ARIMA","title":"Kalman Filter Log-Likelihood","text":"The log-likelihood is computed via the prediction error decomposition:\n\nell(Theta) = -fracn2 log(2pi) - frac12 sum_t=1^n left( log f_t + fracv_t^2f_t right)\n\nwhere\n\nv_t = y_t - haty_tt-1 is the one-step prediction error\nf_t = Z P_tt-1 Z + H is its variance\nn is the number of observations\nTheta denotes the full parameter vector (phi_1 ldots phi_p theta_1 ldots theta_q sigma^2)\n\nInitialization: Uses the unconditional (stationary) distribution when the system is stable, falling back to diffuse initialization (P_0 = 10^6 I) for non-stationary parameters.\n\nReference: Harvey (1993, Chapters 3–4), Durbin & Koopman (2012, Chapter 2)\n\n","category":"section"},{"location":"arima/#Forecasting","page":"ARIMA","title":"Forecasting","text":"","category":"section"},{"location":"arima/#Point-Forecasts","page":"ARIMA","title":"Point Forecasts","text":"The optimal h-step ahead forecast minimizes mean squared error. For an ARMA(p,q) process, forecasts are computed recursively:\n\nhaty_T+hT = c + sum_i=1^p phi_i haty_T+h-iT + sum_j=1^q theta_j hatvarepsilon_T+h-j\n\nwhere haty_T+kT = y_T+k for k leq 0 and hatvarepsilon_T+k = 0 for k geq 1 (future residuals are set to zero as the best linear predictor).","category":"section"},{"location":"arima/#Forecast-Uncertainty","page":"ARIMA","title":"Forecast Uncertainty","text":"Forecast standard errors are derived from the MA(infty) representation. The psi-weights satisfy:\n\npsi_j = sum_i=1^min(pj) phi_i psi_j-i + theta_j mathbb1(j leq q) quad psi_0 = 1\n\nwhere\n\npsi_j is the j-th coefficient in the MA(infty) representation y_t = sum_j=0^infty psi_j varepsilon_t-j\nphi_i are the AR coefficients (zero for i  p)\ntheta_j are the MA coefficients (zero for j  q)\n\nThe h-step ahead forecast variance is:\n\ntextVar(e_T+hT) = sigma^2 left(1 + psi_1^2 + psi_2^2 + cdots + psi_h-1^2 right)\n\nConfidence intervals are symmetric: haty_T+hT pm z_alpha2 cdot textse_h.","category":"section"},{"location":"arima/#ARIMA-Forecasting","page":"ARIMA","title":"ARIMA Forecasting","text":"For ARIMA(p,d,q) models, forecasts are computed on the differenced series and then integrated back to the original scale. For d = 1:\n\nhaty_T+h = y_T + sum_j=1^h widehatDelta y_T+jT\n\nStandard errors are adjusted for the integration via cumulative variance accumulation.\n\n# Forecast 12 steps ahead\nfc = forecast(model, 12)\nfc.forecast    # Point forecasts\nfc.ci_lower    # Lower 95% confidence bound\nfc.ci_upper    # Upper 95% confidence bound\nfc.se          # Standard errors\n\n# Forecast with different confidence level\nfc99 = forecast(model, 12; conf_level=0.99)","category":"section"},{"location":"arima/#ARIMAForecast-Return-Values","page":"ARIMA","title":"ARIMAForecast Return Values","text":"Field Type Description\nforecast Vector{T} Point forecasts haty_T+1 ldots haty_T+h\nci_lower Vector{T} Lower confidence bound\nci_upper Vector{T} Upper confidence bound\nse Vector{T} Forecast standard errors (from psi-weights)\nhorizon Int Forecast horizon h\nconf_level T Confidence level (e.g., 0.95)\n\n","category":"section"},{"location":"arima/#Order-Selection","page":"ARIMA","title":"Order Selection","text":"","category":"section"},{"location":"arima/#Grid-Search","page":"ARIMA","title":"Grid Search","text":"select_arima_order evaluates all ARMA(p,q) combinations up to specified maxima and selects the best model by AIC or BIC:\n\n# Search over p ∈ {0,...,4}, q ∈ {0,...,4}\nselection = select_arima_order(y, 4, 4)\nselection.best_p_bic    # Optimal AR order (BIC)\nselection.best_q_bic    # Optimal MA order (BIC)\nselection.best_p_aic    # Optimal AR order (AIC)\nselection.best_q_aic    # Optimal MA order (AIC)","category":"section"},{"location":"arima/#ARIMAOrderSelection-Return-Values","page":"ARIMA","title":"ARIMAOrderSelection Return Values","text":"Field Type Description\nbest_p_aic Int Optimal AR order by AIC\nbest_q_aic Int Optimal MA order by AIC\nbest_p_bic Int Optimal AR order by BIC\nbest_q_bic Int Optimal MA order by BIC\naic_matrix Matrix{T} (p_max+1) times (q_max+1) matrix of AIC values\nbic_matrix Matrix{T} (p_max+1) times (q_max+1) matrix of BIC values\nbest_model_aic AbstractARIMAModel Best model by AIC\nbest_model_bic AbstractARIMAModel Best model by BIC","category":"section"},{"location":"arima/#Automatic-Selection","page":"ARIMA","title":"Automatic Selection","text":"auto_arima implements an automatic model selection procedure:\n\nbest_model = auto_arima(y)\nbest_model.p     # Selected AR order\nbest_model.q     # Selected MA order (for ARMA) or d (for ARIMA)","category":"section"},{"location":"arima/#Information-Criteria-Table","page":"ARIMA","title":"Information Criteria Table","text":"ic_table provides a formatted comparison of models:\n\n# Get IC values for a grid of models\ntable = ic_table(y, 3, 3)\n\n","category":"section"},{"location":"arima/#StatsAPI-Interface","page":"ARIMA","title":"StatsAPI Interface","text":"All ARIMA models implement the Julia StatsAPI.RegressionModel interface:\n\nusing StatsAPI\n\nmodel = estimate_arma(y, 1, 1)\n\n# StatsAPI accessors\ncoef(model)         # Coefficient vector\nnobs(model)         # Number of observations\ndof(model)          # Degrees of freedom (number of parameters)\ndof_residual(model) # Residual degrees of freedom\nloglikelihood(model) # Log-likelihood\naic(model)          # AIC\nbic(model)          # BIC\nresiduals(model)    # Residual vector\nfitted(model)       # Fitted values\n\n# StatsAPI fit interface\nmodel = fit(ARModel, y, 2)           # AR(2)\nmodel = fit(MAModel, y, 1)           # MA(1)\nmodel = fit(ARMAModel, y, 1, 1)      # ARMA(1,1)\nmodel = fit(ARIMAModel, y, 1, 1, 1)  # ARIMA(1,1,1)\n\n# Prediction\nyhat = predict(model, 12)  # 12-step point forecasts\n\n","category":"section"},{"location":"arima/#Complete-Example","page":"ARIMA","title":"Complete Example","text":"using MacroEconometricModels\n\n# Industrial production growth (monthly, FRED-MD)\nfred = load_example(:fred_md)\ny = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))\n\n# Step 1: Check for unit root\nadf_result = adf_test(y; lags=:aic, regression=:constant)\n# IP growth should be stationary — no differencing needed\n\n# Step 2: Select ARMA order\nsel = select_arima_order(y, 4, 4)\nprintln(\"Best order: ARMA($(sel.best_p_bic), $(sel.best_q_bic))\")\n\n# Step 3: Estimate the model\nmodel = estimate_arma(y, sel.best_p_bic, sel.best_q_bic)\nprintln(\"φ = $(model.phi), θ = $(model.theta)\")\nprintln(\"σ² = $(model.sigma2)\")\nprintln(\"AIC = $(model.aic), BIC = $(model.bic)\")\n\n# Step 4: Forecast IP growth 12 months ahead\nfc = forecast(model, 12; conf_level=0.95)\nprintln(\"1-step forecast: $(fc.forecast[1]) ± $(1.96 * fc.se[1])\")\n\n# Step 5: Diagnostics\nprintln(\"Converged: $(model.converged)\")\nprintln(\"Log-likelihood: $(model.loglik)\")\n\n","category":"section"},{"location":"arima/#References","page":"ARIMA","title":"References","text":"Box, George E. P., and Gwilym M. Jenkins. 1976. Time Series Analysis: Forecasting and Control. San Francisco: Holden-Day. ISBN 978-0-816-21104-3.\nBrockwell, Peter J., and Richard A. Davis. 1991. Time Series: Theory and Methods. 2nd ed. New York: Springer. ISBN 978-1-4419-0319-8.\nDurbin, James, and Siem Jan Koopman. 2012. Time Series Analysis by State Space Methods. 2nd ed. Oxford: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199641178.001.0001\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nHarvey, Andrew C. 1993. Time Series Models. 2nd ed. Cambridge, MA: MIT Press. ISBN 978-0-262-08224-2.","category":"section"},{"location":"vecm/#Vector-Error-Correction-Models","page":"VECM","title":"Vector Error Correction Models","text":"This page documents the Vector Error Correction Model (VECM) implementation in MacroEconometricModels.jl, providing estimation via Johansen maximum likelihood and Engle-Granger two-step methods.","category":"section"},{"location":"vecm/#Quick-Start","page":"VECM","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-QD: log GDP, Consumption, Investment (I(1), cointegrated)\nqd = load_example(:fred_qd)\nY = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Estimate VECM with automatic rank selection\nvecm = estimate_vecm(Y, 2)\n\n# Estimate with explicit rank and deterministic specification\nvecm = estimate_vecm(Y, 2; rank=1, deterministic=:constant)\n\n# Convert to VAR for structural analysis\nvar_model = to_var(vecm)\nirfs = irf(vecm, 20)  # IRFs via automatic VAR conversion\n\n","category":"section"},{"location":"vecm/#Model-Specification","page":"VECM","title":"Model Specification","text":"","category":"section"},{"location":"vecm/#From-VAR-to-VECM","page":"VECM","title":"From VAR to VECM","text":"Consider a VAR(p) model in levels for an n-dimensional I(1) vector y_t:\n\ny_t = c + A_1 y_t-1 + A_2 y_t-2 + cdots + A_p y_t-p + u_t\n\nWhen the variables are cointegrated, the Granger representation theorem (Engle & Granger, 1987) implies the system can be written in Vector Error Correction form:\n\nDelta y_t = alpha beta y_t-1 + Gamma_1 Delta y_t-1 + cdots + Gamma_p-1 Delta y_t-p+1 + mu + u_t\n\nwhere:\n\nPi = alpha beta is the long-run matrix (n times n, rank r)\nalpha is the n times r matrix of adjustment coefficients (loading matrix)\nbeta is the n times r matrix of cointegrating vectors\nGamma_i = -(A_i+1 + cdots + A_p) are the short-run dynamics matrices\nmu is the n times 1 intercept vector\nu_t sim N(0 Sigma) are i.i.d. innovations\n\nThe cointegrating rank r determines the number of long-run equilibrium relationships. When r = 0, there is no cointegration and the system reduces to a VAR in first differences. When r = n, the system is stationary in levels.","category":"section"},{"location":"vecm/#The-Cointegrating-Relationship","page":"VECM","title":"The Cointegrating Relationship","text":"Each column beta_j of beta defines a stationary linear combination:\n\nz_jt = beta_j y_t quad sim I(0) quad j = 1 ldots r\n\nThe corresponding column alpha_j of alpha governs the speed of adjustment: alpha_ij measures how quickly variable i responds to deviations from the j-th equilibrium.\n\nnote: Phillips Normalization\nThe package applies Phillips normalization to beta so that the first r rows form an identity matrix. This ensures unique identification of the cointegrating vectors.\n\n","category":"section"},{"location":"vecm/#Estimation","page":"VECM","title":"Estimation","text":"","category":"section"},{"location":"vecm/#Johansen-Maximum-Likelihood-(Default)","page":"VECM","title":"Johansen Maximum Likelihood (Default)","text":"The Johansen (1991) reduced-rank regression procedure estimates alpha and beta jointly via MLE:\n\nConcentrate out short-run dynamics by regressing Delta Y and Y_t-1 on lagged differences Z = Delta Y_t-1 ldots Delta Y_t-p+1 mu\nCompute moment matrices S_00, S_11, S_01 from the concentrated residuals\nSolve the generalized eigenvalue problem lambda S_11 - S_10 S_00^-1 S_01 = 0\nExtract beta from the first r eigenvectors and compute alpha = S_01 beta (beta S_11 beta)^-1\n\nusing MacroEconometricModels\n\n# Load FRED-QD: log GDP, Consumption, Investment\nqd = load_example(:fred_qd)\nY = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Automatic rank selection via Johansen trace test\nvecm = estimate_vecm(Y, 2)\n\n# Explicit rank specification\nvecm = estimate_vecm(Y, 2; rank=1)\n\n# Different deterministic specifications\nvecm = estimate_vecm(Y, 2; rank=1, deterministic=:none)     # No deterministic terms\nvecm = estimate_vecm(Y, 2; rank=1, deterministic=:constant)  # Constant (default)\nvecm = estimate_vecm(Y, 2; rank=1, deterministic=:trend)     # Linear trend\n\nThe rank can be selected automatically using the Johansen trace test, or specified explicitly. Use select_vecm_rank for fine-grained control:\n\nr = select_vecm_rank(Y, 2; criterion=:trace, significance=0.05)\nr_max = select_vecm_rank(Y, 2; criterion=:max_eigen)","category":"section"},{"location":"vecm/#Engle-Granger-Two-Step","page":"VECM","title":"Engle-Granger Two-Step","text":"For bivariate systems with a single cointegrating relationship (r = 1), the Engle-Granger (1987) two-step estimator is available:\n\nStep 1: Estimate the cointegrating vector via static OLS regression of y_1t on y_2t ldots y_nt\nStep 2: Estimate the VECM equation using the OLS residuals as the error correction term\n\nvecm_eg = estimate_vecm(Y, 2; method=:engle_granger, rank=1)\n\nwarning: Warning\nThe Engle-Granger method only supports rank=1. For systems with multiple cointegrating vectors, use the Johansen method.","category":"section"},{"location":"vecm/#Return-Values","page":"VECM","title":"Return Values","text":"estimate_vecm returns a VECMModel{T} with the following fields:\n\nField Type Description\nY Matrix{T} Original data in levels (T_obs x n)\np Int Underlying VAR order\nrank Int Cointegrating rank r\nalpha Matrix{T} Adjustment coefficients (n x r)\nbeta Matrix{T} Cointegrating vectors (n x r)\nPi Matrix{T} Long-run matrix (n x n)\nGamma Vector{Matrix{T}} Short-run dynamics matrices\nmu Vector{T} Intercept\nU Matrix{T} Residuals\nSigma Matrix{T} Residual covariance\naic, bic, hqic T Information criteria\nloglik T Log-likelihood\ndeterministic Symbol Deterministic specification\nmethod Symbol Estimation method\njohansen_result JohansenResult{T} Johansen test result\n\n","category":"section"},{"location":"vecm/#VAR-Conversion","page":"VECM","title":"VAR Conversion","text":"The to_var function converts a VECM back to a VAR in levels, enabling all structural analysis methods:\n\nA_1 = Pi + I_n + Gamma_1 quad A_i = Gamma_i - Gamma_i-1 quad A_p = -Gamma_p-1\n\nvar_model = to_var(vecm)\n\nThis is critical because it allows all 18+ identification methods (Cholesky, sign restrictions, ICA, etc.) to work automatically with VECM models.\n\n","category":"section"},{"location":"vecm/#Innovation-Accounting","page":"VECM","title":"Innovation Accounting","text":"All structural analysis functions dispatch through to_var(), so VECMModel objects can be passed directly:\n\n# Impulse Response Functions\nirfs = irf(vecm, 20; method=:cholesky)\nirfs = irf(vecm, 20; method=:sign, check_func=f, ci_type=:bootstrap)\n\n# Forecast Error Variance Decomposition\ndecomp = fevd(vecm, 20)\n\n# Historical Decomposition\nT_eff = effective_nobs(to_var(vecm))\nhd = historical_decomposition(vecm, T_eff)\n\n","category":"section"},{"location":"vecm/#Forecasting","page":"VECM","title":"Forecasting","text":"VECM forecasting iterates the VECM equations directly in levels, preserving the cointegrating relationships in the forecast path. This is preferable to forecasting from the converted VAR, as it ensures the error correction mechanism operates during the forecast.\n\n# Point forecast\nfc = forecast(vecm, 10)\nfc.levels       # h x n forecast in levels\nfc.differences  # h x n forecast in first differences\n\n# With bootstrap confidence intervals\nfc = forecast(vecm, 10; ci_method=:bootstrap, reps=500, conf_level=0.95)\n\n# With simulation-based CIs\nfc = forecast(vecm, 10; ci_method=:simulation, reps=500)\n\nThe VECMForecast{T} struct contains:\n\nField Description\nlevels Forecasts in levels (h x n)\ndifferences Forecasts in first differences (h x n)\nci_lower, ci_upper Confidence interval bounds (h x n)\nhorizon Forecast horizon\nci_method Method used for CIs\n\n","category":"section"},{"location":"vecm/#Granger-Causality","page":"VECM","title":"Granger Causality","text":"VECM Granger causality tests decompose causal channels into short-run and long-run components:\n\ng = granger_causality_vecm(vecm, 1, 2)  # Test: Var 1 → Var 2\n\nThree tests are computed:\n\nTest Hypothesis Mechanism\nShort-run Gamma_itexteffect textcause = 0 for all i Causality through lagged differences\nLong-run alphatexteffect  = 0 Causality through error correction\nStrong Joint test of both Combined short-run and long-run causality\n\nEach test reports a Wald chi^2 statistic, degrees of freedom, and p-value.\n\n","category":"section"},{"location":"vecm/#Complete-Example","page":"VECM","title":"Complete Example","text":"using MacroEconometricModels\n\n# Load FRED-QD: log GDP, Consumption, Investment (quarterly, I(1))\nqd = load_example(:fred_qd)\nY = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Step 1: Test for cointegration\njoh = johansen_test(Y, 2)\nprintln(\"Johansen rank: \", joh.rank)\n\n# Step 2: Estimate VECM\nvecm = estimate_vecm(Y, 2)\nreport(vecm)\n\n# Step 3: Examine cointegrating vectors\nprintln(\"β (cointegrating vectors):\")\nprintln(vecm.beta)\nprintln(\"α (adjustment speeds):\")\nprintln(vecm.alpha)\n\n# Step 4: Impulse responses\nirfs = irf(vecm, 20; method=:cholesky)\n\n# Step 5: Forecast\nfc = forecast(vecm, 10; ci_method=:bootstrap, reps=200)\n\n# Step 6: Granger causality — does GDP Granger-cause Consumption?\nnames = [\"GDP\", \"Consumption\", \"Investment\"]\nfor i in 1:3, j in 1:3\n    i == j && continue\n    g = granger_causality_vecm(vecm, i, j)\n    println(\"$(names[i]) → $(names[j]): p=$(round(g.strong_pvalue, digits=4))\")\nend\n\n# Step 7: Convert to VAR for further analysis\nvar_model = to_var(vecm)\ndecomp = fevd(var_model, 20)\n\nInterpretation. The cointegrating vector beta identifies the long-run equilibrium between GDP, consumption, and investment. A cointegrating relationship of the form beta y_t sim I(0) implies these variables share common stochastic trends, consistent with balanced growth path theory. The adjustment coefficients alpha show how each variable responds when the system deviates from this long-run equilibrium — for instance, if consumption overshoots relative to GDP, a negative alpha for consumption would pull it back toward the equilibrium ratio. The Granger causality test decomposes into short-run (through lagged differences Gamma) and long-run (through the error correction term alphabetay_t-1) channels.\n\n","category":"section"},{"location":"vecm/#See-Also","page":"VECM","title":"See Also","text":"VAR Estimation – Reduced-form VAR and structural identification\nHypothesis Tests – Johansen cointegration test details and unit root tests\nData Management – Built-in datasets and data transformations\nAPI Reference – Complete function signatures","category":"section"},{"location":"vecm/#References","page":"VECM","title":"References","text":"Johansen, Soren. 1991. \"Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models.\" Econometrica 59 (6): 1551–1580. https://doi.org/10.2307/2938278\nEngle, Robert F., and Clive W. J. Granger. 1987. \"Co-Integration and Error Correction: Representation, Estimation, and Testing.\" Econometrica 55 (2): 251–276. https://doi.org/10.2307/1913236\nLutkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.","category":"section"},{"location":"plotting/#Visualization","page":"Visualization","title":"Visualization","text":"MacroEconometricModels.jl includes a zero-dependency visualization system that renders interactive HTML/SVG charts using inline D3.js v7. No additional packages are required — D3.js is loaded from CDN at runtime.\n\nThe unified plot_result() function dispatches on 31 result types, producing self-contained HTML documents with interactive tooltips that work in browsers and Jupyter notebooks.","category":"section"},{"location":"plotting/#Quick-Start","page":"Visualization","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD and prepare stationary 3-variable monetary system\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Estimate a VAR and plot IRFs\nm = estimate_var(Y, 4)\nr = irf(m, 20; ci_type=:bootstrap, reps=500)\np = plot_result(r)\n\n# Save to file\nsave_plot(p, \"irf_plot.html\")\n\n# Open in browser\ndisplay_plot(p)\n\n<iframe src=\"../assets/plots/quickstart_irf.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Output-Type","page":"Visualization","title":"Output Type","text":"All plot_result() methods return a PlotOutput struct:\n\nstruct PlotOutput\n    html::String  # Complete self-contained HTML document\nend","category":"section"},{"location":"plotting/#Displaying-Results","page":"Visualization","title":"Displaying Results","text":"Context How it works\nJupyter/IJulia Automatic inline display via MIME\"text/html\"\nREPL display_plot(p) opens in default browser\nFile save_plot(p, \"path.html\") writes HTML to disk\nProgrammatic Access p.html directly\n\n# Save to file\nsave_plot(p, \"my_plot.html\")\n\n# Open in default browser\ndisplay_plot(p)\n\n# Access raw HTML\nprintln(length(p.html), \" bytes\")\n\n","category":"section"},{"location":"plotting/#Impulse-Response-Functions","page":"Visualization","title":"Impulse Response Functions","text":"","category":"section"},{"location":"plotting/#Frequentist-IRF","page":"Visualization","title":"Frequentist IRF","text":"# Stationary 3-variable system from FRED-MD\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nm = estimate_var(Y, 4)\nr = irf(m, 20; ci_type=:bootstrap, reps=500)\n\n# Full n_vars x n_shocks grid\np = plot_result(r)\n\n# Single response variable and shock\np = plot_result(r; var=1, shock=1)\n\n# Custom title\np = plot_result(r; title=\"Monetary Policy Shock\")\n\n<iframe src=\"../assets/plots/irf_freq.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nkwargs: var (Int/String), shock (Int/String), ncols (Int), title (String), save_path (String)","category":"section"},{"location":"plotting/#Bayesian-IRF","page":"Visualization","title":"Bayesian IRF","text":"post = estimate_bvar(Y, 4; n_draws=1000, varnames=[\"INDPRO\", \"UNRATE\", \"CPI\"])\nr = irf(post, 20)\np = plot_result(r)                    # Full grid\np = plot_result(r; var=1, shock=1)    # Single panel\n\n<iframe src=\"../assets/plots/irf_bayesian.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nDisplays posterior median with credible band from the widest quantile interval.","category":"section"},{"location":"plotting/#Local-Projection-IRF","page":"Visualization","title":"Local Projection IRF","text":"lp_m = estimate_lp(Y, 1, 20; lags=4)\nr = lp_irf(lp_m)\np = plot_result(r)          # All response variables\np = plot_result(r; var=1)   # Single variable\n\n<iframe src=\"../assets/plots/irf_lp.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Structural-LP","page":"Visualization","title":"Structural LP","text":"slp = structural_lp(Y, 20; method=:cholesky, lags=4)\np = plot_result(slp)\n\n<iframe src=\"../assets/plots/irf_structural_lp.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Forecast-Error-Variance-Decomposition","page":"Visualization","title":"Forecast Error Variance Decomposition","text":"","category":"section"},{"location":"plotting/#Frequentist-FEVD","page":"Visualization","title":"Frequentist FEVD","text":"f = fevd(m, 20)\np = plot_result(f)          # All variables (stacked area)\np = plot_result(f; var=1)   # Single variable\n\n<iframe src=\"../assets/plots/fevd_freq.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nRendered as stacked area charts with proportions summing to 1.0.","category":"section"},{"location":"plotting/#Bayesian-FEVD","page":"Visualization","title":"Bayesian FEVD","text":"f = fevd(post, 20)\np = plot_result(f)\np = plot_result(f; stat=:mean)  # Default: posterior mean\n\n<iframe src=\"../assets/plots/fevd_bayesian.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#LP-FEVD","page":"Visualization","title":"LP-FEVD","text":"f = lp_fevd(slp, 20)\np = plot_result(f)\np = plot_result(f; bias_corrected=true)  # Default\n\n<iframe src=\"../assets/plots/fevd_lp.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Historical-Decomposition","page":"Visualization","title":"Historical Decomposition","text":"","category":"section"},{"location":"plotting/#Frequentist-HD","page":"Visualization","title":"Frequentist HD","text":"hd = historical_decomposition(m)\np = plot_result(hd)          # All variables\np = plot_result(hd; var=1)   # Single variable\n\n<iframe src=\"../assets/plots/hd_freq.html\" width=\"100%\" height=\"600\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nEach variable produces two panels: stacked bar chart of shock contributions and a line chart comparing actual values with the sum of contributions.","category":"section"},{"location":"plotting/#Bayesian-HD","page":"Visualization","title":"Bayesian HD","text":"hd = historical_decomposition(post)\np = plot_result(hd)\n\n<iframe src=\"../assets/plots/hd_bayesian.html\" width=\"100%\" height=\"600\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Time-Series-Filters","page":"Visualization","title":"Time Series Filters","text":"All five filter types produce a two-panel figure: trend vs. original series and the extracted cycle component.","category":"section"},{"location":"plotting/#Hodrick-Prescott","page":"Visualization","title":"Hodrick-Prescott","text":"# Log industrial production from FRED-MD (monthly, I(1))\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\np = plot_result(hp_filter(y))\n\n<iframe src=\"../assets/plots/filter_hp.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Hamilton-(2018)","page":"Visualization","title":"Hamilton (2018)","text":"p = plot_result(hamilton_filter(y); original=y)\n\n<iframe src=\"../assets/plots/filter_hamilton.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Beveridge-Nelson","page":"Visualization","title":"Beveridge-Nelson","text":"p = plot_result(beveridge_nelson(y))\n\n<iframe src=\"../assets/plots/filter_bn.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Baxter-King","page":"Visualization","title":"Baxter-King","text":"p = plot_result(baxter_king(y); original=y)\n\n<iframe src=\"../assets/plots/filter_bk.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Boosted-HP-(Phillips-and-Shi-2021)","page":"Visualization","title":"Boosted HP (Phillips & Shi 2021)","text":"p = plot_result(boosted_hp(y))\n\n<iframe src=\"../assets/plots/filter_boosted_hp.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nnote: Original series\nHamilton and Baxter-King filters produce shorter output than the input. Pass original=y to overlay the full original series in the plot.\n\n","category":"section"},{"location":"plotting/#Forecasts","page":"Visualization","title":"Forecasts","text":"","category":"section"},{"location":"plotting/#ARIMA-Forecast","page":"Visualization","title":"ARIMA Forecast","text":"# IP growth rate (log first difference) from FRED-MD\nfred = load_example(:fred_md)\ny1 = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))\n\nar = estimate_ar(y1, 2)\nfc = forecast(ar, 20)\n\n# Forecast only\np = plot_result(fc)\n\n# With recent history\np = plot_result(fc; history=y1, n_history=30)\n\n<iframe src=\"../assets/plots/forecast_arima.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Volatility-Forecast","page":"Visualization","title":"Volatility Forecast","text":"# S&P 500 returns from FRED-MD (falls back to INDPRO growth)\nsp_idx = findfirst(v -> occursin(\"S&P\", v) && occursin(\"500\", v), varnames(fred))\ny_vol = sp_idx !== nothing ?\n    filter(isfinite, apply_tcode(fred[:, varnames(fred)[sp_idx]], 5)) : y1\n\ngm = estimate_garch(y_vol, 1, 1)\nfc = forecast(gm, 10)\np = plot_result(fc)\np = plot_result(fc; history=gm.conditional_variance)\n\n<iframe src=\"../assets/plots/forecast_volatility.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#VECM-Forecast","page":"Visualization","title":"VECM Forecast","text":"# Cointegrated quarterly I(1) system from FRED-QD\nqd = load_example(:fred_qd)\nY_ci = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY_ci = Y_ci[all.(isfinite, eachrow(Y_ci)), :]\n\nvecm = estimate_vecm(Y_ci, 2; rank=1)\nfc = forecast(vecm, 10)\np = plot_result(fc)          # All variables\np = plot_result(fc; var=1)   # Single variable\n\n<iframe src=\"../assets/plots/forecast_vecm.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Factor-Forecast","page":"Visualization","title":"Factor Forecast","text":"# Large panel from FRED-MD (safe variables only, first 20 columns)\nfred = load_example(:fred_md)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\nfm = estimate_dynamic_factors(X, 2, 1)\nfc = forecast(fm, 10)\n\np = plot_result(fc)                          # Factor forecasts\np = plot_result(fc; type=:observable, var=1) # Observable forecast\n\n<iframe src=\"../assets/plots/forecast_factor.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#LP-Forecast","page":"Visualization","title":"LP Forecast","text":"# Use last 100 rows of the 3-variable FRED-MD data\nY_lp = Y[end-99:end, :]\nlp_fc = estimate_lp(Y_lp, 1, 10; lags=4)\nshock_path = zeros(10); shock_path[1] = 1.0\nfc = forecast(lp_fc, shock_path)\np = plot_result(fc)\n\n<iframe src=\"../assets/plots/forecast_lp.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Volatility-Models","page":"Visualization","title":"Volatility Models","text":"ARCH, GARCH, EGARCH, and GJR-GARCH models produce a three-panel diagnostic figure: return series, conditional volatility, and standardized residuals with +/-2 standard deviation bounds.\n\n# S&P 500 returns (or INDPRO growth as fallback)\np = plot_result(estimate_arch(y_vol, 2))\np = plot_result(estimate_garch(y_vol, 1, 1))\np = plot_result(estimate_egarch(y_vol, 1, 1))\np = plot_result(estimate_gjr_garch(y_vol, 1, 1))\n\n<iframe src=\"../assets/plots/model_garch.html\" width=\"100%\" height=\"700\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#Stochastic-Volatility","page":"Visualization","title":"Stochastic Volatility","text":"The SV model shows posterior volatility with quantile credible bands:\n\nm = estimate_sv(y_vol; n_samples=2000, burnin=1000)\np = plot_result(m)\n\n<iframe src=\"../assets/plots/model_sv.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Factor-Models","page":"Visualization","title":"Factor Models","text":"Factor models display a scree plot of eigenvalues and the extracted factor time series:\n\n# Static factors (reusing X from factor forecast above)\np = plot_result(estimate_factors(X, 3))\n\n# Dynamic factors\np = plot_result(estimate_dynamic_factors(X, 2, 1))\n\n<iframe src=\"../assets/plots/model_factor_static.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\n","category":"section"},{"location":"plotting/#Data-Containers","page":"Visualization","title":"Data Containers","text":"","category":"section"},{"location":"plotting/#TimeSeriesData","page":"Visualization","title":"TimeSeriesData","text":"# Plot raw FRED-MD series\nfred = load_example(:fred_md)\nd = fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\"]]\np = plot_result(d)                               # All variables\np = plot_result(d; vars=[\"INDPRO\", \"CPIAUCSL\"])  # Subset\n\n<iframe src=\"../assets/plots/data_timeseries.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>","category":"section"},{"location":"plotting/#PanelData","page":"Visualization","title":"PanelData","text":"# Penn World Table: real GDP, population, employment, human capital\npwt = load_example(:pwt)\np = plot_result(pwt; vars=[\"rgdpna\", \"pop\", \"emp\", \"hc\"])\n\n<iframe src=\"../assets/plots/data_panel.html\" width=\"100%\" height=\"500\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nPanel data plots show each variable in a separate panel with one line per group.\n\n","category":"section"},{"location":"plotting/#Nowcasting","page":"Visualization","title":"Nowcasting","text":"","category":"section"},{"location":"plotting/#Nowcast-Result","page":"Visualization","title":"Nowcast Result","text":"# Mixed-frequency panel from FRED-MD\nfred = load_example(:fred_md)\nnc_sub = fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\", \"M2SL\", \"FEDFUNDS\"]]\nY_nc = to_matrix(apply_tcode(nc_sub))\nY_nc = Y_nc[all.(isfinite, eachrow(Y_nc)), :]\nY_nc = Y_nc[end-99:end, :]\nY_nc[end, end] = NaN  # Simulate missing observation\n\ndfm_nc = nowcast_dfm(Y_nc, 4, 1; r=2, p=1)\nnr = nowcast(dfm_nc)\np = plot_result(nr)\n\n<iframe src=\"../assets/plots/nowcast_result.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nDisplays smoothed target variable with nowcast and forecast values annotated in the title.","category":"section"},{"location":"plotting/#Nowcast-News","page":"Visualization","title":"Nowcast News","text":"X_old = copy(Y_nc)\nX_new = copy(X_old); X_new[end, end] = X_old[end-1, end]  # Fill with previous value\ndfm_news = nowcast_dfm(X_old, 4, 1; r=2, p=1)\nnn = nowcast_news(X_new, X_old, dfm_news, 5)\np = plot_result(nn)\n\n<iframe src=\"../assets/plots/nowcast_news.html\" width=\"100%\" height=\"400\" frameborder=\"0\" style=\"border:1px solid #ddd;border-radius:4px;\"></iframe>\n\nBar chart showing per-release impact on the nowcast revision.\n\n","category":"section"},{"location":"plotting/#Common-Options","page":"Visualization","title":"Common Options","text":"All plot_result() methods accept these keyword arguments:\n\nKwarg Type Default Description\ntitle String \"\" Override auto-generated title\nsave_path String or nothing nothing Auto-save HTML to path\nncols Int 0 (auto) Number of columns in multi-panel grid\n\nType-specific kwargs (e.g., var, shock, history, stat, bias_corrected) are documented for each method above.\n\n","category":"section"},{"location":"plotting/#Chart-Types","page":"Visualization","title":"Chart Types","text":"Three D3.js chart types cover all use cases:\n\nChart Used for Features\nLine IRF, forecasts, filters, volatility, data series CI bands, dashed lines, zero reference line, tooltips\nStacked area FEVD Proportions summing to 1.0, per-shock coloring\nBar Historical decomposition, nowcast news Stacked or grouped, diverging stack for negative values\n\nAll charts include interactive tooltips and a consistent color palette.","category":"section"},{"location":"plotting/#References","page":"Visualization","title":"References","text":"Bostock, M., Ogievetsky, V., & Heer, J. (2011). D3: Data-Driven Documents. IEEE Transactions on Visualization and Computer Graphics, 17(12), 2301–2309. DOI: 10.1109/TVCG.2011.185","category":"section"},{"location":"lp/#Local-Projections","page":"Local Projections","title":"Local Projections","text":"This chapter provides a comprehensive treatment of Local Projection (LP) methods for estimating impulse response functions, an alternative to the VAR-based approach that offers greater robustness and flexibility.","category":"section"},{"location":"lp/#Introduction","page":"Local Projections","title":"Introduction","text":"Local Projections, introduced by Jordà (2005), estimate impulse responses by running a series of predictive regressions at each forecast horizon. Unlike VARs, which derive IRFs from a single estimated dynamic system, LPs directly estimate the response at each horizon without imposing the dynamic restrictions inherent in VAR specifications.","category":"section"},{"location":"lp/#Key-Advantages-of-Local-Projections","page":"Local Projections","title":"Key Advantages of Local Projections","text":"Robustness to Misspecification: LPs do not impose the lag structure of VARs, making them robust to dynamic misspecification\nFlexibility: Easy to incorporate nonlinearities, state-dependence, and instrumental variables\nTransparency: Each horizon's estimate is independent, making the source of identification transparent\nInference: Standard regression-based inference applies (with HAC corrections)\n\nReference: Jordà (2005), Plagborg-Møller & Wolf (2021)","category":"section"},{"location":"lp/#Quick-Start","page":"Local Projections","title":"Quick Start","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset: industrial production, CPI, federal funds rate\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nlp = estimate_lp(Y, 3, 20; lags=4, cov_type=:newey_west)              # Standard LP\nZ = reshape([zeros(1); diff(Y[:, 3])], :, 1)                           # Instrument: lagged FFR changes\nlpiv = estimate_lp_iv(Y, 3, Z, 20; lags=4)                            # LP-IV\nslp = estimate_smooth_lp(Y, 3, 20; lambda=1.0, n_knots=4)             # Smooth LP\nirf_result = lp_irf(lp; conf_level=0.95)                              # Extract IRF\nstruc = structural_lp(Y, 20; method=:cholesky)                         # Structural LP\nfc = forecast(lp, ones(20); ci_method=:analytical)                     # LP forecast\nlfevd = lp_fevd(struc, 20; method=:r2, bias_correct=true)             # LP-FEVD\n\n","category":"section"},{"location":"lp/#Standard-Local-Projections","page":"Local Projections","title":"Standard Local Projections","text":"","category":"section"},{"location":"lp/#The-LP-Regression","page":"Local Projections","title":"The LP Regression","text":"For each horizon h = 0 1 ldots H, we estimate:\n\ny_it+h = alpha_ih + beta_ih x_t + gamma_ih w_t + varepsilon_it+h\n\nwhere:\n\ny_it+h is the response variable i at time t+h\nx_t is the shock/treatment variable at time t\nw_t is a vector of controls (typically lagged y and x)\nbeta_ih is the impulse response of variable i to shock x at horizon h","category":"section"},{"location":"lp/#Control-Variables","page":"Local Projections","title":"Control Variables","text":"Standard controls include lags of all endogenous variables:\n\nw_t = (y_t-1 y_t-2 ldots y_t-p x_t-1 ldots x_t-p)\n\nThe number of lags p is typically selected using information criteria or set to match the VAR lag order.","category":"section"},{"location":"lp/#Estimation","page":"Local Projections","title":"Estimation","text":"At each horizon h, OLS yields:\n\nhatbeta_h = (XX)^-1 XY_h\n\nwhere\n\nhatbeta_h is the k times 1 OLS coefficient vector at horizon h\nX is the T_eff times k regressor matrix containing the intercept, shock variable, and controls\nY_h is the T_eff times 1 vector of responses at horizon h\nk = 2 + np (intercept + shock + p lags of n variables)","category":"section"},{"location":"lp/#HAC-Standard-Errors","page":"Local Projections","title":"HAC Standard Errors","text":"Since varepsilon_t+h is serially correlated (at least MA(h-1) under the null), we use Newey-West standard errors:\n\nhatV_NW = (XX)^-1 hatS (XX)^-1\n\nwhere\n\nhatV_NW is the HAC variance-covariance matrix of hatbeta_h\nhatS = hatGamma_0 + sum_j=1^m w_j (hatGamma_j + hatGamma_j) is the long-run covariance\nw_j are Bartlett kernel weights, m is the bandwidth (typically h + 1)\n\nReference: Jordà (2005), Newey & West (1987)","category":"section"},{"location":"lp/#Julia-Implementation","page":"Local Projections","title":"Julia Implementation","text":"note: Technical Note\nLP residuals varepsilon_t+h are serially correlated at least MA(h-1) under the null of correct specification, even when the true DGP has i.i.d. errors. This is because overlapping forecast horizons create mechanical dependence. HAC standard errors (Newey-West) are therefore essential for all horizons h  0. When using automatic bandwidth selection (bandwidth=0, the default), the effective bandwidth at each horizon h is max(m̂_NW, h+1) where m̂_NW is the Newey-West (1994) data-driven selection. This ensures the bandwidth always accounts for the MA(h-1) structure.\n\nusing MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Estimate LP-IRF of a federal funds rate shock up to horizon 20\nlp_model = estimate_lp(Y, 3, 20;       # shock_var=3 (FEDFUNDS)\n    lags = 4,                           # Control lags\n    cov_type = :newey_west,             # HAC standard errors\n    bandwidth = 0                       # 0 = automatic bandwidth\n)\n\n# Extract IRF with confidence intervals\nirf_result = lp_irf(lp_model; conf_level = 0.95)\n\nThe irf_result.values matrix has dimension (H+1) times n_resp, where each row gives the response at a particular horizon. At h = 0, the coefficient hatbeta_0 captures the contemporaneous (impact) effect of a one-unit innovation in the federal funds rate on industrial production and CPI. The standard errors in irf_result.se widen as h increases because longer-horizon LP residuals exhibit stronger serial correlation, and the effective sample shrinks by one observation per horizon.","category":"section"},{"location":"lp/#LPModel-Return-Values","page":"Local Projections","title":"LPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Index of the shock variable\nresponse_vars Vector{Int} Indices of response variables\nhorizon Int Maximum horizon H\nlags Int Number of control lags\nB Vector{Matrix{T}} Coefficient matrices (one per horizon)\nresiduals Vector{Matrix{T}} Residuals at each horizon\nvcov Vector{Matrix{T}} Variance-covariance matrices (HAC)\nT_eff Vector{Int} Effective sample size at each horizon\ncov_estimator AbstractCovarianceEstimator Covariance estimator used","category":"section"},{"location":"lp/#LPImpulseResponse-Return-Values","page":"Local Projections","title":"LPImpulseResponse Return Values","text":"Field Type Description\nvalues Matrix{T} (H+1) times n_resp IRF point estimates\nci_lower Matrix{T} Lower confidence bounds\nci_upper Matrix{T} Upper confidence bounds\nse Matrix{T} Standard errors at each horizon\nhorizon Int Maximum horizon\nresponse_vars Vector{String} Response variable names\nshock_var String Shock variable name\ncov_type Symbol Covariance estimator type\nconf_level T Confidence level (e.g., 0.95)\n\n","category":"section"},{"location":"lp/#Local-Projections-with-Instrumental-Variables-(LP-IV)","page":"Local Projections","title":"Local Projections with Instrumental Variables (LP-IV)","text":"","category":"section"},{"location":"lp/#Motivation","page":"Local Projections","title":"Motivation","text":"When the shock variable x_t is endogenous or measured with error, we need external instruments for identification. Stock & Watson (2018) develop the LP-IV methodology for using external instruments in a local projection framework.","category":"section"},{"location":"lp/#The-LP-IV-Model","page":"Local Projections","title":"The LP-IV Model","text":"We use two-stage least squares (2SLS) at each horizon:\n\nFirst Stage: Regress the endogenous shock on instruments and controls:\n\nx_t = pi_0 + pi_1 z_t + pi_2 w_t + v_t\n\nSecond Stage: Use fitted values in the LP regression:\n\ny_it+h = alpha_ih + beta_ih hatx_t + gamma_ih w_t + varepsilon_it+h\n\nwhere z_t is the vector of external instruments.","category":"section"},{"location":"lp/#Identification-Assumptions","page":"Local Projections","title":"Identification Assumptions","text":"Relevance: Ez_t x_t neq 0 (instruments predict the shock)\nExogeneity: Ez_t varepsilon_t+h = 0 (instruments are uncorrelated with structural errors)","category":"section"},{"location":"lp/#First-Stage-F-Statistic","page":"Local Projections","title":"First-Stage F-Statistic","text":"The first-stage F-statistic tests instrument relevance:\n\nF = frac(hatpi_1 hatV_pi^-1 hatpi_1)q\n\nwhere\n\nhatpi_1 is the vector of first-stage coefficients on the instruments\nhatV_pi is the estimated variance-covariance of hatpi_1\nq is the number of instruments\n\nA rule of thumb is F  10 for strong instruments (Stock & Yogo, 2005).","category":"section"},{"location":"lp/#Weak-Instrument-Robust-Inference","page":"Local Projections","title":"Weak Instrument Robust Inference","text":"When instruments are weak, standard 2SLS inference is unreliable. Options include:\n\nAnderson-Rubin confidence sets\nConditional likelihood ratio tests\nWeak-instrument robust standard errors\n\nReference: Stock & Watson (2018), Stock & Yogo (2005)","category":"section"},{"location":"lp/#Julia-Implementation-2","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Construct instrument: lagged changes in the federal funds rate\nZ = reshape([zeros(1); diff(Y[:, 3])], :, 1)\n\n# LP-IV: instrument FFR with its own lagged changes\nlpiv_model = estimate_lp_iv(Y, 3, Z, 20;    # shock_var=3 (FEDFUNDS)\n    lags = 4,\n    cov_type = :newey_west\n)\n\n# Check first-stage strength\nweak_test = weak_instrument_test(lpiv_model; threshold = 10.0)\nprintln(\"Minimum F-statistic: \", weak_test.min_F)\nprintln(\"All horizons pass: \", weak_test.passes_threshold)\n\n# Extract IRF\nirf_iv = lp_iv_irf(lpiv_model)\n\nThe weak_test.min_F reports the minimum first-stage F-statistic across all horizons. If it exceeds the Stock & Yogo (2005) threshold of 10, the lagged FFR changes are a strong instrument at every horizon. First-stage strength typically declines at longer horizons because the instrument's predictive power for the endogenous shock weakens. If weak_test.passes_threshold is false, the IV estimates at affected horizons should be interpreted cautiously — consider Anderson-Rubin confidence sets for robust inference.","category":"section"},{"location":"lp/#LPIVModel-Return-Values","page":"Local Projections","title":"LPIVModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Index of the endogenous shock variable\nresponse_vars Vector{Int} Response variable indices\ninstruments Matrix{T} External instrument matrix\nhorizon Int Maximum horizon\nlags Int Number of control lags\nB Vector{Matrix{T}} Second-stage coefficient matrices\nresiduals Vector{Matrix{T}} Residuals at each horizon\nvcov Vector{Matrix{T}} Variance-covariance matrices\nfirst_stage_F Vector{T} First-stage F-statistics by horizon\nfirst_stage_coef Vector{Vector{T}} First-stage instrument coefficients\nT_eff Vector{Int} Effective sample sizes\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#Smooth-Local-Projections","page":"Local Projections","title":"Smooth Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-2","page":"Local Projections","title":"Motivation","text":"Standard LPs can produce noisy, erratic impulse responses because each horizon is estimated independently. Barnichon & Brownlees (2019) propose Smooth Local Projections that parameterize the IRF as a smooth function of the horizon using B-spline basis functions.","category":"section"},{"location":"lp/#B-Spline-Representation","page":"Local Projections","title":"B-Spline Representation","text":"The impulse response is modeled as:\n\nbeta(h) = sum_j=1^J theta_j B_j(h)\n\nwhere B_j(h) are B-spline basis functions and theta_j are spline coefficients.","category":"section"},{"location":"lp/#Cubic-B-Splines","page":"Local Projections","title":"Cubic B-Splines","text":"For degree d = 3 (cubic splines), the basis functions are computed recursively using the Cox-de Boor formula:\n\nB_i0(x) = begincases 1  textif  t_i leq x  t_i+1  0  textotherwise endcases\n\nB_id(x) = fracx - t_it_i+d - t_i B_id-1(x) + fract_i+d+1 - xt_i+d+1 - t_i+1 B_i+1d-1(x)","category":"section"},{"location":"lp/#Smoothness-Penalty","page":"Local Projections","title":"Smoothness Penalty","text":"To enforce smoothness, we add a roughness penalty on the second derivative:\n\nmin_theta sum_h=0^H left( hatbeta_h - B(h)theta right)^2 + lambda int left( beta(h) right)^2 dh\n\nwhere\n\nhatbeta_h are the standard LP estimates at horizon h\nB(h) is the J times 1 B-spline basis vector evaluated at h\ntheta is the J times 1 vector of spline coefficients\nlambda geq 0 is the smoothing penalty (lambda = 0 gives unpenalized fit)\n\nThe penalty is computed as theta R theta where:\n\nR_ij = int B_i(x) B_j(x) dx","category":"section"},{"location":"lp/#Two-Step-Estimation","page":"Local Projections","title":"Two-Step Estimation","text":"Estimate standard LP to get hatbeta_h and textVar(hatbeta_h)\nFit weighted penalized spline:\n\nhattheta = left( BWB + lambda R right)^-1 BW hatbeta\n\nwhere\n\nB is the (H+1) times J basis matrix\nW = textdiag(1textVar(hatbeta_h)) is the precision-weight matrix\nR is the J times J roughness penalty matrix\nhatbeta is the (H+1) times 1 vector of standard LP estimates","category":"section"},{"location":"lp/#Cross-Validation-for-λ-Selection","page":"Local Projections","title":"Cross-Validation for λ Selection","text":"The smoothing parameter lambda can be selected by k-fold cross-validation to minimize out-of-sample prediction error.\n\nReference: Barnichon & Brownlees (2019)","category":"section"},{"location":"lp/#Julia-Implementation-3","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Smooth LP with cubic splines — smoothing the FFR shock IRF\nsmooth_model = estimate_smooth_lp(Y, 3, 20;   # shock_var=3 (FEDFUNDS)\n    degree = 3,           # Cubic splines\n    n_knots = 4,          # Interior knots\n    lambda = 1.0,         # Smoothing penalty\n    lags = 4\n)\n\n# Automatic lambda selection via CV\noptimal_lambda = cross_validate_lambda(Y, 3, 20;\n    lambda_grid = 10.0 .^ (-4:0.5:2),\n    k_folds = 5\n)\n\n# Compare smooth vs standard LP\ncomparison = compare_smooth_lp(Y, 3, 20; lambda = optimal_lambda)\nprintln(\"Variance reduction: \", comparison.variance_reduction)\n\nLarger lambda values impose more smoothness, shrinking the IRF toward a low-frequency polynomial. When variance_reduction is positive, the smooth IRF achieves lower pointwise variance at the cost of some bias — a favorable trade-off in moderate samples where standard LP confidence bands are wide. Cross-validation selects the lambda that minimizes out-of-sample prediction error, automatically balancing the bias-variance trade-off.","category":"section"},{"location":"lp/#SmoothLPModel-Return-Values","page":"Local Projections","title":"SmoothLPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Shock variable index\nresponse_vars Vector{Int} Response variable indices\nhorizon Int Maximum horizon\nlags Int Number of control lags\nspline_basis BSplineBasis{T} B-spline basis (knots, degree, basis matrix)\ntheta Matrix{T} Spline coefficients\nvcov_theta Matrix{T} Variance-covariance of spline coefficients\nlambda T Smoothing penalty parameter\nirf_values Matrix{T} Smoothed IRF point estimates\nirf_se Matrix{T} Standard errors of smoothed IRF\nresiduals Matrix{T} Regression residuals\nT_eff Int Effective sample size\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#State-Dependent-Local-Projections","page":"Local Projections","title":"State-Dependent Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-3","page":"Local Projections","title":"Motivation","text":"Economic responses may differ across states of the economy (e.g., recessions vs. expansions). Auerbach & Gorodnichenko (2012, 2013) develop state-dependent LPs using smooth transition functions.","category":"section"},{"location":"lp/#The-State-Dependent-Model","page":"Local Projections","title":"The State-Dependent Model","text":"y_t+h = F(z_t) left alpha_E + beta_E x_t + gamma_E w_t right + (1 - F(z_t)) left alpha_R + beta_R x_t + gamma_R w_t right + varepsilon_t+h\n\nwhere:\n\nF(z_t) is the smooth transition function\nz_t is the state variable (e.g., moving average of GDP growth)\nSubscript E denotes \"expansion\" regime (F to 0)\nSubscript R denotes \"recession\" regime (F to 1)","category":"section"},{"location":"lp/#Logistic-Transition-Function","page":"Local Projections","title":"Logistic Transition Function","text":"The standard specification uses a logistic function:\n\nF(z_t) = fracexp(-gamma(z_t - c))1 + exp(-gamma(z_t - c))\n\nwhere:\n\ngamma  0 controls the transition speed (higher = sharper)\nc is the threshold (often set to 0 for standardized z_t)\n\nProperties:\n\nF(z) to 1 as z to -infty (deep recession)\nF(z) to 0 as z to +infty (strong expansion)\nF(c) = 05 (neutral state)","category":"section"},{"location":"lp/#State-Variable-Construction","page":"Local Projections","title":"State Variable Construction","text":"Following Auerbach & Gorodnichenko, the state variable is typically:\n\nz_t = frac1k sum_j=0^k-1 Delta y_t-j\n\nA k = 7 quarter moving average of GDP growth is common, then standardized to have zero mean and unit variance.","category":"section"},{"location":"lp/#Estimation-2","page":"Local Projections","title":"Estimation","text":"The model is estimated by nonlinear least squares or by treating it as a linear regression in the interaction terms. The parameters (gamma c) can be:\n\nFixed based on prior research\nEstimated via grid search or NLS\nSelected to maximize fit","category":"section"},{"location":"lp/#Testing-for-Regime-Differences","page":"Local Projections","title":"Testing for Regime Differences","text":"Test whether responses differ across regimes:\n\nH_0 beta_E - beta_R = 0\n\nusing a t-test with HAC standard errors:\n\nt = frachatbeta_E - hatbeta_RsqrttextVar(hatbeta_E) + textVar(hatbeta_R) - 2textCov(hatbeta_E hatbeta_R)\n\nReference: Auerbach & Gorodnichenko (2012, 2013), Ramey & Zubairy (2018)","category":"section"},{"location":"lp/#Julia-Implementation-4","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels, Statistics\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Construct state variable: 7-month MA of industrial production growth (recession proxy)\nip_growth = Y[:, 1]  # INDPRO already transformed to growth rate by tcode\nstate_var = [mean(ip_growth[max(1, t-6):t]) for t in 1:length(ip_growth)]\nstate_var = Float64.((state_var .- mean(state_var)) ./ std(state_var))\n\n# Estimate state-dependent LP: FFR shock with IP growth as state\nstate_model = estimate_state_lp(Y, 3, state_var, 20;   # shock_var=3 (FEDFUNDS)\n    gamma = :estimate,      # Estimate transition speed\n    threshold = :estimate,  # Estimate threshold\n    lags = 4\n)\n\n# Extract regime-specific IRFs\nirf_both = state_irf(state_model; regime = :both)\nirf_expansion = state_irf(state_model; regime = :expansion)\nirf_recession = state_irf(state_model; regime = :recession)\n\n# Test for regime differences\ndiff_test = test_regime_difference(state_model)\n\nThe irf_expansion and irf_recession objects contain regime-specific impulse responses. Comparing them reveals whether a monetary policy shock has asymmetric effects across the business cycle — a prediction of many New Keynesian models with binding ZLB or liquidity traps. The test_regime_difference function computes a Wald-type test of H_0 beta_E = beta_R at each horizon using HAC standard errors; rejection implies statistically significant state dependence.","category":"section"},{"location":"lp/#StateLPModel-Return-Values","page":"Local Projections","title":"StateLPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\nshock_var Int Shock variable index\nresponse_vars Vector{Int} Response variable indices\nhorizon Int Maximum horizon\nlags Int Number of control lags\nstate StateTransition{T} State transition function (gamma, threshold, F(z_t) values)\nB_expansion Vector{Matrix{T}} Expansion regime coefficients\nB_recession Vector{Matrix{T}} Recession regime coefficients\nresiduals Vector{Matrix{T}} Residuals at each horizon\nvcov_expansion Vector{Matrix{T}} Expansion regime variance-covariance\nvcov_recession Vector{Matrix{T}} Recession regime variance-covariance\nvcov_diff Vector{Matrix{T}} Variance-covariance of regime difference\nT_eff Vector{Int} Effective sample sizes\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#Propensity-Score-Local-Projections","page":"Local Projections","title":"Propensity Score Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-4","page":"Local Projections","title":"Motivation","text":"When the shock is a discrete treatment (e.g., policy intervention), selection bias may confound causal inference. Angrist, Jordà & Kuersteiner (2018) develop LP with inverse propensity weighting (IPW) to address selection.","category":"section"},{"location":"lp/#The-Setup","page":"Local Projections","title":"The Setup","text":"Let D_t in 0 1 be a binary treatment indicator. We want to estimate the Average Treatment Effect (ATE):\n\ntextATE_h = Ey_t+h(1) - y_t+h(0)\n\nwhere y_t+h(d) is the potential outcome under treatment status d.","category":"section"},{"location":"lp/#Propensity-Score","page":"Local Projections","title":"Propensity Score","text":"The propensity score is the probability of treatment given covariates:\n\np(X_t) = P(D_t = 1  X_t)\n\nestimated via logit or probit:\n\np(X_t) = frac11 + exp(-X_tbeta)","category":"section"},{"location":"lp/#Inverse-Propensity-Weighting-(IPW)","page":"Local Projections","title":"Inverse Propensity Weighting (IPW)","text":"The IPW estimator weights observations by the inverse of their selection probability:\n\nTreated: weight = 1p(X_t)\nControl: weight = 1(1-p(X_t))\n\nThis reweighting creates a pseudo-population where treatment is independent of covariates.","category":"section"},{"location":"lp/#IPW-LP-Estimation-(estimate_propensity_lp)","page":"Local Projections","title":"IPW-LP Estimation (estimate_propensity_lp)","text":"The IPW estimator reweights observations so that, in the pseudo-population, treatment is independent of covariates. Estimation proceeds via weighted least squares (WLS):\n\ny_t+h = alpha_h + beta_h D_t + gamma_h W_t + varepsilon_t+h\n\nwhere W_t includes lagged outcomes and covariates, and the weights are w_t = 1hatp(X_t) for treated and w_t = 1(1-hatp(X_t)) for control observations. The ATE estimate is the treatment coefficient hatbeta_h from the WLS regression. Standard errors are computed from HAC-robust covariance on the weighted residuals.\n\nConsistency requirement: The propensity score model hatp(X_t) must be correctly specified. If the logit/probit model misses important nonlinearities or interactions in the true treatment assignment mechanism, the IPW estimator will be biased.","category":"section"},{"location":"lp/#Doubly-Robust-LP-Estimation-(doubly_robust_lp)","page":"Local Projections","title":"Doubly Robust LP Estimation (doubly_robust_lp)","text":"The doubly robust (DR) estimator, also known as the augmented IPW (AIPW) estimator, combines inverse propensity weighting with separate outcome regressions for treated and control groups. It first fits:\n\nhatmu_1(X_t) = Ey_t+h  D_t=1 X_t — predicted outcome under treatment\nhatmu_0(X_t) = Ey_t+h  D_t=0 X_t — predicted outcome under control\n\nvia OLS on the respective subsamples. Then the ATE is computed from the influence function:\n\nhatpsi_t = begincases displaystylefracD_t(y_t+h - hatmu_1(X_t))hatp(X_t) + hatmu_1(X_t) - hatmu_0(X_t)  textif  D_t = 1 6pt displaystylehatmu_1(X_t) - frac(1-D_t)(y_t+h - hatmu_0(X_t))1-hatp(X_t) - hatmu_0(X_t)  textif  D_t = 0 endcases\n\nhattextATE^DR_h = frac1nsum_t=1^n hatpsi_t qquad textSE_h = fractextsd(hatpsi)sqrtn\n\nDouble robustness property: The DR estimator is consistent if either the propensity score model or the outcome regression model is correctly specified. This provides insurance against misspecification of one component, making it the safer default choice in applied work.","category":"section"},{"location":"lp/#When-Do-the-Two-Estimators-Differ?","page":"Local Projections","title":"When Do the Two Estimators Differ?","text":"Feature estimate_propensity_lp (IPW) doubly_robust_lp (DR/AIPW)\nMethod WLS with inverse propensity weights Influence function combining IPW + outcome regression\nConsistency requires Correct propensity model Correct propensity or outcome model\nATE computation Treatment coefficient from WLS Mean of doubly robust influence function hatpsi_t\nStandard errors HAC-robust on weighted residuals textsd(hatpsi_t)sqrtn\nBest when Propensity model well-specified Uncertainty about either model\n\nThe two estimators coincide only when the propensity score model is exactly correct and the outcome is truly linear in covariates. In practice, they produce different ATE estimates because:\n\nOutcome model bias correction: DR subtracts the predicted outcome hatmu_d(X_t) from the observed outcome before applying IPW. This correction reduces the sensitivity to propensity score misspecification.\nDifferent variance estimation: IPW uses WLS residual-based HAC standard errors; DR uses the cross-sectional standard deviation of the influence function.\n\nnote: Recommendation\nUse doubly_robust_lp as the default. It is never worse than IPW asymptotically, and can be substantially better when the propensity model is misspecified. Use estimate_propensity_lp when you have strong confidence in the propensity score specification or want direct WLS coefficients for all regressors.","category":"section"},{"location":"lp/#Practical-Considerations","page":"Local Projections","title":"Practical Considerations","text":"Trimming: Propensity scores near 0 or 1 lead to extreme weights. Trim at [0.01, 0.99].\nOverlap: Verify that treated and control groups have overlapping covariate distributions.\nBalance: Check that covariates are balanced after reweighting (standardized mean differences < 0.1).\n\nReference: Angrist, Jordà & Kuersteiner (2018), Hirano, Imbens & Ridder (2003), Robins, Rotnitzky & Zhao (1994)","category":"section"},{"location":"lp/#Julia-Implementation-5","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Construct binary treatment: large absolute FFR changes (top quartile)\nffr_changes = abs.(diff(Y[:, 3]))\ntreatment = Bool.(ffr_changes .> quantile(ffr_changes, 0.75))\nY_trim = Y[2:end, :]  # align with diff\n\n# Covariates: lagged INDPRO and CPI growth\ncovariates = Y_trim[:, 1:2]\n\n# IPW estimation — consistent if propensity model is correct\nipw_model = estimate_propensity_lp(Y_trim, treatment, covariates, 20;\n    ps_method = :logit,\n    trimming = (0.01, 0.99),\n    lags = 4\n)\n\n# Doubly robust estimation — consistent if EITHER model is correct\ndr_model = doubly_robust_lp(Y_trim, treatment, covariates, 20;\n    ps_method = :logit,\n    trimming = (0.01, 0.99),\n    lags = 4\n)\n\n# Compare ATE estimates (they will generally differ)\nprintln(\"IPW ATE at h=0: \", ipw_model.ate[1, :])\nprintln(\"DR  ATE at h=0: \", dr_model.ate[1, :])\n\n# Extract ATE impulse response\nate_irf = propensity_irf(ipw_model)\ndr_irf  = propensity_irf(dr_model)\n\n# Diagnostics (works for both model types)\ndiagnostics = propensity_diagnostics(ipw_model)\nprintln(\"Propensity score overlap: \", diagnostics.overlap)\nprintln(\"Max covariate imbalance: \", diagnostics.balance.max_weighted)\n\nThe ate_irf object contains the estimated Average Treatment Effect of large FFR changes at each horizon. The diagnostics check two key assumptions: overlap (sufficient common support between treated and control distributions) and balance (covariate means equalized after reweighting, with standardized differences below 0.1).","category":"section"},{"location":"lp/#PropensityLPModel-Return-Values","page":"Local Projections","title":"PropensityLPModel Return Values","text":"Field Type Description\nY Matrix{T} Original data matrix\ntreatment Vector{Bool} Binary treatment indicator\nresponse_vars Vector{Int} Response variable indices\ncovariates Matrix{T} Selection-relevant covariates\nhorizon Int Maximum horizon\npropensity_scores Vector{T} Estimated propensity scores hatp(X_t)\nipw_weights Vector{T} Inverse propensity weights\nB Vector{Matrix{T}} Weighted regression coefficients\nresiduals Vector{Matrix{T}} Weighted residuals\nvcov Vector{Matrix{T}} Variance-covariance matrices\nate Matrix{T} Average treatment effect estimates\nate_se Matrix{T} Standard errors of ATE\nconfig PropensityScoreConfig{T} Configuration (method, trimming, normalize)\nT_eff Vector{Int} Effective sample sizes\ncov_estimator AbstractCovarianceEstimator Covariance estimator used\n\n","category":"section"},{"location":"lp/#Structural-Local-Projections","page":"Local Projections","title":"Structural Local Projections","text":"","category":"section"},{"location":"lp/#Motivation-5","page":"Local Projections","title":"Motivation","text":"Standard LP estimates the response to a single shock variable. In multivariate settings, however, we often want to trace the dynamic effects of orthogonalized structural shocks — just as in SVAR analysis. Structural Local Projections combine VAR-based identification with LP estimation to achieve this.\n\nPlagborg-Møller & Wolf (2021) establish a deep connection: under correct specification, LP and VAR estimate the same impulse responses. Structural LP leverages this equivalence by using the VAR only for shock identification (computing the rotation matrix Q), then estimating the dynamic responses via LP regressions — gaining the robustness of LP while retaining the structural interpretability of SVAR.","category":"section"},{"location":"lp/#Algorithm","page":"Local Projections","title":"Algorithm","text":"The structural LP procedure proceeds in five steps:\n\nEstimate VAR(p): Fit a VAR on the data Y to obtain the residual covariance hatSigma and reduced-form residuals hatu_t\nIdentify structural shocks: Compute the rotation matrix Q via the chosen identification method (Cholesky, sign restrictions, long-run, ICA, etc.)\nRecover structural shocks: Compute hatvarepsilon_t = QL^-1hatu_t where L = textchol(hatSigma)\nRun LP regressions: For each structural shock j, estimate LP regressions using hatvarepsilon_jt as the shock variable:\n\ny_it+h = alpha_ih^(j) + beta_ih^(j) hatvarepsilon_jt + gamma_ih^(j)prime w_t + u_it+h^(j)\n\nwhere\n\ny_it+h is the response variable i at horizon t+h\nhatvarepsilon_jt is the identified structural shock j\nw_t contains lagged values of Y as controls\nbeta_ih^(j) is the structural impulse response of variable i to shock j at horizon h\n\nStack into 3D IRF array: Thetah i j = hatbeta_ih^(j) for h = 1 ldots H","category":"section"},{"location":"lp/#Identification-Methods","page":"Local Projections","title":"Identification Methods","text":"Structural LP supports all identification methods available for SVAR:\n\nMethod Keyword Description\nCholesky :cholesky Recursive ordering (lower triangular B_0)\nSign restrictions :sign Constrain signs of responses (Uhlig, 2005)\nLong-run :long_run Blanchard-Quah (1989) zero long-run effect\nNarrative :narrative Historical events + sign restrictions (Antolín-Díaz & Rubio-Ramírez, 2018)\nFastICA :fastica Non-Gaussian ICA (Hyvärinen, 1999)\nJADE :jade Joint Approximate Diagonalization of Eigenmatrices\nSOBI :sobi Second-Order Blind Identification\ndCov :dcov Distance covariance independence criterion\nHSIC :hsic Hilbert-Schmidt independence criterion\nStudent-t ML :student_t Maximum likelihood with Student-t errors\nMixture-normal ML :mixture_normal Gaussian mixture ML\nPML :pml Pseudo maximum likelihood","category":"section"},{"location":"lp/#Julia-Implementation-6","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset: [INDPRO, CPI, FFR]\n# Cholesky ordering: output → prices → monetary policy\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Structural LP with Cholesky identification\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\n\n# Access 3D IRF array: irfs.values[h, i, j]\n# Shock 3 = monetary policy shock (FFR), Var 1 = INDPRO\nprintln(\"FFR shock → INDPRO at h=1: \", round(slp.irf.values[1, 1, 3], digits=4))\nprintln(\"FFR shock → CPI at h=8: \", round(slp.irf.values[8, 2, 3], digits=4))\n\n# Standard errors\nprintln(\"SE at h=1: \", round(slp.se[1, 1, 3], digits=4))\n\n# With bootstrap CIs\nusing Random; Random.seed!(42)\nslp_ci = structural_lp(Y, 20; method=:cholesky, ci_type=:bootstrap, reps=500)\n\n# With sign restrictions: positive supply shock raises output and lowers prices\ncheck_fn(irf) = irf[1, 1, 1] > 0 && irf[1, 2, 1] < 0\nslp_sign = structural_lp(Y, 20; method=:sign, check_func=check_fn)\n\n# Dispatch to IRF, FEVD, HD\nirf_result = irf(slp)           # Returns the ImpulseResponse from StructuralLP\ndecomp = fevd(slp, 20)          # LP-FEVD (Gorodnichenko & Lee 2019)\nhd = historical_decomposition(slp)  # LP-based historical decomposition\n\nThe slp.irf.values array has shape H times n times n, where values[h, i, j] gives the response of variable i to structural shock j at horizon h. Under Cholesky identification with ordering [INDPRO, CPI, FFR], the monetary policy shock (shock 3) can affect all three variables contemporaneously, but the federal funds rate does not respond to output or price shocks within the period. The standard errors in slp.se are computed from HAC-corrected LP regressions and tend to be wider than VAR-based IRF confidence bands, reflecting the efficiency cost of LP's robustness to dynamic misspecification.","category":"section"},{"location":"lp/#StructuralLP-Return-Values","page":"Local Projections","title":"StructuralLP Return Values","text":"Field Type Description\nirf ImpulseResponse{T} 3D IRF result (H times n times n) with optional bootstrap CIs\nstructural_shocks Matrix{T} T_eff times n recovered structural shocks\nvar_model VARModel{T} Underlying VAR model used for identification\nQ Matrix{T} n times n rotation/identification matrix\nmethod Symbol Identification method used\nlags Int Number of LP control lags\ncov_type Symbol HAC estimator type (:newey_west, :white)\nse Array{T,3} H times n times n standard errors\nlp_models Vector{LPModel{T}} Individual LP model per shock\n\nReference: Plagborg-Møller & Wolf (2021)\n\n","category":"section"},{"location":"lp/#LP-Forecasting","page":"Local Projections","title":"LP Forecasting","text":"","category":"section"},{"location":"lp/#Direct-Multi-Step-Forecasts","page":"Local Projections","title":"Direct Multi-Step Forecasts","text":"LP-based forecasts use horizon-specific regression coefficients directly — no VAR recursion required. For each horizon h = 1 ldots H, the forecast is:\n\nhaty_T+h = hatalpha_h + hatbeta_h cdot s_h + hatGamma_h w_T\n\nwhere\n\nhaty_T+h is the h-step-ahead point forecast\nhatalpha_h is the horizon-specific intercept\nhatbeta_h is the coefficient on the assumed shock path value s_h\nhatGamma_h is the coefficient vector on controls w_T (last p observations of Y)\n\nThis \"direct\" approach has a key advantage over recursive (iterated) VAR forecasts: each horizon uses its own regression, so misspecification in the short-horizon model does not compound into longer horizons.","category":"section"},{"location":"lp/#Confidence-Intervals","page":"Local Projections","title":"Confidence Intervals","text":"Three CI methods are available:\n\nMethod Description\n:analytical HAC standard errors + normal quantiles: haty_T+h pm z_alpha2 cdot hatsigma_h\n:bootstrap Residual resampling with percentile CIs\n:none Point forecasts only (no CIs)","category":"section"},{"location":"lp/#Julia-Implementation-7","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# Estimate LP model: FFR shock\nlp = estimate_lp(Y, 3, 20; lags=4, cov_type=:newey_west)\n\n# Forecast with a unit shock path (1 at all horizons)\nshock_path = ones(20)\nfc = forecast(lp, shock_path; ci_method=:analytical, conf_level=0.95)\n\nprintln(\"INDPRO forecast at h=1: \", round(fc.forecasts[1, 1], digits=4))\nprintln(\"INDPRO forecast at h=8: \", round(fc.forecasts[8, 1], digits=4))\nprintln(\"95% CI at h=8: [\", round(fc.ci_lower[8, 1], digits=4),\n        \", \", round(fc.ci_upper[8, 1], digits=4), \"]\")\n\n# Structural LP forecast with monetary policy shock\nslp = structural_lp(Y, 20; method=:cholesky)\nusing Random; Random.seed!(42)\nfc_struct = forecast(slp, 3, shock_path;  # shock_idx=3 (monetary policy)\n                     ci_method=:bootstrap, n_boot=500)\n\nThe fc.forecasts matrix has shape H times n_resp, where each row gives the point forecast at a given horizon. The analytical CIs widen with the horizon because the LP regression residuals exhibit increasing variance at longer horizons and the effective sample shrinks. The bootstrap CIs are generally more reliable in small samples because they do not rely on the normal approximation; however, they require the LP residuals to be approximately exchangeable, which holds under correct specification.","category":"section"},{"location":"lp/#LPForecast-Return-Values","page":"Local Projections","title":"LPForecast Return Values","text":"Field Type Description\nforecasts Matrix{T} H times n_resp point forecasts\nci_lower Matrix{T} Lower CI bounds\nci_upper Matrix{T} Upper CI bounds\nse Matrix{T} Standard errors at each horizon\nhorizon Int Maximum forecast horizon H\nresponse_vars Vector{Int} Response variable indices\nshock_var Int Shock variable index\nshock_path Vector{T} Assumed shock trajectory\nconf_level T Confidence level\nci_method Symbol CI method used (:analytical, :bootstrap, :none)\n\nReference: Jordà (2005), Plagborg-Møller & Wolf (2021)\n\n","category":"section"},{"location":"lp/#LP-Based-FEVD","page":"Local Projections","title":"LP-Based FEVD","text":"","category":"section"},{"location":"lp/#Motivation-6","page":"Local Projections","title":"Motivation","text":"Standard FEVD computes the share of forecast error variance attributable to each structural shock using the VMA (Vector Moving Average) representation. However, if the VAR is misspecified, VMA-based FEVD inherits those errors. Gorodnichenko & Lee (2019) propose an LP-based FEVD that estimates variance shares directly via R² regressions, inheriting the robustness properties of LP.","category":"section"},{"location":"lp/#The-R-Estimator","page":"Local Projections","title":"The R² Estimator","text":"At each horizon h, the share of variable i's forecast error variance due to shock j is estimated by:\n\nObtain LP forecast error residuals hatf_t+ht-1 from the LP regression\nRegress these residuals on structural shock leads hatvarepsilon_jt+h hatvarepsilon_jt+h-1 ldots hatvarepsilon_jt\nThe R² from this regression is the FEVD share:\n\nwidehattextFEVD_ij(h) = R^2left(hatf_it+ht-1 sim hatvarepsilon_jt+h hatvarepsilon_jt+h-1 ldots hatvarepsilon_jtright)\n\nwhere\n\nhatf_it+ht-1 are LP forecast error residuals for variable i at horizon h\nhatvarepsilon_jt+k are leads and current values of structural shock j\nR^2 measures the fraction of forecast error variance explained by shock j","category":"section"},{"location":"lp/#Alternative-Estimators","page":"Local Projections","title":"Alternative Estimators","text":"Two additional estimators are available:\n\nLP-A Estimator (Gorodnichenko & Lee 2019, Eq. 9):\n\nhats_ij^A(h) = fracsum_k=0^h (hatbeta_0ik^LP)^2 hatsigma_varepsilon_j^2textVar(hatf_it+ht-1)\n\nwhere\n\nhatbeta_0ik^LP is the LP coefficient on shock j at horizon k\nhatsigma_varepsilon_j^2 is the variance of structural shock j\n\nLP-B Estimator (Gorodnichenko & Lee 2019, Eq. 10):\n\nhats_ij^B(h) = fractextnumerator^Atextnumerator^A + textVar(tildev_t+h)\n\nwhere tildev_t+h are the residuals from the R² regression. LP-B replaces the total forecast error variance in the denominator with the sum of explained and unexplained components, which can improve finite-sample performance.","category":"section"},{"location":"lp/#Bias-Correction","page":"Local Projections","title":"Bias Correction","text":"LP-FEVD estimates can be biased in finite samples. Following Kilian (1998), the package implements VAR-based bootstrap bias correction:\n\nFit a bivariate VAR(L) on (z y) with HQIC-selected lag order\nCompute the \"true\" FEVD from this VAR (theoretical benchmark)\nSimulate B bootstrap samples from the VAR\nFor each simulation, compute LP-FEVD and estimate bias = textmean(textboot) - texttrue\nBias-corrected estimate = raw - bias\nCIs from the centered bootstrap distribution","category":"section"},{"location":"lp/#Julia-Implementation-8","page":"Local Projections","title":"Julia Implementation","text":"using MacroEconometricModels\n\n# Load FRED-MD monetary policy dataset\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\n# First estimate structural LP with Cholesky ordering [INDPRO, CPI, FFR]\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)\n\n# R²-based LP-FEVD with bias correction\nusing Random; Random.seed!(42)\nlfevd = lp_fevd(slp, 20; method=:r2, bias_correct=true, n_boot=500)\n\n# Access results: how much of INDPRO variance is due to the monetary policy shock?\nprintln(\"FEVD of INDPRO due to FFR shock:\")\nfor h in [1, 4, 8, 12, 20]\n    raw = round(lfevd.proportions[1, 3, h] * 100, digits=1)\n    bc = round(lfevd.bias_corrected[1, 3, h] * 100, digits=1)\n    println(\"  h=$h: raw=$(raw)%, bias-corrected=$(bc)%\")\nend\n\n# Alternative estimators\nlfevd_a = lp_fevd(slp, 20; method=:lp_a)\nlfevd_b = lp_fevd(slp, 20; method=:lp_b)\n\n# Via dispatch\ndecomp = fevd(slp, 20)  # Equivalent to lp_fevd(slp, 20)\n\nThe raw FEVD proportions in lfevd.proportions[i, j, h] give the R² from regressing variable i's forecast error on shock j's leads at horizon h. Bias correction typically matters most at short horizons where finite-sample bias is largest. At long horizons, the R²-based and VMA-based FEVD should converge under correct specification. Comparing the three estimators (:r2, :lp_a, :lp_b) provides a robustness check — substantial disagreement suggests the VAR specification may be unreliable, in which case the LP-based estimates are preferred.","category":"section"},{"location":"lp/#LPFEVD-Return-Values","page":"Local Projections","title":"LPFEVD Return Values","text":"Field Type Description\nproportions Array{T,3} n times n times H raw FEVD estimates: proportions[i, j, h] = share of variable i's FEV due to shock j at horizon h\nbias_corrected Array{T,3} n times n times H bias-corrected FEVD\nse Array{T,3} Bootstrap standard errors\nci_lower Array{T,3} Lower CI bounds\nci_upper Array{T,3} Upper CI bounds\nmethod Symbol Estimator used (:r2, :lp_a, :lp_b)\nhorizon Int Maximum FEVD horizon\nn_boot Int Number of bootstrap replications\nconf_level T Confidence level for CIs\nbias_correction Bool Whether bias correction was applied\n\nReference: Gorodnichenko, Yuriy, and Byoungchan Lee. 2019. \"Forecast Error Variance Decompositions with Local Projections.\" Journal of Business & Economic Statistics 38 (4): 921–933. https://doi.org/10.1080/07350015.2019.1610661\n\n","category":"section"},{"location":"lp/#Comparing-LP-and-VAR","page":"Local Projections","title":"Comparing LP and VAR","text":"","category":"section"},{"location":"lp/#LP-vs.-VAR-Trade-offs","page":"Local Projections","title":"LP vs. VAR Trade-offs","text":"Aspect VAR Local Projections\nEfficiency More efficient if correctly specified Less efficient, but robust\nBias Biased if dynamics misspecified Consistent under weak conditions\nLong horizons Compounds specification error Each horizon estimated directly\nNonlinearities Requires extensions Easy to incorporate\nExternal instruments SVAR-IV LP-IV","category":"section"},{"location":"lp/#Asymptotic-Equivalence","page":"Local Projections","title":"Asymptotic Equivalence","text":"Plagborg-Møller & Wolf (2021) show that under correct specification, LP and VAR IRFs are asymptotically equivalent:\n\nsqrtT(hatbeta_h^LP - beta_h) xrightarrowd N(0 V^LP)\n\nsqrtT(hattheta_h^VAR - theta_h) xrightarrowd N(0 V^VAR)\n\nwith V^LP geq V^VAR (VAR is weakly more efficient).","category":"section"},{"location":"lp/#When-to-Use-LP","page":"Local Projections","title":"When to Use LP","text":"Concerned about VAR misspecification\nNeed to incorporate external instruments\nInterested in nonlinear/state-dependent responses\nWorking with discrete treatments\nLong horizons where VAR error compounds\n\nReference: Plagborg-Møller & Wolf (2021)\n\n","category":"section"},{"location":"lp/#See-Also","page":"Local Projections","title":"See Also","text":"VAR Estimation – VAR-based impulse responses for comparison with LP estimates\nInnovation Accounting – LP-FEVD and structural decomposition details\nHypothesis Tests – Unit root and Granger causality tests\nAPI Reference – Complete function signatures","category":"section"},{"location":"lp/#References","page":"Local Projections","title":"References","text":"","category":"section"},{"location":"lp/#Local-Projections-Core","page":"Local Projections","title":"Local Projections - Core","text":"Jordà, Òscar. 2005. \"Estimation and Inference of Impulse Responses by Local Projections.\" American Economic Review 95 (1): 161–182. https://doi.org/10.1257/0002828053828518\nPlagborg-Møller, Mikkel, and Christian K. Wolf. 2021. \"Local Projections and VARs Estimate the Same Impulse Responses.\" Econometrica 89 (2): 955–980. https://doi.org/10.3982/ECTA17813","category":"section"},{"location":"lp/#LP-IV","page":"Local Projections","title":"LP-IV","text":"Stock, James H., and Mark W. Watson. 2018. \"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments.\" Economic Journal 128 (610): 917–948. https://doi.org/10.1111/ecoj.12593\nStock, James H., and Motohiro Yogo. 2005. \"Testing for Weak Instruments in Linear IV Regression.\" In Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg, edited by Donald W. K. Andrews and James H. Stock, 80–108. Cambridge: Cambridge University Press.","category":"section"},{"location":"lp/#Smooth-LP","page":"Local Projections","title":"Smooth LP","text":"Barnichon, Regis, and Christian Brownlees. 2019. \"Impulse Response Estimation by Smooth Local Projections.\" Review of Economics and Statistics 101 (3): 522–530. https://doi.org/10.1162/resta00778","category":"section"},{"location":"lp/#State-Dependent-LP","page":"Local Projections","title":"State-Dependent LP","text":"Auerbach, Alan J., and Yuriy Gorodnichenko. 2012. \"Measuring the Output Responses to Fiscal Policy.\" American Economic Journal: Economic Policy 4 (2): 1–27. https://doi.org/10.1257/pol.4.2.1\nAuerbach, Alan J., and Yuriy Gorodnichenko. 2013. \"Fiscal Multipliers in Recession and Expansion.\" In Fiscal Policy after the Financial Crisis, edited by Alberto Alesina and Francesco Giavazzi, 63–98. Chicago: University of Chicago Press. https://doi.org/10.7208/chicago/9780226018584.003.0003\nRamey, Valerie A., and Sarah Zubairy. 2018. \"Government Spending Multipliers in Good Times and in Bad: Evidence from US Historical Data.\" Journal of Political Economy 126 (2): 850–901. https://doi.org/10.1086/696277","category":"section"},{"location":"lp/#Structural-LP-and-LP-FEVD","page":"Local Projections","title":"Structural LP and LP-FEVD","text":"Gorodnichenko, Yuriy, and Byoungchan Lee. 2019. \"Forecast Error Variance Decompositions with Local Projections.\" Journal of Business & Economic Statistics 38 (4): 921–933. https://doi.org/10.1080/07350015.2019.1610661\nKilian, Lutz. 1998. \"Small-Sample Confidence Intervals for Impulse Response Functions.\" Review of Economics and Statistics 80 (2): 218–230. https://doi.org/10.1162/003465398557465","category":"section"},{"location":"lp/#Propensity-Score-Methods","page":"Local Projections","title":"Propensity Score Methods","text":"Angrist, Joshua D., Òscar Jordà, and Guido M. Kuersteiner. 2018. \"Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited.\" Journal of Business & Economic Statistics 36 (3): 371–387. https://doi.org/10.1080/07350015.2016.1204919\nHirano, Keisuke, Guido W. Imbens, and Geert Ridder. 2003. \"Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.\" Econometrica 71 (4): 1161–1189. https://doi.org/10.1111/1468-0262.00442","category":"section"},{"location":"lp/#Inference","page":"Local Projections","title":"Inference","text":"Newey, Whitney K., and Kenneth D. West. 1987. \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.\" Econometrica 55 (3): 703–708. https://doi.org/10.2307/1913610","category":"section"},{"location":"#MacroEconometricModels.jl","page":"Home","title":"MacroEconometricModels.jl","text":"A comprehensive Julia package for macroeconometric research and analysis","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"MacroEconometricModels.jl provides a unified, high-performance framework for estimating and analyzing macroeconometric models in Julia. The package implements state-of-the-art methods spanning the full empirical macro workflow: from unit root testing and trend-cycle decomposition, through univariate and multivariate model estimation, to structural identification and publication-quality output.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Univariate Models\n\nTime Series Filters: Hodrick-Prescott (1997), Hamilton (2018) regression, Beveridge-Nelson (1981), Baxter-King (1999) band-pass, and boosted HP (Phillips & Shi 2021) with unified trend()/cycle() accessors\nARIMA: AR, MA, ARMA, ARIMA estimation via OLS, CSS, MLE (Kalman filter), and CSS-MLE; automatic order selection (auto_arima); multi-step forecasting with confidence intervals\nVolatility Models: ARCH (Engle 1982), GARCH (Bollerslev 1986), EGARCH (Nelson 1991), GJR-GARCH (Glosten et al. 1993) via MLE; Stochastic Volatility via Kim-Shephard-Chib (1998) Gibbs sampler (basic, leverage, Student-t variants); news impact curves, ARCH-LM diagnostics, multi-step forecasting\n\nMultivariate Models\n\nVAR: OLS estimation with lag order selection (AIC, BIC, HQ), stability diagnostics, companion matrix\nBayesian VAR: Conjugate Normal-Inverse-Wishart posterior with Minnesota prior; direct and Gibbs samplers; automatic hyperparameter optimization via marginal likelihood (Giannone, Lenza & Primiceri 2015)\nVECM: Johansen MLE and Engle-Granger two-step estimation for cointegrated systems; automatic rank selection; IRF/FEVD/HD via VAR conversion (to_var); VECM-specific forecasting; Granger causality (short-run, long-run, strong)\nPanel VAR: GMM estimation via Arellano-Bond (1991) first-difference and Blundell-Bond (1998) system GMM; fixed-effects OLS; Windmeijer (2005) corrected standard errors; Hansen J-test, Andrews-Lu MMSC; OIRF, GIRF, FEVD; group-level bootstrap CIs; lag selection\nLocal Projections: Jorda (2005) with extensions for IV (Stock & Watson 2018), smooth LP (Barnichon & Brownlees 2019), state-dependence (Auerbach & Gorodnichenko 2013), propensity score weighting (Angrist et al. 2018), structural LP (Plagborg-Moller & Wolf 2021), LP forecasting, and LP-FEVD (Gorodnichenko & Lee 2019)\nFactor Models: Static (PCA), dynamic (two-step/EM), and generalized dynamic (spectral GDFM) with Bai-Ng information criteria; unified forecasting with theoretical and bootstrap CIs\nGMM: Flexible estimation with one-step, two-step, and iterated weighting; Hansen J-test\n\nInnovation Accounting\n\nIRF: Impulse responses with bootstrap, theoretical, and Bayesian credible intervals\nFEVD: Forecast error variance decomposition (frequentist and Bayesian)\nHistorical Decomposition: Decompose observed movements into structural shock contributions\nLP-FEVD: R-squared, LP-A, and LP-B estimators (Gorodnichenko & Lee 2019)\n\nStructural Identification (18+ methods)\n\nCholesky, sign restrictions, long-run (Blanchard-Quah), narrative restrictions, Arias et al. (2018), Mountford-Uhlig (2009) penalty function\nNon-Gaussian ICA: FastICA, JADE, SOBI, dCov, HSIC\nNon-Gaussian ML: Student-t, mixture-normal, PML, skew-normal\nHeteroskedasticity-based: Markov-switching, GARCH, smooth-transition, external volatility\nIdentifiability diagnostics: gaussianity tests, independence tests, bootstrap strength tests\n\nNowcasting\n\nDynamic Factor Model (DFM): EM algorithm with Kalman smoother for mixed-frequency data with arbitrary missing patterns (Banbura & Modugno 2014); Mariano-Murasawa temporal aggregation; block factor structure; AR(1)/IID idiosyncratic components\nLarge Bayesian VAR: GLP-style Normal-Inverse-Wishart prior with hyperparameter optimization via marginal likelihood (Cimadomo et al. 2022); Minnesota shrinkage with sum-of-coefficients and co-persistence priors\nBridge Equations: OLS bridge regressions combining pairs of monthly indicators via median (Banbura et al. 2023); transparent and fast baseline\nNews Decomposition: Attribute nowcast revisions to individual data releases via Kalman gain weights\nPanel Balancing: balance_panel() fills NaN in TimeSeriesData/PanelData using DFM imputation\n\nHypothesis Tests\n\nUnit root: ADF, KPSS, Phillips-Perron, Zivot-Andrews, Ng-Perron\nCointegration: Johansen trace and max-eigenvalue tests\nGranger causality: pairwise Wald, block (multivariate), all-pairs matrix\nModel comparison: likelihood ratio (LR) and Lagrange multiplier (LM/score) tests for nested models\nNormality: Jarque-Bera, Mardia, Doornik-Hansen, Henze-Zirkler\nStationarity diagnostics: unit_root_summary(), test_all_variables()\n\nData Management\n\nTyped Containers: TimeSeriesData, PanelData, CrossSectionData with metadata (frequency, variable names, transformation codes)\nValidation: diagnose() detects NaN/Inf/constant columns; fix() repairs via listwise deletion, interpolation, or mean imputation\nFRED Transforms: apply_tcode() / inverse_tcode() implement all 7 FRED-MD transformation codes (McCracken & Ng 2016)\nPanel Support: Stata-style xtset() for panel construction, group_data() for per-entity extraction\nSummary Statistics: describe_data() with N, Mean, Std, Min, P25, Median, P75, Max, Skewness, Kurtosis\nEstimation Dispatch: All estimation functions accept TimeSeriesData directly\n\nOutput and References\n\nDisplay backends: switchable text, LaTeX, and HTML table output via set_display_backend()\nPublication-quality tables: report(), table(), print_table()\nBibliographic references: refs(model) in AEA text, BibTeX, LaTeX, or HTML format (209 entries)","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"MacroEconometricModels\")\n\nOr from the Julia REPL package mode:\n\n] add MacroEconometricModels","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#One-Liner-Overview","page":"Home","title":"One-Liner Overview","text":"using MacroEconometricModels\n\n# Univariate\nhp = hp_filter(y; lambda=1600.0)                   # Trend-cycle decomposition\nar = estimate_ar(y, 2)                              # AR(2) via OLS\ngarch = estimate_garch(y, 1, 1)                     # GARCH(1,1)\nsv = estimate_sv(y; n_samples=2000)                  # Stochastic Volatility\n\n# Multivariate\nmodel = estimate_var(Y, 2)                           # VAR(2) via OLS\nirfs = irf(model, 20; method=:cholesky)              # Impulse responses\npost = estimate_bvar(Y, 2; prior=:minnesota)         # Bayesian VAR\nvecm = estimate_vecm(Y, 2; rank=:auto)               # VECM\nlp = estimate_lp(Y, 1, 20; cov_type=:newey_west)    # Local Projections\nfm = estimate_factors(X, 3)                          # Factor model\ngmm = estimate_gmm(g, theta0, data; weighting=:two_step)  # GMM\n\n# Tests & diagnostics\nadf = adf_test(y)                                    # Unit root test\ng = granger_test(model, 1, 2)                        # Granger causality\nsuite = normality_test_suite(model)                   # Normality tests\n\n# Output\nrefs(model)                                           # Bibliographic references\nset_display_backend(:latex)                            # Switch to LaTeX tables\n\n","category":"section"},{"location":"#Time-Series-Filters","page":"Home","title":"Time Series Filters","text":"using MacroEconometricModels\n\n# Log industrial production from FRED-MD (monthly, I(1))\nfred = load_example(:fred_md)\ny = filter(isfinite, log.(fred[:, \"INDPRO\"]))\n\nhp  = hp_filter(y; lambda=129600.0)        # Hodrick-Prescott (monthly)\nham = hamilton_filter(y; h=24, p=12)        # Hamilton (2018)\nbn  = beveridge_nelson(y)                   # Beveridge-Nelson\nbk  = baxter_king(y; pl=18, pu=96, K=36)  # Baxter-King band-pass (monthly)\nbhp = boosted_hp(y; stopping=:BIC)          # Boosted HP\n\ntrend(hp)  # trend component\ncycle(hp)  # cyclical component","category":"section"},{"location":"#ARIMA","page":"Home","title":"ARIMA","text":"using MacroEconometricModels\n\n# IP growth rate (log first difference) from FRED-MD\nfred = load_example(:fred_md)\ny = filter(isfinite, apply_tcode(fred[:, \"INDPRO\"], 5))\n\nbest = auto_arima(y)                        # Automatic order selection\narma = estimate_arma(y, 1, 1)              # ARMA(1,1) via CSS-MLE\nfc = forecast(arma, 12; conf_level=0.95)   # 12-step forecast with CIs","category":"section"},{"location":"#Volatility-Models","page":"Home","title":"Volatility Models","text":"using MacroEconometricModels\n\n# S&P 500 returns from FRED-MD\nfred = load_example(:fred_md)\nsp_idx = findfirst(v -> occursin(\"S&P\", v) && occursin(\"500\", v), varnames(fred))\ny = filter(isfinite, apply_tcode(fred[:, varnames(fred)[sp_idx]], 5))\n\ngarch  = estimate_garch(y, 1, 1)                    # GARCH(1,1)\negarch = estimate_egarch(y, 1, 1)                    # EGARCH(1,1)\ngjr    = estimate_gjr_garch(y, 1, 1)                # GJR-GARCH(1,1)\nsv     = estimate_sv(y; n_samples=2000, burnin=1000) # Stochastic Volatility\n\nnic = news_impact_curve(egarch)    # Asymmetry diagnostic\nfc = forecast(garch, 20)           # Volatility forecast\npersistence(garch)                  # Volatility persistence","category":"section"},{"location":"#VAR-and-Structural-Identification","page":"Home","title":"VAR and Structural Identification","text":"using MacroEconometricModels\n\n# Stationary 3-variable monetary VAR (IP, CPI, Fed Funds) from FRED-MD\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nmodel = estimate_var(Y, 2)                          # OLS estimation\nirfs = irf(model, 20; method=:cholesky)             # Cholesky IRF\ndecomp = fevd(model, 20; method=:cholesky)          # FEVD\nhd = historical_decomposition(model)                # Historical decomposition","category":"section"},{"location":"#Bayesian-VAR","page":"Home","title":"Bayesian VAR","text":"using MacroEconometricModels, Random\n\n# Same 3-variable monetary VAR data as above\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nRandom.seed!(42)  # for reproducible MCMC draws\nbest_hyper = optimize_hyperparameters(Y, 2; grid_size=20)\npost = estimate_bvar(Y, 2; n_draws=1000,\n                     prior=:minnesota, hyper=best_hyper)\nbirf = irf(post, 20; method=:cholesky)   # Bayesian IRF with credible intervals","category":"section"},{"location":"#VECM","page":"Home","title":"VECM","text":"using MacroEconometricModels\n\n# Cointegrated quarterly I(1) system: log GDP, consumption, investment from FRED-QD\nqd = load_example(:fred_qd)\nY = log.(to_matrix(qd[:, [\"GDPC1\", \"PCECC96\", \"GPDIC1\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\njoh = johansen_test(Y, 2)                          # Cointegration test\nvecm = estimate_vecm(Y, 2; rank=:auto)             # VECM estimation\nirfs = irf(vecm, 20; method=:cholesky)             # IRF via VAR conversion\nfc = forecast(vecm, 12; ci_method=:bootstrap)      # VECM forecast\ngc = granger_causality_vecm(vecm, 1, 2)            # VECM Granger causality","category":"section"},{"location":"#Local-Projections","page":"Home","title":"Local Projections","text":"using MacroEconometricModels\n\n# 3-variable monetary VAR data\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nlp = estimate_lp(Y, 1, 20; lags=4, cov_type=:newey_west)   # Standard LP\nslp = structural_lp(Y, 20; method=:cholesky, lags=4)        # Structural LP\nlfevd = lp_fevd(slp)                                         # LP-FEVD","category":"section"},{"location":"#Factor-Models","page":"Home","title":"Factor Models","text":"using MacroEconometricModels\n\n# Large panel from FRED-MD (safe variables only, first 20 columns)\nfred = load_example(:fred_md)\nsafe_idx = [i for i in 1:nvars(fred)\n            if fred.tcode[i] < 4 || all(x -> isfinite(x) && x > 0, fred.data[:, i])]\nfred_safe = fred[:, varnames(fred)[safe_idx]]\nX = to_matrix(apply_tcode(fred_safe))\nX = X[all.(isfinite, eachrow(X)), 1:min(20, size(X, 2))]\n\nic = ic_criteria(X, 10)                                # Bai-Ng criteria\nfm = estimate_factors(X, ic.r_IC2; standardize=true)   # Static PCA\ndfm = estimate_dynamic_factors(X, 3, 2)                # Dynamic factors\nfc = forecast(fm, 12; ci_method=:theoretical)           # Forecast with CIs","category":"section"},{"location":"#Hypothesis-Tests","page":"Home","title":"Hypothesis Tests","text":"using MacroEconometricModels\n\n# Unit root tests on CPI inflation\nfred = load_example(:fred_md)\ny_cpi = filter(isfinite, apply_tcode(fred[:, \"CPIAUCSL\"], 5))\nadf = adf_test(y_cpi; lags=:aic)\nkpss = kpss_test(y_cpi)\nunit_root_summary(y_cpi)                  # Combined ADF + KPSS\n\n# Granger causality on 3-variable monetary VAR\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\nmodel = estimate_var(Y, 2)\ng = granger_test(model, 1, 2)        # Pairwise\ng_block = granger_test(model, [1,2], 3)  # Block\nresults = granger_test_all(model)    # All pairs\n\n# Model comparison\nlr = lr_test(restricted, unrestricted)   # Likelihood ratio\nlm = lm_test(restricted, unrestricted)   # Lagrange multiplier","category":"section"},{"location":"#Non-Gaussian-Identification","page":"Home","title":"Non-Gaussian Identification","text":"using MacroEconometricModels\n\n# 3-variable monetary VAR\nfred = load_example(:fred_md)\nY = to_matrix(apply_tcode(fred[:, [\"INDPRO\", \"CPIAUCSL\", \"FEDFUNDS\"]]))\nY = Y[all.(isfinite, eachrow(Y)), :]\n\nmodel = estimate_var(Y, 2)\nsuite = normality_test_suite(model)          # Test for non-Gaussianity\nica = identify_fastica(model)                # ICA identification\nml = identify_student_t(model)               # ML identification\nirfs = irf(model, 20; method=:fastica)       # IRF with ICA-identified shocks","category":"section"},{"location":"#Nowcasting","page":"Home","title":"Nowcasting","text":"using MacroEconometricModels\n\n# Mixed-frequency panel from FRED-MD\nfred = load_example(:fred_md)\nsub = fred[:, [\"INDPRO\", \"UNRATE\", \"CPIAUCSL\", \"FEDFUNDS\"]]\nY = to_matrix(apply_tcode(sub))\nY = Y[all.(isfinite, eachrow(Y)), :]\nnM = 3; nQ = 1  # 3 monthly indicators + 1 quarterly target\n\ndfm_model = nowcast_dfm(Y, nM, nQ; r=2, p=1)    # DFM (EM + Kalman)\nbvar = nowcast_bvar(Y, nM, nQ; lags=5)           # Large BVAR\nbridge = nowcast_bridge(Y, nM, nQ)                # Bridge equations\n\nresult = nowcast(dfm_model)                        # Current-quarter nowcast\nfc = forecast(dfm_model, 6)                        # 6-step forecast","category":"section"},{"location":"#Output-and-References","page":"Home","title":"Output and References","text":"using MacroEconometricModels\n\nset_display_backend(:latex)          # LaTeX tables for papers\nprint_table(irfs, 1, 1)             # Print IRF table\n\nrefs(model)                          # AEA-style references\nrefs(model; format=:bibtex)          # BibTeX for .bib files\nrefs(:johansen; format=:html)        # HTML with DOI links","category":"section"},{"location":"#Package-Structure","page":"Home","title":"Package Structure","text":"The package is organized into the following modules:\n\nModule Description\ndata/ Data containers, validation, FRED transforms, panel support, summary statistics\ncore/ Shared infrastructure: types, utilities, display backends, covariance estimators\narima/ ARIMA suite: types, Kalman filter, estimation (CSS/MLE), forecasting, order selection\nfilters/ Time series filters: HP, Hamilton, Beveridge-Nelson, Baxter-King, boosted HP\narch/ ARCH(q) estimation via MLE, volatility forecasting\ngarch/ GARCH, EGARCH, GJR-GARCH estimation via MLE, news impact curves, forecasting\nsv/ Stochastic Volatility via KSC (1998) Gibbs sampler, posterior predictive forecasts\nvar/ VAR estimation (OLS), structural identification, IRF, FEVD, historical decomposition\nvecm/ VECM: Johansen MLE, Engle-Granger, cointegrating vectors, forecasting, Granger causality\nbvar/ Bayesian VAR: conjugate NIW posterior sampling, Minnesota prior, hyperparameter optimization\nlp/ Local Projections: core, IV, smooth, state-dependent, propensity, structural LP, forecast, LP-FEVD\nfactor/ Static (PCA), dynamic (two-step/EM), generalized (spectral) factor models with forecasting\nnongaussian/ Non-Gaussian structural identification: ICA, ML, heteroskedastic-ID\nteststat/ Statistical tests: unit root, Johansen, normality, Granger causality, LR/LM, ARCH diagnostics\npvar/ Panel VAR: types, transforms, instruments, estimation (GMM/FE-OLS), analysis, bootstrap, tests\ngmm/ Generalized Method of Moments\nnowcast/ Nowcasting: DFM (EM + Kalman), large BVAR, bridge equations, news decomposition\nsummary.jl Publication-quality summary tables and refs() bibliographic references","category":"section"},{"location":"#Mathematical-Notation","page":"Home","title":"Mathematical Notation","text":"Throughout this documentation, we use the following notation conventions:\n\nSymbol Description\ny_t n times 1 vector of endogenous variables at time t\nY T times n data matrix\np Number of lags in VAR\nA_i n times n coefficient matrix for lag i\nSigma n times n reduced-form error covariance\nB_0 n times n contemporaneous impact matrix\nvarepsilon_t n times 1 structural shocks\nu_t n times n reduced-form residuals\nh Forecast/impulse response horizon\nH Maximum horizon","category":"section"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"#Univariate-Time-Series","page":"Home","title":"Univariate Time Series","text":"Box, George E. P., and Gwilym M. Jenkins. 1976. Time Series Analysis: Forecasting and Control. San Francisco: Holden-Day. ISBN 978-0-816-21104-3.\nBrockwell, Peter J., and Richard A. Davis. 1991. Time Series: Theory and Methods. 2nd ed. New York: Springer. ISBN 978-1-4419-0319-8.\nHarvey, Andrew C. 1993. Time Series Models. 2nd ed. Cambridge, MA: MIT Press. ISBN 978-0-262-08224-2.","category":"section"},{"location":"#Time-Series-Filters-2","page":"Home","title":"Time Series Filters","text":"Hodrick, Robert J., and Edward C. Prescott. 1997. \"Postwar U.S. Business Cycles: An Empirical Investigation.\" Journal of Money, Credit and Banking 29 (1): 1–16. https://doi.org/10.2307/2953682\nHamilton, James D. 2018. \"Why You Should Never Use the Hodrick-Prescott Filter.\" Review of Economics and Statistics 100 (5): 831–843. https://doi.org/10.1162/resta00706\nBeveridge, Stephen, and Charles R. Nelson. 1981. \"A New Approach to Decomposition of Economic Time Series into Permanent and Transitory Components.\" Journal of Monetary Economics 7 (2): 151–174. https://doi.org/10.1016/0304-3932(81)90040-4\nBaxter, Marianne, and Robert G. King. 1999. \"Measuring Business Cycles: Approximate Band-Pass Filters for Economic Time Series.\" Review of Economics and Statistics 81 (4): 575–593. https://doi.org/10.1162/003465399558454\nPhillips, Peter C. B., and Zhentao Shi. 2021. \"Boosting: Why You Can Use the HP Filter.\" International Economic Review 62 (2): 521–570. https://doi.org/10.1111/iere.12495","category":"section"},{"location":"#Volatility-Models-2","page":"Home","title":"Volatility Models","text":"Bollerslev, Tim. 1986. \"Generalized Autoregressive Conditional Heteroskedasticity.\" Journal of Econometrics 31 (3): 307–327. https://doi.org/10.1016/0304-4076(86)90063-1\nEngle, Robert F. 1982. \"Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation.\" Econometrica 50 (4): 987–1007. https://doi.org/10.2307/1912773\nGlosten, Lawrence R., Ravi Jagannathan, and David E. Runkle. 1993. \"On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks.\" Journal of Finance 48 (5): 1779–1801. https://doi.org/10.1111/j.1540-6261.1993.tb05128.x\nNelson, Daniel B. 1991. \"Conditional Heteroskedasticity in Asset Returns: A New Approach.\" Econometrica 59 (2): 347–370. https://doi.org/10.2307/2938260\nKim, Sangjoon, Neil Shephard, and Siddhartha Chib. 1998. \"Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models.\" Review of Economic Studies 65 (3): 361–393. https://doi.org/10.1111/1467-937X.00050\nTaylor, Stephen J. 1986. Modelling Financial Time Series. Chichester: Wiley. ISBN 978-0-471-90975-7.","category":"section"},{"location":"#VAR-and-Structural-Identification-2","page":"Home","title":"VAR and Structural Identification","text":"Blanchard, Olivier Jean, and Danny Quah. 1989. \"The Dynamic Effects of Aggregate Demand and Supply Disturbances.\" American Economic Review 79 (4): 655–673.\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press. ISBN 978-0-691-04289-3.\nKilian, Lutz, and Helmut Lutkepohl. 2017. Structural Vector Autoregressive Analysis. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108164818\nLutkepohl, Helmut. 2005. New Introduction to Multiple Time Series Analysis. Berlin: Springer. ISBN 978-3-540-40172-8.\nSims, Christopher A. 1980. \"Macroeconomics and Reality.\" Econometrica 48 (1): 1–48. https://doi.org/10.2307/1912017\nArias, Jonas E., Juan F. Rubio-Ramirez, and Daniel F. Waggoner. 2018. \"Inference Based on Structural Vector Autoregressions Identified with Sign and Zero Restrictions: Theory and Applications.\" Econometrica 86 (2): 685–720. https://doi.org/10.3982/ECTA14468\nMountford, Andrew, and Harald Uhlig. 2009. \"What Are the Effects of Fiscal Policy Shocks?\" Journal of Applied Econometrics 24 (6): 960–992. https://doi.org/10.1002/jae.1079","category":"section"},{"location":"#Bayesian-Methods","page":"Home","title":"Bayesian Methods","text":"Doan, Thomas, Robert Litterman, and Christopher Sims. 1984. \"Forecasting and Conditional Projection Using Realistic Prior Distributions.\" Econometric Reviews 3 (1): 1–100. https://doi.org/10.1080/07474938408800053\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. \"Prior Selection for Vector Autoregressions.\" Review of Economics and Statistics 97 (2): 436–451. https://doi.org/10.1162/RESTa00483\nLitterman, Robert B. 1986. \"Forecasting with Bayesian Vector Autoregressions–-Five Years of Experience.\" Journal of Business & Economic Statistics 4 (1): 25–38. https://doi.org/10.1080/07350015.1986.10509491","category":"section"},{"location":"#VECM-and-Cointegration","page":"Home","title":"VECM and Cointegration","text":"Engle, Robert F., and Clive W. J. Granger. 1987. \"Co-Integration and Error Correction: Representation, Estimation, and Testing.\" Econometrica 55 (2): 251–276. https://doi.org/10.2307/1913236\nJohansen, Soren. 1991. \"Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models.\" Econometrica 59 (6): 1551–1580. https://doi.org/10.2307/2938278","category":"section"},{"location":"#Local-Projections-2","page":"Home","title":"Local Projections","text":"Angrist, Joshua D., Oscar Jorda, and Guido M. Kuersteiner. 2018. \"Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited.\" Journal of Business & Economic Statistics 36 (3): 371–387. https://doi.org/10.1080/07350015.2016.1204919\nAuerbach, Alan J., and Yuriy Gorodnichenko. 2013. \"Fiscal Multipliers in Recession and Expansion.\" In Fiscal Policy after the Financial Crisis, edited by Alberto Alesina and Francesco Giavazzi, 63–98. Chicago: University of Chicago Press. https://doi.org/10.7208/chicago/9780226018584.003.0003\nBarnichon, Regis, and Christian Brownlees. 2019. \"Impulse Response Estimation by Smooth Local Projections.\" Review of Economics and Statistics 101 (3): 522–530. https://doi.org/10.1162/resta00778\nJorda, Oscar. 2005. \"Estimation and Inference of Impulse Responses by Local Projections.\" American Economic Review 95 (1): 161–182. https://doi.org/10.1257/0002828053828518\nStock, James H., and Mark W. Watson. 2018. \"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments.\" Economic Journal 128 (610): 917–948. https://doi.org/10.1111/ecoj.12593\nPlagborg-Moller, Mikkel, and Christian K. Wolf. 2021. \"Local Projections and VARs Estimate the Same Impulse Responses.\" Econometrica 89 (2): 955–980. https://doi.org/10.3982/ECTA17813\nGorodnichenko, Yuriy, and Byoungchan Lee. 2019. \"Forecast Error Variance Decompositions with Local Projections.\" Journal of Business & Economic Statistics 38 (4): 921–933. https://doi.org/10.1080/07350015.2019.1610661","category":"section"},{"location":"#Factor-Models-2","page":"Home","title":"Factor Models","text":"Bai, Jushan, and Serena Ng. 2002. \"Determining the Number of Factors in Approximate Factor Models.\" Econometrica 70 (1): 191–221. https://doi.org/10.1111/1468-0262.00273\nForni, Mario, Marc Hallin, Marco Lippi, and Lucrezia Reichlin. 2000. \"The Generalized Dynamic-Factor Model: Identification and Estimation.\" Review of Economics and Statistics 82 (4): 540–554. https://doi.org/10.1162/003465300559037\nStock, James H., and Mark W. Watson. 2002. \"Forecasting Using Principal Components from a Large Number of Predictors.\" Journal of the American Statistical Association 97 (460): 1167–1179. https://doi.org/10.1198/016214502388618960","category":"section"},{"location":"#Panel-VAR","page":"Home","title":"Panel VAR","text":"Holtz-Eakin, Douglas, Whitney Newey, and Harvey S. Rosen. 1988. \"Estimating Vector Autoregressions with Panel Data.\" Econometrica 56 (6): 1371–1395. https://doi.org/10.2307/1913103\nArellano, Manuel, and Stephen Bond. 1991. \"Some Tests of Specification for Panel Data.\" Review of Economic Studies 58 (2): 277–297. https://doi.org/10.2307/2297968\nBlundell, Richard, and Stephen Bond. 1998. \"Initial Conditions and Moment Restrictions in Dynamic Panel Data Models.\" Journal of Econometrics 87 (1): 115–143. https://doi.org/10.1016/S0304-4076(98)00009-8\nWindmeijer, Frank. 2005. \"A Finite Sample Correction for the Variance of Linear Efficient Two-Step GMM Estimators.\" Journal of Econometrics 126 (1): 25–51. https://doi.org/10.1016/j.jeconom.2004.02.005\nAndrews, Donald W. K., and Biao Lu. 2001. \"Consistent Model and Moment Selection Procedures for GMM Estimation.\" Journal of Econometrics 101 (1): 123–164. https://doi.org/10.1016/S0304-4076(00)00077-4","category":"section"},{"location":"#Robust-Inference","page":"Home","title":"Robust Inference","text":"Andrews, Donald W. K. 1991. \"Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.\" Econometrica 59 (3): 817–858. https://doi.org/10.2307/2938229\nHansen, Lars Peter. 1982. \"Large Sample Properties of Generalized Method of Moments Estimators.\" Econometrica 50 (4): 1029–1054. https://doi.org/10.2307/1912775\nNewey, Whitney K., and Kenneth D. West. 1987. \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.\" Econometrica 55 (3): 703–708. https://doi.org/10.2307/1913610\nNewey, Whitney K., and Kenneth D. West. 1994. \"Automatic Lag Selection in Covariance Matrix Estimation.\" Review of Economic Studies 61 (4): 631–653. https://doi.org/10.2307/2297912","category":"section"},{"location":"#Non-Gaussian-Identification-2","page":"Home","title":"Non-Gaussian Identification","text":"Hyvarinen, Aapo. 1999. \"Fast and Robust Fixed-Point Algorithms for Independent Component Analysis.\" IEEE Transactions on Neural Networks 10 (3): 626–634. https://doi.org/10.1109/72.761722\nLanne, Markku, and Helmut Lutkepohl. 2010. \"Structural Vector Autoregressions with Nonnormal Residuals.\" Journal of Business & Economic Statistics 28 (1): 159–168. https://doi.org/10.1198/jbes.2009.06003\nLanne, Markku, Mika Meitz, and Pentti Saikkonen. 2017. \"Identification and Estimation of Non-Gaussian Structural Vector Autoregressions.\" Journal of Econometrics 196 (2): 288–304. https://doi.org/10.1016/j.jeconom.2016.06.002","category":"section"},{"location":"#Hypothesis-Tests-2","page":"Home","title":"Hypothesis Tests","text":"Dickey, David A., and Wayne A. Fuller. 1979. \"Distribution of the Estimators for Autoregressive Time Series with a Unit Root.\" Journal of the American Statistical Association 74 (366): 427–431. https://doi.org/10.1080/01621459.1979.10482531\nKwiatkowski, Denis, Peter C. B. Phillips, Peter Schmidt, and Yongcheol Shin. 1992. \"Testing the Null Hypothesis of Stationarity Against the Alternative of a Unit Root.\" Journal of Econometrics 54 (1–3): 159–178. https://doi.org/10.1016/0304-4076(92)90104-Y\nGranger, Clive W. J. 1969. \"Investigating Causal Relations by Econometric Models and Cross-spectral Methods.\" Econometrica 37 (3): 424–438. https://doi.org/10.2307/1912791\nWilks, Samuel S. 1938. \"The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.\" Annals of Mathematical Statistics 9 (1): 60–62. https://doi.org/10.1214/aoms/1177732360","category":"section"},{"location":"#Nowcasting-2","page":"Home","title":"Nowcasting","text":"Banbura, Marta, and Michele Modugno. 2014. \"Maximum Likelihood Estimation of Factor Models on Datasets with Arbitrary Pattern of Missing Data.\" Journal of Applied Econometrics 29 (1): 133–160. https://doi.org/10.1002/jae.2306\nCimadomo, Jacopo, Domenico Giannone, Michele Lenza, Francesca Monti, and Andrej Sokol. 2022. \"Nowcasting with Large Bayesian Vector Autoregressions.\" ECB Working Paper No. 2696.\nBanbura, Marta, Irina Belousova, Katalin Bodnar, and Mate Barnabas Toth. 2023. \"Nowcasting Employment in the Euro Area.\" ECB Working Paper No. 2815.","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This package is released under the GNU General Public License v3.0.","category":"section"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"Contributions are welcome! Please see the GitHub repository for contribution guidelines.","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"data.md\", \"filters.md\", \"arima.md\", \"volatility.md\", \"manual.md\", \"bayesian.md\", \"vecm.md\", \"pvar.md\", \"lp.md\", \"factormodels.md\", \"innovation_accounting.md\", \"nowcast.md\", \"nongaussian.md\", \"hypothesis_tests.md\", \"examples.md\", \"api.md\", \"api_types.md\", \"api_functions.md\"]\nDepth = 2","category":"section"}]
}
